{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predicting $PM_{2.5}$ with non-CNN Models \n",
    "## (Ridge Regression, Random Forest, XGboost) \n",
    "\n",
    "The following tutorial explains how to use our alternative, non-CNN models. Here, we implement ridge regression and random forest, but XGboost is implemented similarly. To be more transparent, each cell contains copied code from our top-level python scripts. However, in practice you should only need to run their associated $\\texttt{.sh}$ files. To make the tutorial functional on a PC, we use a curated data subset that only takes 100 days of each sensor's outputs (found in the '../data' folder)\n",
    "\n",
    "The tutorial is broken into the following steps: \n",
    "- 1) Load data\n",
    "- 2) Making Train/Test Split\n",
    "- 3) Imputing data with ridge regression or random forest \n",
    "- 4) Determining optimal prediction model hyperparameters with cross-validation \n",
    "- 5) Training and testing your prediction model \n",
    "\n",
    "Each step (in bold markdown) contains a description of the code, the name of the python script, the name of the shell script in $\\texttt{.../src}$, and is followed by 3 cells: \n",
    "- 1) arguments for the associated script. Each argument should be passed to the associated shell script in the command line. Here, the arguments are all python variables with the format \"$\\texttt{args_{argument name}}$\".\n",
    "- 2) The imported dependencies required for that step \n",
    "- 3) A copy of the top-level python script found in the directory $\\texttt{src}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import configparser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['./config/py_config.ini']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config = configparser.RawConfigParser()\n",
    "config.read(\"./config/py_config.ini\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import 100 Day Data Subset: \n",
    " - to produce subset: \n",
    "  - execute in command line $\\texttt{Rscript data_setup.R}$\n",
    "  - note: the data path variables are set in $\\texttt{/src/config/data_setup_config.R}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Import preprocessed data\n",
    "dat = pd.read_csv('../data/data_to_impute.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Splitting the Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make Train/Test Split. Train/Test is advised for non-CNN prediction models, where train/test/validation are required for CNN prediction models. \n",
    "With our script, a given site will be all in train or all in test, in order to make a fair test of predictive capability\n",
    " - set argument val_split to 0, and argument val to False, to only do train/test split\n",
    " - train/test split is executed by $\\texttt{train_val_test_split.py}$\n",
    " - to execute, use $\\texttt{tvt-split.sh}$ with necessary arguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Shell arguments for tvt-split.sh: \n",
    "args_val = False\n",
    "args_train_split = 0.7\n",
    "args_val_split = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#CODE FROM train_val_test_split.py\n",
    "import sys\n",
    "from data_split_tune_utils import train_test_split, train_val_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#CODE FROM train_val_test_split.py\n",
    "if (not args_val) and args_val_split != 0:\n",
    "    print(\"Validation split specified without validation flag!\")\n",
    "    sys.exit()\n",
    "\n",
    "\n",
    "if args_train_split < 0 or args_train_split > 1 or \\\n",
    "   args_val_split < 0 or args_val_split > 1:\n",
    "    print(\"Split value out of range!\")\n",
    "    sys.exit()\n",
    "\n",
    "\n",
    "if args_train_split + args_val_split > 1:\n",
    "    print(\"Invalid train/validation split ratio! Must fall under a total of 1.\")\n",
    "    sys.exit()\n",
    "\n",
    "\n",
    "test_split = 1. - args_train_split - args_val_split\n",
    "\n",
    "\n",
    "data = pd.read_csv(config[\"data\"][\"data_to_impute\"])\n",
    "\n",
    "# set seed for reproducibility\n",
    "np.random.seed(1)\n",
    "\n",
    "if args_val: # split data into train, validation, and test sets and save\n",
    "    train, val, test = train_val_test_split(data, train_prop=args_train_split, test_prop=test_split, site_var_name=\"site\")\n",
    "    train.to_csv(config[\"data\"][\"trainV\"], index=False)\n",
    "    val.to_csv(  config[\"data\"][\"valV\"], index=False)\n",
    "    test.to_csv( config[\"data\"][\"testV\"], index=False)\n",
    "\n",
    "\n",
    "else: # split data into train and test sets and save\n",
    "    train, test = train_test_split(data, train_prop=args_train_split, site_var_name=\"site\")\n",
    "    train.to_csv(config[\"data\"][\"train\"], index=False)\n",
    "    test.to_csv(config[\"data\"][\"test\"], index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Size:  (145500, 62)\n",
      "Test Size:  (62400, 62)\n"
     ]
    }
   ],
   "source": [
    "print('Training Size: ', train.shape)\n",
    "print('Test Size: ', test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imputing Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Impute data with Ridge Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### First, train a ridge regression imputation model \n",
    "- The code 'pickles' the trained ridge imputation model, so you don't need to train every time you wish to impute. Furthermore, you are able to train the imputer on a subset of data (to save time). The next section will use the trained imputer to actually impute the data\n",
    "- the following is implemented in $\\texttt{ridge_imputer_fit.py}$\n",
    "- to run in shell, use $\\texttt{ridge-imputer-fit.sh}$ with appropriate args"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#shell arguments: \n",
    "args_val = False \n",
    "args_impute_split = 0.5\n",
    "args_max_iter = 10\n",
    "args_initial_strategy = 'mean'\n",
    "args_alpha = 0.0001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pickle \n",
    "from data_split_tune_utils import train_test_split, X_y_site_split\n",
    "from predictiveImputer_mod import PredictiveImputer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of variables: 60\n",
      "Iteration 1\n",
      "Variable # 0\n",
      "Variable # 1\n",
      "Variable # 2\n",
      "Variable # 3\n",
      "Variable # 4\n",
      "Variable # 5\n",
      "Variable # 6\n",
      "Variable # 7\n",
      "Variable # 8\n",
      "Variable # 9\n",
      "Variable # 10\n",
      "Variable # 11\n",
      "Variable # 12\n",
      "Variable # 13\n",
      "Variable # 14\n",
      "Variable # 15\n",
      "Variable # 16\n",
      "Variable # 17\n",
      "Variable # 18\n",
      "Variable # 19\n",
      "Variable # 20\n",
      "Variable # 21\n",
      "Variable # 22\n",
      "Variable # 23\n",
      "Variable # 24\n",
      "Variable # 25\n",
      "Variable # 26\n",
      "Variable # 27\n",
      "Variable # 28\n",
      "Variable # 29\n",
      "Variable # 30\n",
      "Variable # 31\n",
      "Variable # 32\n",
      "Variable # 33\n",
      "Variable # 34\n",
      "Variable # 35\n",
      "Variable # 36\n",
      "Variable # 37\n",
      "Variable # 38\n",
      "Variable # 39\n",
      "Variable # 40\n",
      "Variable # 41\n",
      "Variable # 42\n",
      "Variable # 43\n",
      "Variable # 44\n",
      "Variable # 45\n",
      "Variable # 46\n",
      "Variable # 47\n",
      "Variable # 48\n",
      "Variable # 49\n",
      "Variable # 50\n",
      "Variable # 51\n",
      "Variable # 52\n",
      "Variable # 53\n",
      "Variable # 54\n",
      "Variable # 55\n",
      "Variable # 56\n",
      "Variable # 57\n",
      "Variable # 58\n",
      "Variable # 59\n",
      "Difference: 0.00168568382991\n",
      "\n",
      "Iteration 2\n",
      "Variable # 0\n",
      "Variable # 1\n",
      "Variable # 2\n",
      "Variable # 3\n",
      "Variable # 4\n",
      "Variable # 5\n",
      "Variable # 6\n",
      "Variable # 7\n",
      "Variable # 8\n",
      "Variable # 9\n",
      "Variable # 10\n",
      "Variable # 11\n",
      "Variable # 12\n",
      "Variable # 13\n",
      "Variable # 14\n",
      "Variable # 15\n",
      "Variable # 16\n",
      "Variable # 17\n",
      "Variable # 18\n",
      "Variable # 19\n",
      "Variable # 20\n",
      "Variable # 21\n",
      "Variable # 22\n",
      "Variable # 23\n",
      "Variable # 24\n",
      "Variable # 25\n",
      "Variable # 26\n",
      "Variable # 27\n",
      "Variable # 28\n",
      "Variable # 29\n",
      "Variable # 30\n",
      "Variable # 31\n",
      "Variable # 32\n",
      "Variable # 33\n",
      "Variable # 34\n",
      "Variable # 35\n",
      "Variable # 36\n",
      "Variable # 37\n",
      "Variable # 38\n",
      "Variable # 39\n",
      "Variable # 40\n",
      "Variable # 41\n",
      "Variable # 42\n",
      "Variable # 43\n",
      "Variable # 44\n",
      "Variable # 45\n",
      "Variable # 46\n",
      "Variable # 47\n",
      "Variable # 48\n",
      "Variable # 49\n",
      "Variable # 50\n",
      "Variable # 51\n",
      "Variable # 52\n",
      "Variable # 53\n",
      "Variable # 54\n",
      "Variable # 55\n",
      "Variable # 56\n",
      "Variable # 57\n",
      "Variable # 58\n",
      "Variable # 59\n",
      "Difference: 6.08643205308e-06\n",
      "\n",
      "Iteration 3\n",
      "Variable # 0\n",
      "Variable # 1\n",
      "Variable # 2\n",
      "Variable # 3\n",
      "Variable # 4\n",
      "Variable # 5\n",
      "Variable # 6\n",
      "Variable # 7\n",
      "Variable # 8\n",
      "Variable # 9\n",
      "Variable # 10\n",
      "Variable # 11\n",
      "Variable # 12\n",
      "Variable # 13\n",
      "Variable # 14\n",
      "Variable # 15\n",
      "Variable # 16\n",
      "Variable # 17\n",
      "Variable # 18\n",
      "Variable # 19\n",
      "Variable # 20\n",
      "Variable # 21\n",
      "Variable # 22\n",
      "Variable # 23\n",
      "Variable # 24\n",
      "Variable # 25\n",
      "Variable # 26\n",
      "Variable # 27\n",
      "Variable # 28\n",
      "Variable # 29\n",
      "Variable # 30\n",
      "Variable # 31\n",
      "Variable # 32\n",
      "Variable # 33\n",
      "Variable # 34\n",
      "Variable # 35\n",
      "Variable # 36\n",
      "Variable # 37\n",
      "Variable # 38\n",
      "Variable # 39\n",
      "Variable # 40\n",
      "Variable # 41\n",
      "Variable # 42\n",
      "Variable # 43\n",
      "Variable # 44\n",
      "Variable # 45\n",
      "Variable # 46\n",
      "Variable # 47\n",
      "Variable # 48\n",
      "Variable # 49\n",
      "Variable # 50\n",
      "Variable # 51\n",
      "Variable # 52\n",
      "Variable # 53\n",
      "Variable # 54\n",
      "Variable # 55\n",
      "Variable # 56\n",
      "Variable # 57\n",
      "Variable # 58\n",
      "Variable # 59\n",
      "Difference: 1.59328443714e-06\n",
      "\n",
      "Iteration 4\n",
      "Variable # 0\n",
      "Variable # 1\n",
      "Variable # 2\n",
      "Variable # 3\n",
      "Variable # 4\n",
      "Variable # 5\n",
      "Variable # 6\n",
      "Variable # 7\n",
      "Variable # 8\n",
      "Variable # 9\n",
      "Variable # 10\n",
      "Variable # 11\n",
      "Variable # 12\n",
      "Variable # 13\n",
      "Variable # 14\n",
      "Variable # 15\n",
      "Variable # 16\n",
      "Variable # 17\n",
      "Variable # 18\n",
      "Variable # 19\n",
      "Variable # 20\n",
      "Variable # 21\n",
      "Variable # 22\n",
      "Variable # 23\n",
      "Variable # 24\n",
      "Variable # 25\n",
      "Variable # 26\n",
      "Variable # 27\n",
      "Variable # 28\n",
      "Variable # 29\n",
      "Variable # 30\n",
      "Variable # 31\n",
      "Variable # 32\n",
      "Variable # 33\n",
      "Variable # 34\n",
      "Variable # 35\n",
      "Variable # 36\n",
      "Variable # 37\n",
      "Variable # 38\n",
      "Variable # 39\n",
      "Variable # 40\n",
      "Variable # 41\n",
      "Variable # 42\n",
      "Variable # 43\n",
      "Variable # 44\n",
      "Variable # 45\n",
      "Variable # 46\n",
      "Variable # 47\n",
      "Variable # 48\n",
      "Variable # 49\n",
      "Variable # 50\n",
      "Variable # 51\n",
      "Variable # 52\n",
      "Variable # 53\n",
      "Variable # 54\n",
      "Variable # 55\n",
      "Variable # 56\n",
      "Variable # 57\n",
      "Variable # 58\n",
      "Variable # 59\n",
      "Difference: 4.76145626788e-07\n",
      "\n",
      "Iteration 5\n",
      "Variable # 0\n",
      "Variable # 1\n",
      "Variable # 2\n",
      "Variable # 3\n",
      "Variable # 4\n",
      "Variable # 5\n",
      "Variable # 6\n",
      "Variable # 7\n",
      "Variable # 8\n",
      "Variable # 9\n",
      "Variable # 10\n",
      "Variable # 11\n",
      "Variable # 12\n",
      "Variable # 13\n",
      "Variable # 14\n",
      "Variable # 15\n",
      "Variable # 16\n",
      "Variable # 17\n",
      "Variable # 18\n",
      "Variable # 19\n",
      "Variable # 20\n",
      "Variable # 21\n",
      "Variable # 22\n",
      "Variable # 23\n",
      "Variable # 24\n",
      "Variable # 25\n",
      "Variable # 26\n",
      "Variable # 27\n",
      "Variable # 28\n",
      "Variable # 29\n",
      "Variable # 30\n",
      "Variable # 31\n",
      "Variable # 32\n",
      "Variable # 33\n",
      "Variable # 34\n",
      "Variable # 35\n",
      "Variable # 36\n",
      "Variable # 37\n",
      "Variable # 38\n",
      "Variable # 39\n",
      "Variable # 40\n",
      "Variable # 41\n",
      "Variable # 42\n",
      "Variable # 43\n",
      "Variable # 44\n",
      "Variable # 45\n",
      "Variable # 46\n",
      "Variable # 47\n",
      "Variable # 48\n",
      "Variable # 49\n",
      "Variable # 50\n",
      "Variable # 51\n",
      "Variable # 52\n",
      "Variable # 53\n",
      "Variable # 54\n",
      "Variable # 55\n",
      "Variable # 56\n",
      "Variable # 57\n",
      "Variable # 58\n",
      "Variable # 59\n",
      "Difference: 2.37140440419e-07\n",
      "\n",
      "Iteration 6\n",
      "Variable # 0\n",
      "Variable # 1\n",
      "Variable # 2\n",
      "Variable # 3\n",
      "Variable # 4\n",
      "Variable # 5\n",
      "Variable # 6\n",
      "Variable # 7\n",
      "Variable # 8\n",
      "Variable # 9\n",
      "Variable # 10\n",
      "Variable # 11\n",
      "Variable # 12\n",
      "Variable # 13\n",
      "Variable # 14\n",
      "Variable # 15\n",
      "Variable # 16\n",
      "Variable # 17\n",
      "Variable # 18\n",
      "Variable # 19\n",
      "Variable # 20\n",
      "Variable # 21\n",
      "Variable # 22\n",
      "Variable # 23\n",
      "Variable # 24\n",
      "Variable # 25\n",
      "Variable # 26\n",
      "Variable # 27\n",
      "Variable # 28\n",
      "Variable # 29\n",
      "Variable # 30\n",
      "Variable # 31\n",
      "Variable # 32\n",
      "Variable # 33\n",
      "Variable # 34\n",
      "Variable # 35\n",
      "Variable # 36\n",
      "Variable # 37\n",
      "Variable # 38\n",
      "Variable # 39\n",
      "Variable # 40\n",
      "Variable # 41\n",
      "Variable # 42\n",
      "Variable # 43\n",
      "Variable # 44\n",
      "Variable # 45\n",
      "Variable # 46\n",
      "Variable # 47\n",
      "Variable # 48\n",
      "Variable # 49\n",
      "Variable # 50\n",
      "Variable # 51\n",
      "Variable # 52\n",
      "Variable # 53\n",
      "Variable # 54\n",
      "Variable # 55\n",
      "Variable # 56\n",
      "Variable # 57\n",
      "Variable # 58\n",
      "Variable # 59\n",
      "Difference: 1.60301782248e-07\n",
      "\n",
      "Iteration 7\n",
      "Variable # 0\n",
      "Variable # 1\n",
      "Variable # 2\n",
      "Variable # 3\n",
      "Variable # 4\n",
      "Variable # 5\n",
      "Variable # 6\n",
      "Variable # 7\n",
      "Variable # 8\n",
      "Variable # 9\n",
      "Variable # 10\n",
      "Variable # 11\n",
      "Variable # 12\n",
      "Variable # 13\n",
      "Variable # 14\n",
      "Variable # 15\n",
      "Variable # 16\n",
      "Variable # 17\n",
      "Variable # 18\n",
      "Variable # 19\n",
      "Variable # 20\n",
      "Variable # 21\n",
      "Variable # 22\n",
      "Variable # 23\n",
      "Variable # 24\n",
      "Variable # 25\n",
      "Variable # 26\n",
      "Variable # 27\n",
      "Variable # 28\n",
      "Variable # 29\n",
      "Variable # 30\n",
      "Variable # 31\n",
      "Variable # 32\n",
      "Variable # 33\n",
      "Variable # 34\n",
      "Variable # 35\n",
      "Variable # 36\n",
      "Variable # 37\n",
      "Variable # 38\n",
      "Variable # 39\n",
      "Variable # 40\n",
      "Variable # 41\n",
      "Variable # 42\n",
      "Variable # 43\n",
      "Variable # 44\n",
      "Variable # 45\n",
      "Variable # 46\n",
      "Variable # 47\n",
      "Variable # 48\n",
      "Variable # 49\n",
      "Variable # 50\n",
      "Variable # 51\n",
      "Variable # 52\n",
      "Variable # 53\n",
      "Variable # 54\n",
      "Variable # 55\n",
      "Variable # 56\n",
      "Variable # 57\n",
      "Variable # 58\n",
      "Variable # 59\n",
      "Difference: 1.33172512993e-07\n",
      "\n",
      "Iteration 8\n",
      "Variable # 0\n",
      "Variable # 1\n",
      "Variable # 2\n",
      "Variable # 3\n",
      "Variable # 4\n",
      "Variable # 5\n",
      "Variable # 6\n",
      "Variable # 7\n",
      "Variable # 8\n",
      "Variable # 9\n",
      "Variable # 10\n",
      "Variable # 11\n",
      "Variable # 12\n",
      "Variable # 13\n",
      "Variable # 14\n",
      "Variable # 15\n",
      "Variable # 16\n",
      "Variable # 17\n",
      "Variable # 18\n",
      "Variable # 19\n",
      "Variable # 20\n",
      "Variable # 21\n",
      "Variable # 22\n",
      "Variable # 23\n",
      "Variable # 24\n",
      "Variable # 25\n",
      "Variable # 26\n",
      "Variable # 27\n",
      "Variable # 28\n",
      "Variable # 29\n",
      "Variable # 30\n",
      "Variable # 31\n",
      "Variable # 32\n",
      "Variable # 33\n",
      "Variable # 34\n",
      "Variable # 35\n",
      "Variable # 36\n",
      "Variable # 37\n",
      "Variable # 38\n",
      "Variable # 39\n",
      "Variable # 40\n",
      "Variable # 41\n",
      "Variable # 42\n",
      "Variable # 43\n",
      "Variable # 44\n",
      "Variable # 45\n",
      "Variable # 46\n",
      "Variable # 47\n",
      "Variable # 48\n",
      "Variable # 49\n",
      "Variable # 50\n",
      "Variable # 51\n",
      "Variable # 52\n",
      "Variable # 53\n",
      "Variable # 54\n",
      "Variable # 55\n",
      "Variable # 56\n",
      "Variable # 57\n",
      "Variable # 58\n",
      "Variable # 59\n",
      "Difference: 1.08305959346e-07\n",
      "\n",
      "Iteration 9\n",
      "Variable # 0\n",
      "Variable # 1\n",
      "Variable # 2\n",
      "Variable # 3\n",
      "Variable # 4\n",
      "Variable # 5\n",
      "Variable # 6\n",
      "Variable # 7\n",
      "Variable # 8\n",
      "Variable # 9\n",
      "Variable # 10\n",
      "Variable # 11\n",
      "Variable # 12\n",
      "Variable # 13\n",
      "Variable # 14\n",
      "Variable # 15\n",
      "Variable # 16\n",
      "Variable # 17\n",
      "Variable # 18\n",
      "Variable # 19\n",
      "Variable # 20\n",
      "Variable # 21\n",
      "Variable # 22\n",
      "Variable # 23\n",
      "Variable # 24\n",
      "Variable # 25\n",
      "Variable # 26\n",
      "Variable # 27\n",
      "Variable # 28\n",
      "Variable # 29\n",
      "Variable # 30\n",
      "Variable # 31\n",
      "Variable # 32\n",
      "Variable # 33\n",
      "Variable # 34\n",
      "Variable # 35\n",
      "Variable # 36\n",
      "Variable # 37\n",
      "Variable # 38\n",
      "Variable # 39\n",
      "Variable # 40\n",
      "Variable # 41\n",
      "Variable # 42\n",
      "Variable # 43\n",
      "Variable # 44\n",
      "Variable # 45\n",
      "Variable # 46\n",
      "Variable # 47\n",
      "Variable # 48\n",
      "Variable # 49\n",
      "Variable # 50\n",
      "Variable # 51\n",
      "Variable # 52\n",
      "Variable # 53\n",
      "Variable # 54\n",
      "Variable # 55\n",
      "Variable # 56\n",
      "Variable # 57\n",
      "Variable # 58\n",
      "Variable # 59\n",
      "Difference: 6.32075115951e-08\n",
      "\n",
      "Iteration 10\n",
      "Variable # 0\n",
      "Variable # 1\n",
      "Variable # 2\n",
      "Variable # 3\n",
      "Variable # 4\n",
      "Variable # 5\n",
      "Variable # 6\n",
      "Variable # 7\n",
      "Variable # 8\n",
      "Variable # 9\n",
      "Variable # 10\n",
      "Variable # 11\n",
      "Variable # 12\n",
      "Variable # 13\n",
      "Variable # 14\n",
      "Variable # 15\n",
      "Variable # 16\n",
      "Variable # 17\n",
      "Variable # 18\n",
      "Variable # 19\n",
      "Variable # 20\n",
      "Variable # 21\n",
      "Variable # 22\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Variable # 23\n",
      "Variable # 24\n",
      "Variable # 25\n",
      "Variable # 26\n",
      "Variable # 27\n",
      "Variable # 28\n",
      "Variable # 29\n",
      "Variable # 30\n",
      "Variable # 31\n",
      "Variable # 32\n",
      "Variable # 33\n",
      "Variable # 34\n",
      "Variable # 35\n",
      "Variable # 36\n",
      "Variable # 37\n",
      "Variable # 38\n",
      "Variable # 39\n",
      "Variable # 40\n",
      "Variable # 41\n",
      "Variable # 42\n",
      "Variable # 43\n",
      "Variable # 44\n",
      "Variable # 45\n",
      "Variable # 46\n",
      "Variable # 47\n",
      "Variable # 48\n",
      "Variable # 49\n",
      "Variable # 50\n",
      "Variable # 51\n",
      "Variable # 52\n",
      "Variable # 53\n",
      "Variable # 54\n",
      "Variable # 55\n",
      "Variable # 56\n",
      "Variable # 57\n",
      "Variable # 58\n",
      "Variable # 59\n",
      "Difference: 3.62899334253e-08\n",
      "\n"
     ]
    }
   ],
   "source": [
    "if args_impute_split < 0 or args_impute_split > 1:\n",
    "    print(\"Impute split value out of range!\")\n",
    "    sys.exit()\n",
    "\n",
    "\n",
    "train = None\n",
    "if args_val:\n",
    "    train = pd.read_csv(config[\"data\"][\"trainV\"])\n",
    "\n",
    "else:\n",
    "    train = pd.read_csv(config[\"data\"][\"train\"])\n",
    "\n",
    "\n",
    "# set seed for reproducibility\n",
    "np.random.seed(1)\n",
    "\n",
    "# split train up; only fit imputer on part of train set due to memory/time\n",
    "train1, train2 = train_test_split(train, train_prop=args_impute_split, site_var_name=\"site\")\n",
    "\n",
    "# split part of train set to fit imputer on into x, y, and site\n",
    "train1_x, train1_y, train1_sites = X_y_site_split(train1, y_var_name=\"MonitorData\", site_var_name=\"site\")\n",
    "\n",
    "# create imputer and fit on part of train set\n",
    "ridge_imputer = PredictiveImputer(max_iter=args_max_iter, initial_strategy=args_initial_strategy, f_model=\"Ridge\")\n",
    "ridge_imputer.fit(train1_x, alpha=args_alpha, fit_intercept=True, normalize=True, random_state=1)\n",
    "\n",
    "# save fitted imputer\n",
    "pickle.dump(ridge_imputer, open(config[\"Ridge_Imputation\"][\"model\"], \"wb\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Then, run the trained ridge regression model to impute the data\n",
    "- the following is implemented in $\\texttt{ridge_impute_eval_train_val_test.py}$\n",
    "- to run in shell, use $\\texttt{ridge-impute-tt.sh}$ with appropriate arguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Shell arguments: \n",
    "args_val = False\n",
    "args_backup_strategy = 'mean'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "# this imported function was created for this package to split datasets (see data_split_tune_utils.py)\n",
    "from data_split_tune_utils import X_y_site_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train, val, test = None, None, None\n",
    "\n",
    "if args_val:\n",
    "    train = pd.read_csv(config[\"data\"][\"trainV\"])\n",
    "    val   = pd.read_csv(config[\"data\"][\"valV\"])\n",
    "    test  = pd.read_csv(config[\"data\"][\"testV\"])\n",
    "\n",
    "else:\n",
    "    train = pd.read_csv(config[\"data\"][\"train\"])\n",
    "    test  = pd.read_csv(config[\"data\"][\"test\"])\n",
    "\n",
    "# set seed for reproducibility\n",
    "np.random.seed(1)\n",
    "\n",
    "# load fitted ridge imputer\n",
    "ridge_imputer = pickle.load(open(config[\"Ridge_Imputation\"][\"model\"], \"rb\"))\n",
    "\n",
    "# split train and test datasets into x, y, and site\n",
    "train_x, train_y, train_sites = X_y_site_split(train, y_var_name=\"MonitorData\", site_var_name=\"site\")\n",
    "test_x, test_y, test_sites = X_y_site_split(test, y_var_name=\"MonitorData\", site_var_name=\"site\")\n",
    "\n",
    "### make imputations on train and test data matrices and create dataframes with imputation R^2 evaluations; computed weighted R^2 values\n",
    "train_x_imp, train_r2_scores_df = ridge_imputer.transform(train_x, evaluate=True, backup_impute_strategy=args_backup_strategy)\n",
    "train_r2_scores_df.columns = [\"Train_R2\", \"Train_num_missing\"]\n",
    "train_r2_scores_df.loc[max(train_r2_scores_df.index)+1, :] = [np.average(train_r2_scores_df.loc[:, \"Train_R2\"].values,\n",
    "                                                                   weights=train_r2_scores_df.loc[:, \"Train_num_missing\"].values,\n",
    "                                                                   axis=0), np.mean(train_r2_scores_df.loc[:, \"Train_num_missing\"].values)]\n",
    "\n",
    "test_x_imp, test_r2_scores_df = ridge_imputer.transform(test_x, evaluate=True, backup_impute_strategy=args_backup_strategy)\n",
    "test_r2_scores_df.columns = [\"Test_R2\", \"Test_num_missing\"]\n",
    "test_r2_scores_df.loc[max(test_r2_scores_df.index)+1, :] = [np.average(test_r2_scores_df.loc[:, \"Test_R2\"].values,\n",
    "                                                                   weights=test_r2_scores_df.loc[:, \"Test_num_missing\"].values,\n",
    "                                                                   axis=0), np.mean(test_r2_scores_df.loc[:, \"Test_num_missing\"].values)]\n",
    "\n",
    "### convert imputed train and test data matrices back into pandas dataframes with column names\n",
    "cols = [\"site\", \"MonitorData\"] + list(train_x.columns)\n",
    "train_imp_df = pd.DataFrame(np.concatenate([train_sites.values.reshape(len(train_sites), -1),\n",
    "                                              train_y.values.reshape(len(train_y), -1),\n",
    "                                              train_x_imp], axis=1),\n",
    "                                              columns=cols)\n",
    "\n",
    "test_imp_df = pd.DataFrame(np.concatenate([test_sites.values.reshape(len(test_sites), -1),\n",
    "                                              test_y.values.reshape(len(test_y), -1),\n",
    "                                              test_x_imp], axis=1),\n",
    "                                              columns=cols)\n",
    "\n",
    "var_df = pd.DataFrame(np.array(cols[2:] + [\"Weighted_Mean_R2\"]).reshape(len(cols)-2+1, -1), columns=[\"Variable\"])\n",
    "\n",
    "\n",
    "\n",
    "if args_val:\n",
    "    # split val into x, y, and site\n",
    "    val_x, val_y, val_sites = X_y_site_split(val, y_var_name=\"MonitorData\", site_var_name=\"site\")\n",
    "\n",
    "    ### make imputations on val data matrix and create dataframe with imputation R^2 evaluations; computed weighted R^2 values\n",
    "    val_x_imp, val_r2_scores_df = ridge_imputer.transform(val_x, evaluate=True, backup_impute_strategy=\"mean\")\n",
    "    val_r2_scores_df.columns = [\"Val_R2\", \"Val_num_missing\"]\n",
    "    val_r2_scores_df.loc[max(val_r2_scores_df.index)+1, :] = [np.average(val_r2_scores_df.loc[:, \"Val_R2\"].values,\n",
    "                                                                       weights=val_r2_scores_df.loc[:, \"Val_num_missing\"].values,\n",
    "                                                                       axis=0), np.mean(val_r2_scores_df.loc[:, \"Val_num_missing\"].values)]\n",
    "\n",
    "    ### convert imputed val data matrix back into pandas dataframes with column names\n",
    "    val_imp_df = pd.DataFrame(np.concatenate([val_sites.values.reshape(len(val_sites), -1),\n",
    "                                                  val_y.values.reshape(len(val_y), -1),\n",
    "                                                  val_x_imp], axis=1),\n",
    "                                                  columns=cols)\n",
    "    # save imputed datasets\n",
    "    train_imp_df.to_csv(config[\"Ridge_Imputation\"][\"trainV\"], index=False)\n",
    "    test_imp_df.to_csv(config[\"Ridge_Imputation\"][\"testV\"], index=False)\n",
    "    val_imp_df.to_csv(config[\"Ridge_Imputation\"][\"valV\"], index=False)\n",
    "\n",
    "    # put R^2 evaluations for train, val, and test datasets into same pandas dataframe\n",
    "    r2_scores_df = pd.concat([var_df, train_r2_scores_df, val_r2_scores_df, test_r2_scores_df], axis=1)\n",
    "\n",
    "    # save evaluations\n",
    "    r2_scores_df.to_csv(config[\"Ridge_Imputation\"][\"r2_scores\"], index=False)\n",
    "    \n",
    "else:\n",
    "    # save imputed train and test datasets\n",
    "    train_imp_df.to_csv(config[\"Ridge_Imputation\"][\"train\"], index=False)\n",
    "    test_imp_df.to_csv(config[\"Ridge_Imputation\"][\"test\"], index=False)\n",
    "    # put R^2 evaluations for train and test datasets into same pandas dataframe and save\n",
    "    r2_scores_df = pd.concat([var_df, train_r2_scores_df, test_r2_scores_df], axis=1)\n",
    "    r2_scores_df.to_csv(config[\"Ridge_Imputation\"][\"r2_scores\"], index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Variable</th>\n",
       "      <th>Train_R2</th>\n",
       "      <th>Train_num_missing</th>\n",
       "      <th>Test_R2</th>\n",
       "      <th>Test_num_missing</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Unnamed: 0</td>\n",
       "      <td>0.059603</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.026848</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>month</td>\n",
       "      <td>0.999993</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.999993</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>cumulative_month</td>\n",
       "      <td>0.999993</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.999993</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>sin_time</td>\n",
       "      <td>0.951986</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.952512</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>cos_time</td>\n",
       "      <td>0.998624</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.998614</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           Variable  Train_R2  Train_num_missing   Test_R2  Test_num_missing\n",
       "0        Unnamed: 0  0.059603                0.0  0.026848               0.0\n",
       "1             month  0.999993                0.0  0.999993               0.0\n",
       "2  cumulative_month  0.999993                0.0  0.999993               0.0\n",
       "3          sin_time  0.951986                0.0  0.952512               0.0\n",
       "4          cos_time  0.998624                0.0  0.998614               0.0"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nrows = 5\n",
    "r2_scores_df.head(nrows)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Impute with Random Forest\n",
    "- This random forest distribution, while effective, struggles with memory. As such the random forest imputation does not save the model (since the model object itself uses excessive memory), but rather trains and imputes in one function. \n",
    "- We use minimal iterations, trees, max_features to speed up training\n",
    "- The following is implemented in $\\texttt{rf_fit_impute_eval_train_val_test.py}$\n",
    "- To run, use $\\texttt{rf-impute-tvt.sh}$ with appropriate args"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Shell arguments for rf-impute-tvt.sh: \n",
    "args_val = True \n",
    "args_impute_split = 0.3\n",
    "args_initial_strategy = 'mean'\n",
    "args_backup_strategy = 'median'\n",
    "args_max_iter = 2\n",
    "args_max_features = 5 \n",
    "args_n_estimators = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#CODE FROM rf_fit_impute_eval_train_val_test.py\n",
    "# these are imported functions created for this package that split datasets (see data_split_tune_utils.py)\n",
    "from data_split_tune_utils import train_test_split, X_y_site_split, train_val_test_split\n",
    "# this is the PredictiveImputer class inspired by the MissForest algorithm (see predictiveImputer_mod.py)\n",
    "from predictiveImputer_mod import PredictiveImputer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of variables: 60\n",
      "Iteration 1\n",
      "Variable # 0\n",
      "Variable # 1\n",
      "Variable # 2\n",
      "Variable # 3\n",
      "Variable # 4\n",
      "Variable # 5\n",
      "Variable # 6\n",
      "Variable # 7\n",
      "Variable # 8\n",
      "Variable # 9\n",
      "Variable # 10\n",
      "Variable # 11\n",
      "Variable # 12\n",
      "Variable # 13\n",
      "Variable # 14\n",
      "Variable # 15\n",
      "Variable # 16\n",
      "Variable # 17\n",
      "Variable # 18\n",
      "Variable # 19\n",
      "Variable # 20\n",
      "Variable # 21\n",
      "Variable # 22\n",
      "Variable # 23\n",
      "Variable # 24\n",
      "Variable # 25\n",
      "Variable # 26\n",
      "Variable # 27\n",
      "Variable # 28\n",
      "Variable # 29\n",
      "Variable # 30\n",
      "Variable # 31\n",
      "Variable # 32\n",
      "Variable # 33\n",
      "Variable # 34\n",
      "Variable # 35\n",
      "Variable # 36\n",
      "Variable # 37\n",
      "Variable # 38\n",
      "Variable # 39\n",
      "Variable # 40\n",
      "Variable # 41\n",
      "Variable # 42\n",
      "Variable # 43\n",
      "Variable # 44\n",
      "Variable # 45\n",
      "Variable # 46\n",
      "Variable # 47\n",
      "Variable # 48\n",
      "Variable # 49\n",
      "Variable # 50\n",
      "Variable # 51\n",
      "Variable # 52\n",
      "Variable # 53\n",
      "Variable # 54\n",
      "Variable # 55\n",
      "Variable # 56\n",
      "Variable # 57\n",
      "Variable # 58\n",
      "Variable # 59\n",
      "Difference: 0.000576077413529\n",
      "\n",
      "Iteration 2\n",
      "Variable # 0\n",
      "Variable # 1\n",
      "Variable # 2\n",
      "Variable # 3\n",
      "Variable # 4\n",
      "Variable # 5\n",
      "Variable # 6\n",
      "Variable # 7\n",
      "Variable # 8\n",
      "Variable # 9\n",
      "Variable # 10\n",
      "Variable # 11\n",
      "Variable # 12\n",
      "Variable # 13\n",
      "Variable # 14\n",
      "Variable # 15\n",
      "Variable # 16\n",
      "Variable # 17\n",
      "Variable # 18\n",
      "Variable # 19\n",
      "Variable # 20\n",
      "Variable # 21\n",
      "Variable # 22\n",
      "Variable # 23\n",
      "Variable # 24\n",
      "Variable # 25\n",
      "Variable # 26\n",
      "Variable # 27\n",
      "Variable # 28\n",
      "Variable # 29\n",
      "Variable # 30\n",
      "Variable # 31\n",
      "Variable # 32\n",
      "Variable # 33\n",
      "Variable # 34\n",
      "Variable # 35\n",
      "Variable # 36\n",
      "Variable # 37\n",
      "Variable # 38\n",
      "Variable # 39\n",
      "Variable # 40\n",
      "Variable # 41\n",
      "Variable # 42\n",
      "Variable # 43\n",
      "Variable # 44\n",
      "Variable # 45\n",
      "Variable # 46\n",
      "Variable # 47\n",
      "Variable # 48\n",
      "Variable # 49\n",
      "Variable # 50\n",
      "Variable # 51\n",
      "Variable # 52\n",
      "Variable # 53\n",
      "Variable # 54\n",
      "Variable # 55\n",
      "Variable # 56\n",
      "Variable # 57\n",
      "Variable # 58\n",
      "Variable # 59\n",
      "Difference: 0.000155448984076\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#CODE FROM rf_fit_impute_eval_train_val_test.py\n",
    "train, val, test = None, None, None\n",
    "\n",
    "if args_val:\n",
    "    train = pd.read_csv(config[\"data\"][\"trainV\"])\n",
    "    val   = pd.read_csv(config[\"data\"][\"valV\"])\n",
    "    test  = pd.read_csv(config[\"data\"][\"testV\"])\n",
    "\n",
    "else:\n",
    "    train = pd.read_csv(config[\"data\"][\"train\"])\n",
    "    test  = pd.read_csv(config[\"data\"][\"test\"])\n",
    "\n",
    "# set seed for reproducibility\n",
    "np.random.seed(1)\n",
    "\n",
    "# split train up; only fit imputer on part of train set due to memory/time\n",
    "train1, train2 = train_test_split(train, train_prop=args_impute_split, site_var_name=\"site\")\n",
    "\n",
    "# split train and test datasets into x, y, and site\n",
    "train1_x, train1_y, train1_sites = X_y_site_split(train1, y_var_name=\"MonitorData\", site_var_name=\"site\")\n",
    "train2_x, train2_y, train2_sites = X_y_site_split(train2, y_var_name=\"MonitorData\", site_var_name=\"site\")\n",
    "test_x, test_y, test_sites = X_y_site_split(test, y_var_name=\"MonitorData\", site_var_name=\"site\")\n",
    "\n",
    "# create imputer and fit on part of train set\n",
    "rf_imputer = PredictiveImputer(max_iter=args_max_iter, initial_strategy=args_initial_strategy, f_model=\"RandomForest\")\n",
    "rf_imputer.fit(train1_x, max_features=args_max_features, n_estimators=args_n_estimators, n_jobs=-1, verbose=0, random_state=1)\n",
    "\n",
    "### make imputations on train and test data matrices and create dataframes with imputation R^2 evaluations; computed weighted R^2 values\n",
    "train1_x_imp, train1_r2_scores_df = rf_imputer.transform(train1_x, evaluate=True, backup_impute_strategy=args_backup_strategy)\n",
    "train1_r2_scores_df.columns = [\"Train1_R2\", \"Train1_num_missing\"]\n",
    "train1_r2_scores_df.loc[max(train1_r2_scores_df.index)+1, :] = [np.average(train1_r2_scores_df.loc[:, \"Train1_R2\"].values,\n",
    "                                                                   weights=train1_r2_scores_df.loc[:, \"Train1_num_missing\"].values,\n",
    "                                                                   axis=0), np.mean(train1_r2_scores_df.loc[:, \"Train1_num_missing\"].values)]\n",
    "\n",
    "train2_x_imp, train2_r2_scores_df = rf_imputer.transform(train2_x, evaluate = True, backup_impute_strategy = \"mean\")\n",
    "train2_r2_scores_df.columns = [\"Train2_R2\", \"Train2_num_missing\"]\n",
    "train2_r2_scores_df.loc[max(train2_r2_scores_df.index)+1, :] = [np.average(train2_r2_scores_df.loc[:, \"Train2_R2\"].values,\n",
    "                                                                   weights=train2_r2_scores_df.loc[:, \"Train2_num_missing\"].values,\n",
    "                                                                   axis=0), np.mean(train2_r2_scores_df.loc[:, \"Train2_num_missing\"].values)]\n",
    "\n",
    "test_x_imp, test_r2_scores_df = rf_imputer.transform(test_x, evaluate = True, backup_impute_strategy = \"mean\")\n",
    "test_r2_scores_df.columns = [\"Test_R2\", \"Test_num_missing\"]\n",
    "test_r2_scores_df.loc[max(test_r2_scores_df.index)+1, :] = [np.average(test_r2_scores_df.loc[:, \"Test_R2\"].values,\n",
    "                                                                   weights = test_r2_scores_df.loc[:, \"Test_num_missing\"].values,\n",
    "                                                                   axis=0), np.mean(test_r2_scores_df.loc[:, \"Test_num_missing\"].values)]\n",
    "\n",
    "### convert imputed train and test data matrices back into pandas dataframes with column names\n",
    "cols = [\"site\", \"MonitorData\"] + list(train1_x.columns)\n",
    "train1_imp_df = pd.DataFrame(np.concatenate([train1_sites.values.reshape(len(train1_sites), -1),\n",
    "                                              train1_y.values.reshape(len(train1_y), -1),\n",
    "                                              train1_x_imp], axis=1),\n",
    "                                              columns=cols)\n",
    "\n",
    "train2_imp_df = pd.DataFrame(np.concatenate([train2_sites.values.reshape(len(train2_sites), -1),\n",
    "                                              train2_y.values.reshape(len(train2_y), -1),\n",
    "                                              train2_x_imp], axis=1),\n",
    "                                              columns=cols)\n",
    "\n",
    "test_imp_df = pd.DataFrame(np.concatenate([test_sites.values.reshape(len(test_sites), -1),\n",
    "                                              test_y.values.reshape(len(test_y), -1),\n",
    "                                              test_x_imp], axis=1),\n",
    "                                              columns=cols)\n",
    "\n",
    "# put R^2 evaluations for train and test datasets into same pandas dataframe\n",
    "var_df = pd.DataFrame(np.array(cols[2:] + [\"Weighted_Mean_R2\"]).reshape(len(cols)-2+1, -1), columns=[\"Variable\"])\n",
    "train_imp_df = pd.concat([train1_imp_df, train2_imp_df])\n",
    "\n",
    "# recombine partial train sets (both imputed) into single train set\n",
    "train_imp_df = train_imp_df.reset_index().sort_values([\"site\", \"index\"])\n",
    "train_imp_df.drop(\"index\", axis=1, inplace=True)\n",
    "train_imp_df.reset_index(inplace=True, drop=True)\n",
    "\n",
    "# save evaluations\n",
    "#pickle.dump(rf_imputer, open(\"rfV_imputer.pkl\", \"wb\"))\n",
    "\n",
    "if args_val:\n",
    "    # split val into x, y, and site\n",
    "    val_x, val_y, val_sites = X_y_site_split(val, y_var_name=\"MonitorData\", site_var_name=\"site\")\n",
    "\n",
    "    ### make imputations on val data matrix and create dataframe with imputation R^2 evaluations; computed weighted R^2 values\n",
    "    val_x_imp, val_r2_scores_df = rf_imputer.transform(val_x, evaluate = True, backup_impute_strategy = \"mean\")\n",
    "    val_r2_scores_df.columns = [\"Val_R2\", \"Val_num_missing\"]\n",
    "    val_r2_scores_df.loc[max(val_r2_scores_df.index)+1, :] = [np.average(val_r2_scores_df.loc[:, \"Val_R2\"].values,\n",
    "                                                                       weights = val_r2_scores_df.loc[:, \"Val_num_missing\"].values,\n",
    "                                                                       axis=0), np.mean(val_r2_scores_df.loc[:, \"Val_num_missing\"].values)]\n",
    "\n",
    "    ### convert imputed val data matrix back into pandas dataframes with column names\n",
    "    val_imp_df = pd.DataFrame(np.concatenate([val_sites.values.reshape(len(val_sites), -1),\n",
    "                                                  val_y.values.reshape(len(val_y), -1),\n",
    "                                                  val_x_imp], axis=1),\n",
    "                                                  columns=cols)\n",
    "\n",
    "    # save imputed datasets\n",
    "    train_imp_df.to_csv(config[\"RF_Imputation\"][\"trainV\"], index=False)\n",
    "    val_imp_df.to_csv(config[\"RF_Imputation\"][\"valV\"], index=False)\n",
    "    test_imp_df.to_csv(config[\"RF_Imputation\"][\"testV\"], index=False)\n",
    "\n",
    "    # put R^2 evaluations for train, val, and test datasets into same pandas dataframe\n",
    "    r2_scores_df = pd.concat([var_df, train1_r2_scores_df, train2_r2_scores_df, val_r2_scores_df, test_r2_scores_df], axis=1)\n",
    "\n",
    "    # save evaluations\n",
    "    r2_scores_df.to_csv(config[\"RF_Imputation\"][\"r2_scores\"], index=False)\n",
    "\n",
    "\n",
    "else:\n",
    "    train_imp_df.to_csv(config[\"RF_Imputation\"][\"train\"], index=False)\n",
    "    test_imp_df.to_csv(config[\"RF_Imputation\"][\"test\"], index=False)\n",
    "\n",
    "    # put R^2 evaluations for train and test datasets into same pandas dataframe and save\n",
    "    r2_scores_df = pd.concat([var_df, train1_r2_scores_df, train2_r2_scores_df, test_r2_scores_df], axis=1)\n",
    "    r2_scores_df.to_csv(config[\"RF_Imputation\"][\"r2_scores\"], index=False)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Making Pollution Predictions with Ridge Regression \n",
    "Note: the process is identical for random forest and XGBoost. Simply change the argument 'model'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cross-validation\n",
    "We use cross validation to tune any of ridge regression, random forest, or XGBoost models. Below, we use cross validation to tune a ridge regression predictor. Optimal model hyperparamters are stored ('pickled') as a dict type, and then used when training the model in the next section \n",
    "- the following is implemented in $\\texttt{model_cross_validation.py}$\n",
    "- to run in the shell, use $\\texttt{model-cross-val.sh}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "args_model = \"ridge\"\n",
    "args_n_folds = 3\n",
    "args_dataset = 'ridgeImp' #Dataset imputed with ridge regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import sklearn.linear_model\n",
    "import sklearn.ensemble\n",
    "import sklearn.metrics\n",
    "import sys\n",
    "import xgboost as xgb\n",
    "# these are imported functions created for this package that involve splitting datasets or performing cross-validation\n",
    "# see data_split_tune_utils.py\n",
    "from data_split_tune_utils import cross_validation_splits, X_y_site_split, cross_validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training model \"ridge\" on dataset \"ridgeImp\"...)\n",
      "Cross-validation R^2: 0.737384561994\n",
      "Best hyper-parameters: {'alpha': 0.01}\n"
     ]
    }
   ],
   "source": [
    "if args_n_folds < 1:\n",
    "    print(\"n_folds must be at least 1!\")\n",
    "    sys.exit()\n",
    "\n",
    "train = None\n",
    "\n",
    "if args_dataset == \"ridgeImp\":\n",
    "    train = pd.read_csv(config[\"Ridge_Imputation\"][\"train\"])\n",
    "\n",
    "\n",
    "elif args_dataset == \"rfImp\":\n",
    "    train = pd.read_csv(config[\"RF_Imputation\"][\"train\"])\n",
    "\n",
    "\n",
    "if train.empty: # failsafe\n",
    "    print(\"Invalid dataset!\")\n",
    "    sys.exit()\n",
    "\n",
    "\n",
    "# set seed for reproducibility\n",
    "np.random.seed(1)\n",
    "\n",
    "# drop rows with no monitor data response value\n",
    "train = train.dropna(axis=0)\n",
    "\n",
    "print(\"Training model \\\"{}\\\" on dataset \\\"{}\\\"...)\".format(args_model, args_dataset))\n",
    "\n",
    "if args_model == \"ridge\":\n",
    "    # instantiate ridge\n",
    "    ridge = sklearn.linear_model.Ridge(random_state=1, normalize=True, fit_intercept=True)\n",
    "\n",
    "    # ridge hyper-parameters to test in cross-validation\n",
    "    parameter_grid_ridge = {\"alpha\" : [0.1, 0.01, 0.001, 0.0001, 0.00001]}\n",
    "\n",
    "    # run cross-validation\n",
    "    cv_r2, best_hyperparams = cross_validation(data=train, model=ridge, hyperparam_dict=parameter_grid_ridge, num_folds=args_n_folds, y_var_name=\"MonitorData\", site_var_name=\"site\")\n",
    "    print(\"Cross-validation R^2: \" + str(cv_r2))\n",
    "    print(\"Best hyper-parameters: \" + str(best_hyperparams))\n",
    "    \n",
    "    # save dictionary with best ridge hyper-parameter combination\n",
    "    pickle.dump(best_hyperparams, open(config[\"Reg_Best_Hyperparams\"][\"ridge\"], \"wb\"))\n",
    "\n",
    "elif args_model == \"rf\":\n",
    "    # instantiate random forest\n",
    "    rf = sklearn.ensemble.RandomForestRegressor(n_estimators=200, random_state=1, n_jobs=-1)\n",
    "\n",
    "    # random forest hyper-parameters to test in cross-validation\n",
    "    parameter_grid_rf = {\"max_features\" : [10, 15, 20, 25]}\n",
    "\n",
    "    # run cross-validation\n",
    "    cv_r2, best_hyperparams = cross_validation(data=train, model=rf, hyperparam_dict=parameter_grid_rf, num_folds=args_n_folds, y_var_name=\"MonitorData\", site_var_name=\"site\")\n",
    "    print(\"Cross-validation R^2: \" + str(cv_r2))\n",
    "    print(\"Best hyper-parameters: \" + str(best_hyperparams))\n",
    "\n",
    "    # save dictionary with best random forest hyper-parameter combination\n",
    "    pickle.dump(best_hyperparams, open(config[\"Reg_Best_Hyperparams\"][\"rf\"], \"wb\"))\n",
    "\n",
    "elif args_model == \"xgb\":\n",
    "    # instantiate xgboost\n",
    "    xgboost = xgb.XGBRegressor(random_state=1, n_jobs=-1)\n",
    "\n",
    "    #Information on gradient boosting parameter tuning\n",
    "    #https://machinelearningmastery.com/configure-gradient-boosting-algorithm\n",
    "    # xgboost hyper-parameters to test in cross-validation\n",
    "    parameter_grid_xgboost = {\"learning_rate\": [0.001, 0.01, 0.05, 0.1], \"max_depth\": [4, 6, 8, 10], \"n_estimators\": [100, 250, 500, 750, 1000]}\n",
    "\n",
    "    # run cross-validation\n",
    "    cv_r2, best_hyperparams = cross_validation(data=train, model=xgboost, hyperparam_dict=parameter_grid_xgboost, num_folds=4, y_var_name=\"MonitorData\", site_var_name=\"site\")\n",
    "    print(\"Cross-validation R^2: \" + str(cv_r2))\n",
    "    print(\"Best hyper-parameters: \" + str(best_hyperparams))\n",
    "\n",
    "    # save dictionary with best xgboost hyper-parameter combination\n",
    "    pickle.dump(best_hyperparams, open(config[\"Reg_Best_Hyperparams\"][\"xgb\"], \"wb\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feed the cross-validation hyper-parameters to our train/test script\n",
    "- optimal hyper parameters from cross-validation are pickled in a dict, and loaded into the following training code\n",
    "- the following is implemented in $\\texttt{final_train_test.py}$\n",
    "- if doing random forest, feature importances are stored "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Command Line Arguments: \n",
    "args_model = 'ridge'\n",
    "args_dataset = 'ridgeImp'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training model \"ridge\" on dataset \"ridgeImp\"...)\n",
      "Test R^2: 0.682429611604\n"
     ]
    }
   ],
   "source": [
    "import sklearn.linear_model\n",
    "import sklearn.ensemble\n",
    "import sklearn.metrics\n",
    "import sys\n",
    "import xgboost as xgb\n",
    "# this is an imported functions created for this package that splits datasets (see data_split_tune_utils.py)\n",
    "from data_split_tune_utils import X_y_site_split\n",
    "\n",
    "models = [\"ridge\", \"rf\", \"xgb\"]\n",
    "datasets = [\"ridgeImp\", \"rfImp\"]\n",
    "\n",
    "if args_model not in models:\n",
    "    print(\"Invalid regression model!\")\n",
    "    sys.exit()\n",
    "\n",
    "\n",
    "if args_dataset not in datasets:\n",
    "    print(\"Invalid dataset!\")\n",
    "    sys.exit()\n",
    "\n",
    "\n",
    "train = None\n",
    "test  = None\n",
    "\n",
    "\n",
    "if args_dataset == \"ridgeImp\":\n",
    "    train = pd.read_csv(config[\"Ridge_Imputation\"][\"train\"])\n",
    "    test  = pd.read_csv(config[\"Ridge_Imputation\"][\"test\"])\n",
    "\n",
    "\n",
    "elif args_dataset == \"rfImp\":\n",
    "    train = pd.read_csv(config[\"RF_Imputation\"][\"train\"])\n",
    "    test  = pd.read_csv(config[\"RF_Imputation\"][\"test\"])\n",
    "\n",
    "if train.empty or test.empty: # failsafe\n",
    "    print(\"Invalid dataset!\")\n",
    "    sys.exit()\n",
    "\n",
    "\n",
    "# drop rows with no monitor data response value\n",
    "train = train.dropna(axis=0)\n",
    "test = test.dropna(axis=0)\n",
    "\n",
    "# split train and test datasets into x, y, and site\n",
    "train_x, train_y, train_sites = X_y_site_split(train, y_var_name=\"MonitorData\", site_var_name=\"site\")\n",
    "test_x, test_y, test_sites = X_y_site_split(test, y_var_name=\"MonitorData\", site_var_name=\"site\")\n",
    "\n",
    "\n",
    "print(\"Training model \\\"{}\\\" on dataset \\\"{}\\\"...)\".format(args_model, args_dataset))\n",
    "\n",
    "if args_model == \"ridge\":\n",
    "    # load dictionary with best ridge hyper-parameters from cross-validation\n",
    "    best_hyperparams = pickle.load(open(config[\"Reg_Best_Hyperparams\"][\"ridge\"], \"rb\"))\n",
    "\n",
    "    # instantiate ridge\n",
    "    ridge = sklearn.linear_model.Ridge(random_state=1, normalize=True, fit_intercept=True)\n",
    "\n",
    "    # set ridge attributes to best combination of hyper-parameters from cross-validation\n",
    "    for key in list(best_hyperparams.keys()):\n",
    "        setattr(ridge, key, best_hyperparams[key])\n",
    "\n",
    "    # fit ridge on train data and make predictions on test data; compute test R^2\n",
    "    ridge.fit(train_x, train_y)\n",
    "    test_pred_ridge = ridge.predict(test_x)\n",
    "    test_r2_ridge = sklearn.metrics.r2_score(test_y, test_pred_ridge)\n",
    "    print(\"Test R^2: \" + str(test_r2_ridge))\n",
    "\n",
    "    # put ridge predictions into test dataframe (note that these predictions do not include those for rows where there is no response value)\n",
    "    # save ridge predictions and fitted ridge model\n",
    "    test[\"MonitorData_pred\"] = pd.Series(test_pred_ridge, index=test.index)\n",
    "    test.to_csv(config[\"Regression\"][\"ridge_pred\"], index=False)\n",
    "    pickle.dump(ridge, open(config[\"Regression\"][\"ridge_final\"], \"wb\"))\n",
    "\n",
    "\n",
    "elif args_model == \"rf\":\n",
    "    # load dictionary with best random forest hyper-parameters from cross-validation\n",
    "    best_hyperparams = pickle.load(open(config[\"Reg_Best_Hyperparams\"][\"rf\"], \"rb\"))\n",
    "\n",
    "    # instantiate random forest\n",
    "    rf = sklearn.ensemble.RandomForestRegressor(n_estimators=500, random_state=1, n_jobs=-1)\n",
    "\n",
    "    # set random forest attributes to best combination of hyper-parameters from cross-validation\n",
    "    for key in list(best_hyperparams.keys()):\n",
    "        setattr(rf, key, best_hyperparams[key])\n",
    "\n",
    "    # fit random forest on train data and make predictions on test data; compute test R^2\n",
    "    rf.fit(train_x, train_y)\n",
    "    test_pred_rf = rf.predict(test_x)\n",
    "    test_r2_rf = sklearn.metrics.r2_score(test_y, test_pred_rf)\n",
    "    print(\"Test R^2: \" + str(test_r2_rf))\n",
    "\n",
    "    # put random forest predictions into test dataframe (note that these predictions do not include those for rows where there is no response value)\n",
    "    # save random forest predictions; don't save fitted random forest model due to memory\n",
    "    \n",
    "    test[\"MonitorData_pred\"] = pd.Series(test_pred_rf, index=test.index)\n",
    "    test.to_csv(\"../data/test_rfPred.csv\", index=False)\n",
    "    #pickle.dump(rf, open(\"rf_final.pkl\", \"wb\"))\n",
    "\n",
    "    # create dataframe of random forest feature importances and save\n",
    "    feature_importance_df = pd.DataFrame(rf.feature_importances_.reshape(len(rf.feature_importances_), -1), columns=[\"RF_Feature_Importance\"])\n",
    "    feature_importance_df[\"Variable\"] = pd.Series(train_x.columns, index=feature_importance_df.index)\n",
    "    feature_importance_df.to_csv(config[\"Regression\"][\"rf_ftImp\"], index=False)\n",
    "\n",
    "elif args_model == \"xgb\":\n",
    "    # load dictionary with best xgboost hyper-parameters from cross-validation\n",
    "    best_hyperparams = pickle.load(open(config[\"Reg_Best_Hyperparams\"][\"xgb\"], \"rb\"))\n",
    "\n",
    "    # instantiate xgboost\n",
    "    xgboost = xgb.XGBRegressor(random_state=1, n_jobs=-1)\n",
    "\n",
    "    # set xgboost attributes to best combination of hyper-parameters from cross-validation\n",
    "    for key in list(best_hyperparams.keys()):\n",
    "        setattr(xgboost, key, best_hyperparams[key])\n",
    "\n",
    "    # fit xgboost on train data and make predictions on test data; compute test R^2\n",
    "    xgboost.fit(train_x, train_y)\n",
    "    test_pred_xgboost = xgboost.predict(test_x)\n",
    "    test_r2_xgboost = sklearn.metrics.r2_score(test_y, test_pred_xgboost)\n",
    "    print(\"Test R^2: \" + str(test_r2_xgboost))\n",
    "\n",
    "    # put xgboost predictions into test dataframe (note that these predictions do not include those for rows where there is no response value)\n",
    "    # save xgboost predictions; don't save fitted xgboost model due to memory\n",
    "    test[\"MonitorData_pred\"] = pd.Series(test_pred_xgboost, index=test.index)\n",
    "    test.to_csv(config[\"Regression\"][\"xgb_pred\"], index=False)\n",
    "    #pickle.dump(xgboost, open(\"xgboost_final.pkl\", \"wb\"))\n",
    "\n",
    "    # create dataframe of xgboost feature importances and save\n",
    "    feature_importance_df = pd.DataFrame(xgboost.feature_importances_.reshape(len(xgboost.feature_importances_), -1), columns=[\"XGBoost_Feature_Importance\"])\n",
    "    feature_importance_df[\"Variable\"] = pd.Series(train_x.columns, index=feature_importance_df.index)\n",
    "    feature_importance_df.to_csv(config[\"Regression\"][\"xgb_ftImp\"], index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
