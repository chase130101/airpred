{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predicting $PM_{2.5}$ with our CNN Architecture \n",
    "\n",
    "The following tutorial explains how to use our CNN architecture. To be more transparent, each cell contains copied code from our top-level python scripts. However, in practice you should only need to run their associated $\\texttt{.sh}$ files. To make the tutorial functional on a PC, we use a curated data subset that only takes 100 days of each sensor (found in the '../data' folder).\n",
    "\n",
    "The tutorial is broken into the following steps: \n",
    "- 1) Load data\n",
    "- 2) Making Train/Test/Validation Split\n",
    "- 3) Imputing data with ridge regression or random forest variants of MissForest\n",
    "- 4) Determining optimal CNN hyper-parameters \n",
    "- 5) Training and testing one of the two CNN architectures \n",
    "\n",
    "Each step (in bold markdown) contains a description of the code, the name of the python script, the name of the shell script in $\\texttt{.../src}$, and is followed by 3 cells: \n",
    "- 1) arguments for the associated script. Each argument should be passed to the associated shell script in the command line. Here, the arguments are all python variables with the format \"$\\texttt{args_{argument name}}$\".\n",
    "- 2) The imported dependencies required for that step \n",
    "- 3) A copy of the top-level python script found in the directory $\\texttt{src}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import configparser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['./config/py_config.ini']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config = configparser.RawConfigParser()\n",
    "config.read(\"./config/py_config.ini\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import 100 Day Data Subset: \n",
    " - to produce subset: \n",
    "  - execute in command line $\\texttt{Rscript data_setup.R}$\n",
    "  - note: the data path variables are set in $\\texttt{/src/config/data_setup_config.R}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import preprocessed data\n",
    "dat = pd.read_csv('../data/data_to_impute.csv')\n",
    "#Drop useless index column \n",
    "dat = dat.iloc[:, 1:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Splitting the Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make Train/Test/Val Split. Train/Test/Val is necessary for CNN prediction models, whereas only train and test are required for other prediction models. The other models can use cross-validation for tuning. \n",
    "With our script, a given site will be all in train or all in test, in order to make a fair test of predictive capability\n",
    "\n",
    " - train/test/val split is executed by $\\texttt{train_val_test_split.py}$\n",
    " - to execute, use $\\texttt{tvt-split.sh}$ with necessary arguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Shell arguments for tvt_split.sh, which calls train_val_test_split.py: \n",
    "args_val = True\n",
    "args_train_split = 0.7\n",
    "args_val_split = 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#CODE FROM /src/train_val_test_split.py\n",
    "#Get utils \n",
    "import sys\n",
    "from data_split_tune_utils import train_test_split, train_val_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#CODE FROM /src/train_val_test_split.py\n",
    "if (not args_val) and args_val_split != 0:\n",
    "    print(\"Validation split specified without validation flag!\")\n",
    "    sys.exit()\n",
    "\n",
    "\n",
    "if args_train_split < 0 or args_train_split > 1 or \\\n",
    "   args_val_split < 0 or args_val_split > 1:\n",
    "    print(\"Split value out of range!\")\n",
    "    sys.exit()\n",
    "\n",
    "\n",
    "if args_train_split + args_val_split > 1:\n",
    "    print(\"Invalid train/validation split ratio! Must fall under a total of 1.\")\n",
    "    sys.exit()\n",
    "\n",
    "\n",
    "test_split = 1. - args_train_split - args_val_split\n",
    "\n",
    "\n",
    "data = pd.read_csv(config[\"data\"][\"data_to_impute\"])\n",
    "\n",
    "# set seed for reproducibility\n",
    "np.random.seed(1)\n",
    "\n",
    "if args_val: # split data into train, validation, and test sets and save\n",
    "    train, val, test = train_val_test_split(data, train_prop=args_train_split, test_prop=test_split, site_var_name=\"site\")\n",
    "    train.to_csv(config[\"data\"][\"trainV\"], index=False)\n",
    "    val.to_csv(  config[\"data\"][\"valV\"], index=False)\n",
    "    test.to_csv( config[\"data\"][\"testV\"], index=False)\n",
    "\n",
    "\n",
    "else: # split data into train and test sets and save\n",
    "    train, test = train_test_split(data, train_prop=args_train_split, site_var_name=\"site\")\n",
    "    train.to_csv(config[\"data\"][\"train\"], index=False)\n",
    "    test.to_csv(config[\"data\"][\"test\"], index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Size:  (145500, 62)\n",
      "Test Size:  (41600, 62)\n"
     ]
    }
   ],
   "source": [
    "print('Training Size: ', train.shape)\n",
    "print('Test Size: ', test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imputing Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Impute data with ridge regression variant of MissForest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### First, fit ridge regression imputation models\n",
    "- The code 'pickles' the trained ridge imputation model, so you don't need to train every time you wish to impute. Furthermore, you are able to train the imputer on a subset of data (to save time). The next section will use the trained imputer to actually impute the data\n",
    "- the following is implemented in $\\texttt{ridge_imputer_fit.py}$\n",
    "- to run in shell, use $\\texttt{ridge-imputer-fit.sh}$ with appropriate args"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#shell arguments to ridge-imputer-fit.sh, which calls ridge_imputer_fit.py: \n",
    "args_val = True \n",
    "args_impute_split = 0.5\n",
    "args_max_iter = 10\n",
    "args_initial_strategy = 'mean'\n",
    "args_alpha = 0.0001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#CODE FROM ridge_imputer_fit.py\n",
    "import pickle \n",
    "from data_split_tune_utils import train_test_split, X_y_site_split\n",
    "from predictiveImputer_mod import PredictiveImputer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of variables: 60\n",
      "Iteration 1\n",
      "Variable # 0\n",
      "Variable # 1\n",
      "Variable # 2\n",
      "Variable # 3\n",
      "Variable # 4\n",
      "Variable # 5\n",
      "Variable # 6\n",
      "Variable # 7\n",
      "Variable # 8\n",
      "Variable # 9\n",
      "Variable # 10\n",
      "Variable # 11\n",
      "Variable # 12\n",
      "Variable # 13\n",
      "Variable # 14\n",
      "Variable # 15\n",
      "Variable # 16\n",
      "Variable # 17\n",
      "Variable # 18\n",
      "Variable # 19\n",
      "Variable # 20\n",
      "Variable # 21\n",
      "Variable # 22\n",
      "Variable # 23\n",
      "Variable # 24\n",
      "Variable # 25\n",
      "Variable # 26\n",
      "Variable # 27\n",
      "Variable # 28\n",
      "Variable # 29\n",
      "Variable # 30\n",
      "Variable # 31\n",
      "Variable # 32\n",
      "Variable # 33\n",
      "Variable # 34\n",
      "Variable # 35\n",
      "Variable # 36\n",
      "Variable # 37\n",
      "Variable # 38\n",
      "Variable # 39\n",
      "Variable # 40\n",
      "Variable # 41\n",
      "Variable # 42\n",
      "Variable # 43\n",
      "Variable # 44\n",
      "Variable # 45\n",
      "Variable # 46\n",
      "Variable # 47\n",
      "Variable # 48\n",
      "Variable # 49\n",
      "Variable # 50\n",
      "Variable # 51\n",
      "Variable # 52\n",
      "Variable # 53\n",
      "Variable # 54\n",
      "Variable # 55\n",
      "Variable # 56\n",
      "Variable # 57\n",
      "Variable # 58\n",
      "Variable # 59\n",
      "Difference: 0.000688761218348\n",
      "\n",
      "Iteration 2\n",
      "Variable # 0\n",
      "Variable # 1\n",
      "Variable # 2\n",
      "Variable # 3\n",
      "Variable # 4\n",
      "Variable # 5\n",
      "Variable # 6\n",
      "Variable # 7\n",
      "Variable # 8\n",
      "Variable # 9\n",
      "Variable # 10\n",
      "Variable # 11\n",
      "Variable # 12\n",
      "Variable # 13\n",
      "Variable # 14\n",
      "Variable # 15\n",
      "Variable # 16\n",
      "Variable # 17\n",
      "Variable # 18\n",
      "Variable # 19\n",
      "Variable # 20\n",
      "Variable # 21\n",
      "Variable # 22\n",
      "Variable # 23\n",
      "Variable # 24\n",
      "Variable # 25\n",
      "Variable # 26\n",
      "Variable # 27\n",
      "Variable # 28\n",
      "Variable # 29\n",
      "Variable # 30\n",
      "Variable # 31\n",
      "Variable # 32\n",
      "Variable # 33\n",
      "Variable # 34\n",
      "Variable # 35\n",
      "Variable # 36\n",
      "Variable # 37\n",
      "Variable # 38\n",
      "Variable # 39\n",
      "Variable # 40\n",
      "Variable # 41\n",
      "Variable # 42\n",
      "Variable # 43\n",
      "Variable # 44\n",
      "Variable # 45\n",
      "Variable # 46\n",
      "Variable # 47\n",
      "Variable # 48\n",
      "Variable # 49\n",
      "Variable # 50\n",
      "Variable # 51\n",
      "Variable # 52\n",
      "Variable # 53\n",
      "Variable # 54\n",
      "Variable # 55\n",
      "Variable # 56\n",
      "Variable # 57\n",
      "Variable # 58\n",
      "Variable # 59\n",
      "Difference: 1.1812879841e-05\n",
      "\n",
      "Iteration 3\n",
      "Variable # 0\n",
      "Variable # 1\n",
      "Variable # 2\n",
      "Variable # 3\n",
      "Variable # 4\n",
      "Variable # 5\n",
      "Variable # 6\n",
      "Variable # 7\n",
      "Variable # 8\n",
      "Variable # 9\n",
      "Variable # 10\n",
      "Variable # 11\n",
      "Variable # 12\n",
      "Variable # 13\n",
      "Variable # 14\n",
      "Variable # 15\n",
      "Variable # 16\n",
      "Variable # 17\n",
      "Variable # 18\n",
      "Variable # 19\n",
      "Variable # 20\n",
      "Variable # 21\n",
      "Variable # 22\n",
      "Variable # 23\n",
      "Variable # 24\n",
      "Variable # 25\n",
      "Variable # 26\n",
      "Variable # 27\n",
      "Variable # 28\n",
      "Variable # 29\n",
      "Variable # 30\n",
      "Variable # 31\n",
      "Variable # 32\n",
      "Variable # 33\n",
      "Variable # 34\n",
      "Variable # 35\n",
      "Variable # 36\n",
      "Variable # 37\n",
      "Variable # 38\n",
      "Variable # 39\n",
      "Variable # 40\n",
      "Variable # 41\n",
      "Variable # 42\n",
      "Variable # 43\n",
      "Variable # 44\n",
      "Variable # 45\n",
      "Variable # 46\n",
      "Variable # 47\n",
      "Variable # 48\n",
      "Variable # 49\n",
      "Variable # 50\n",
      "Variable # 51\n",
      "Variable # 52\n",
      "Variable # 53\n",
      "Variable # 54\n",
      "Variable # 55\n",
      "Variable # 56\n",
      "Variable # 57\n",
      "Variable # 58\n",
      "Variable # 59\n",
      "Difference: 3.0202213298e-06\n",
      "\n",
      "Iteration 4\n",
      "Variable # 0\n",
      "Variable # 1\n",
      "Variable # 2\n",
      "Variable # 3\n",
      "Variable # 4\n",
      "Variable # 5\n",
      "Variable # 6\n",
      "Variable # 7\n",
      "Variable # 8\n",
      "Variable # 9\n",
      "Variable # 10\n",
      "Variable # 11\n",
      "Variable # 12\n",
      "Variable # 13\n",
      "Variable # 14\n",
      "Variable # 15\n",
      "Variable # 16\n",
      "Variable # 17\n",
      "Variable # 18\n",
      "Variable # 19\n",
      "Variable # 20\n",
      "Variable # 21\n",
      "Variable # 22\n",
      "Variable # 23\n",
      "Variable # 24\n",
      "Variable # 25\n",
      "Variable # 26\n",
      "Variable # 27\n",
      "Variable # 28\n",
      "Variable # 29\n",
      "Variable # 30\n",
      "Variable # 31\n",
      "Variable # 32\n",
      "Variable # 33\n",
      "Variable # 34\n",
      "Variable # 35\n",
      "Variable # 36\n",
      "Variable # 37\n",
      "Variable # 38\n",
      "Variable # 39\n",
      "Variable # 40\n",
      "Variable # 41\n",
      "Variable # 42\n",
      "Variable # 43\n",
      "Variable # 44\n",
      "Variable # 45\n",
      "Variable # 46\n",
      "Variable # 47\n",
      "Variable # 48\n",
      "Variable # 49\n",
      "Variable # 50\n",
      "Variable # 51\n",
      "Variable # 52\n",
      "Variable # 53\n",
      "Variable # 54\n",
      "Variable # 55\n",
      "Variable # 56\n",
      "Variable # 57\n",
      "Variable # 58\n",
      "Variable # 59\n",
      "Difference: 7.9055068219e-07\n",
      "\n",
      "Iteration 5\n",
      "Variable # 0\n",
      "Variable # 1\n",
      "Variable # 2\n",
      "Variable # 3\n",
      "Variable # 4\n",
      "Variable # 5\n",
      "Variable # 6\n",
      "Variable # 7\n",
      "Variable # 8\n",
      "Variable # 9\n",
      "Variable # 10\n",
      "Variable # 11\n",
      "Variable # 12\n",
      "Variable # 13\n",
      "Variable # 14\n",
      "Variable # 15\n",
      "Variable # 16\n",
      "Variable # 17\n",
      "Variable # 18\n",
      "Variable # 19\n",
      "Variable # 20\n",
      "Variable # 21\n",
      "Variable # 22\n",
      "Variable # 23\n",
      "Variable # 24\n",
      "Variable # 25\n",
      "Variable # 26\n",
      "Variable # 27\n",
      "Variable # 28\n",
      "Variable # 29\n",
      "Variable # 30\n",
      "Variable # 31\n",
      "Variable # 32\n",
      "Variable # 33\n",
      "Variable # 34\n",
      "Variable # 35\n",
      "Variable # 36\n",
      "Variable # 37\n",
      "Variable # 38\n",
      "Variable # 39\n",
      "Variable # 40\n",
      "Variable # 41\n",
      "Variable # 42\n",
      "Variable # 43\n",
      "Variable # 44\n",
      "Variable # 45\n",
      "Variable # 46\n",
      "Variable # 47\n",
      "Variable # 48\n",
      "Variable # 49\n",
      "Variable # 50\n",
      "Variable # 51\n",
      "Variable # 52\n",
      "Variable # 53\n",
      "Variable # 54\n",
      "Variable # 55\n",
      "Variable # 56\n",
      "Variable # 57\n",
      "Variable # 58\n",
      "Variable # 59\n",
      "Difference: 4.69660423831e-07\n",
      "\n",
      "Iteration 6\n",
      "Variable # 0\n",
      "Variable # 1\n",
      "Variable # 2\n",
      "Variable # 3\n",
      "Variable # 4\n",
      "Variable # 5\n",
      "Variable # 6\n",
      "Variable # 7\n",
      "Variable # 8\n",
      "Variable # 9\n",
      "Variable # 10\n",
      "Variable # 11\n",
      "Variable # 12\n",
      "Variable # 13\n",
      "Variable # 14\n",
      "Variable # 15\n",
      "Variable # 16\n",
      "Variable # 17\n",
      "Variable # 18\n",
      "Variable # 19\n",
      "Variable # 20\n",
      "Variable # 21\n",
      "Variable # 22\n",
      "Variable # 23\n",
      "Variable # 24\n",
      "Variable # 25\n",
      "Variable # 26\n",
      "Variable # 27\n",
      "Variable # 28\n",
      "Variable # 29\n",
      "Variable # 30\n",
      "Variable # 31\n",
      "Variable # 32\n",
      "Variable # 33\n",
      "Variable # 34\n",
      "Variable # 35\n",
      "Variable # 36\n",
      "Variable # 37\n",
      "Variable # 38\n",
      "Variable # 39\n",
      "Variable # 40\n",
      "Variable # 41\n",
      "Variable # 42\n",
      "Variable # 43\n",
      "Variable # 44\n",
      "Variable # 45\n",
      "Variable # 46\n",
      "Variable # 47\n",
      "Variable # 48\n",
      "Variable # 49\n",
      "Variable # 50\n",
      "Variable # 51\n",
      "Variable # 52\n",
      "Variable # 53\n",
      "Variable # 54\n",
      "Variable # 55\n",
      "Variable # 56\n",
      "Variable # 57\n",
      "Variable # 58\n",
      "Variable # 59\n",
      "Difference: 4.41268679602e-07\n",
      "\n",
      "Iteration 7\n",
      "Variable # 0\n",
      "Variable # 1\n",
      "Variable # 2\n",
      "Variable # 3\n",
      "Variable # 4\n",
      "Variable # 5\n",
      "Variable # 6\n",
      "Variable # 7\n",
      "Variable # 8\n",
      "Variable # 9\n",
      "Variable # 10\n",
      "Variable # 11\n",
      "Variable # 12\n",
      "Variable # 13\n",
      "Variable # 14\n",
      "Variable # 15\n",
      "Variable # 16\n",
      "Variable # 17\n",
      "Variable # 18\n",
      "Variable # 19\n",
      "Variable # 20\n",
      "Variable # 21\n",
      "Variable # 22\n",
      "Variable # 23\n",
      "Variable # 24\n",
      "Variable # 25\n",
      "Variable # 26\n",
      "Variable # 27\n",
      "Variable # 28\n",
      "Variable # 29\n",
      "Variable # 30\n",
      "Variable # 31\n",
      "Variable # 32\n",
      "Variable # 33\n",
      "Variable # 34\n",
      "Variable # 35\n",
      "Variable # 36\n",
      "Variable # 37\n",
      "Variable # 38\n",
      "Variable # 39\n",
      "Variable # 40\n",
      "Variable # 41\n",
      "Variable # 42\n",
      "Variable # 43\n",
      "Variable # 44\n",
      "Variable # 45\n",
      "Variable # 46\n",
      "Variable # 47\n",
      "Variable # 48\n",
      "Variable # 49\n",
      "Variable # 50\n",
      "Variable # 51\n",
      "Variable # 52\n",
      "Variable # 53\n",
      "Variable # 54\n",
      "Variable # 55\n",
      "Variable # 56\n",
      "Variable # 57\n",
      "Variable # 58\n",
      "Variable # 59\n",
      "Difference: 3.60194683575e-07\n",
      "\n",
      "Iteration 8\n",
      "Variable # 0\n",
      "Variable # 1\n",
      "Variable # 2\n",
      "Variable # 3\n",
      "Variable # 4\n",
      "Variable # 5\n",
      "Variable # 6\n",
      "Variable # 7\n",
      "Variable # 8\n",
      "Variable # 9\n",
      "Variable # 10\n",
      "Variable # 11\n",
      "Variable # 12\n",
      "Variable # 13\n",
      "Variable # 14\n",
      "Variable # 15\n",
      "Variable # 16\n",
      "Variable # 17\n",
      "Variable # 18\n",
      "Variable # 19\n",
      "Variable # 20\n",
      "Variable # 21\n",
      "Variable # 22\n",
      "Variable # 23\n",
      "Variable # 24\n",
      "Variable # 25\n",
      "Variable # 26\n",
      "Variable # 27\n",
      "Variable # 28\n",
      "Variable # 29\n",
      "Variable # 30\n",
      "Variable # 31\n",
      "Variable # 32\n",
      "Variable # 33\n",
      "Variable # 34\n",
      "Variable # 35\n",
      "Variable # 36\n",
      "Variable # 37\n",
      "Variable # 38\n",
      "Variable # 39\n",
      "Variable # 40\n",
      "Variable # 41\n",
      "Variable # 42\n",
      "Variable # 43\n",
      "Variable # 44\n",
      "Variable # 45\n",
      "Variable # 46\n",
      "Variable # 47\n",
      "Variable # 48\n",
      "Variable # 49\n",
      "Variable # 50\n",
      "Variable # 51\n",
      "Variable # 52\n",
      "Variable # 53\n",
      "Variable # 54\n",
      "Variable # 55\n",
      "Variable # 56\n",
      "Variable # 57\n",
      "Variable # 58\n",
      "Variable # 59\n",
      "Difference: 2.07768591876e-07\n",
      "\n",
      "Iteration 9\n",
      "Variable # 0\n",
      "Variable # 1\n",
      "Variable # 2\n",
      "Variable # 3\n",
      "Variable # 4\n",
      "Variable # 5\n",
      "Variable # 6\n",
      "Variable # 7\n",
      "Variable # 8\n",
      "Variable # 9\n",
      "Variable # 10\n",
      "Variable # 11\n",
      "Variable # 12\n",
      "Variable # 13\n",
      "Variable # 14\n",
      "Variable # 15\n",
      "Variable # 16\n",
      "Variable # 17\n",
      "Variable # 18\n",
      "Variable # 19\n",
      "Variable # 20\n",
      "Variable # 21\n",
      "Variable # 22\n",
      "Variable # 23\n",
      "Variable # 24\n",
      "Variable # 25\n",
      "Variable # 26\n",
      "Variable # 27\n",
      "Variable # 28\n",
      "Variable # 29\n",
      "Variable # 30\n",
      "Variable # 31\n",
      "Variable # 32\n",
      "Variable # 33\n",
      "Variable # 34\n",
      "Variable # 35\n",
      "Variable # 36\n",
      "Variable # 37\n",
      "Variable # 38\n",
      "Variable # 39\n",
      "Variable # 40\n",
      "Variable # 41\n",
      "Variable # 42\n",
      "Variable # 43\n",
      "Variable # 44\n",
      "Variable # 45\n",
      "Variable # 46\n",
      "Variable # 47\n",
      "Variable # 48\n",
      "Variable # 49\n",
      "Variable # 50\n",
      "Variable # 51\n",
      "Variable # 52\n",
      "Variable # 53\n",
      "Variable # 54\n",
      "Variable # 55\n",
      "Variable # 56\n",
      "Variable # 57\n",
      "Variable # 58\n",
      "Variable # 59\n",
      "Difference: 1.31043518155e-07\n",
      "\n",
      "Iteration 10\n",
      "Variable # 0\n",
      "Variable # 1\n",
      "Variable # 2\n",
      "Variable # 3\n",
      "Variable # 4\n",
      "Variable # 5\n",
      "Variable # 6\n",
      "Variable # 7\n",
      "Variable # 8\n",
      "Variable # 9\n",
      "Variable # 10\n",
      "Variable # 11\n",
      "Variable # 12\n",
      "Variable # 13\n",
      "Variable # 14\n",
      "Variable # 15\n",
      "Variable # 16\n",
      "Variable # 17\n",
      "Variable # 18\n",
      "Variable # 19\n",
      "Variable # 20\n",
      "Variable # 21\n",
      "Variable # 22\n",
      "Variable # 23\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Variable # 24\n",
      "Variable # 25\n",
      "Variable # 26\n",
      "Variable # 27\n",
      "Variable # 28\n",
      "Variable # 29\n",
      "Variable # 30\n",
      "Variable # 31\n",
      "Variable # 32\n",
      "Variable # 33\n",
      "Variable # 34\n",
      "Variable # 35\n",
      "Variable # 36\n",
      "Variable # 37\n",
      "Variable # 38\n",
      "Variable # 39\n",
      "Variable # 40\n",
      "Variable # 41\n",
      "Variable # 42\n",
      "Variable # 43\n",
      "Variable # 44\n",
      "Variable # 45\n",
      "Variable # 46\n",
      "Variable # 47\n",
      "Variable # 48\n",
      "Variable # 49\n",
      "Variable # 50\n",
      "Variable # 51\n",
      "Variable # 52\n",
      "Variable # 53\n",
      "Variable # 54\n",
      "Variable # 55\n",
      "Variable # 56\n",
      "Variable # 57\n",
      "Variable # 58\n",
      "Variable # 59\n",
      "Difference: 9.35447205916e-08\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#CODE FROM ridge_imputer_fit.py\n",
    "if args_impute_split < 0 or args_impute_split > 1:\n",
    "    print(\"Impute split value out of range!\")\n",
    "    sys.exit()\n",
    "\n",
    "\n",
    "train = None\n",
    "if args_val:\n",
    "    train = pd.read_csv(config[\"data\"][\"trainV\"])\n",
    "\n",
    "else:\n",
    "    train = pd.read_csv(config[\"data\"][\"train\"])\n",
    "\n",
    "\n",
    "# set seed for reproducibility\n",
    "np.random.seed(1)\n",
    "\n",
    "# split train up; only fit imputer on part of train set due to memory/time\n",
    "train1, train2 = train_test_split(train, train_prop=args_impute_split, site_var_name=\"site\")\n",
    "\n",
    "# split part of train set to fit imputer on into x, y, and site\n",
    "train1_x, train1_y, train1_sites = X_y_site_split(train1, y_var_name=\"MonitorData\", site_var_name=\"site\")\n",
    "\n",
    "# create imputer and fit on part of train set\n",
    "ridge_imputer = PredictiveImputer(max_iter=args_max_iter, initial_strategy=args_initial_strategy, f_model=\"Ridge\")\n",
    "ridge_imputer.fit(train1_x, alpha=args_alpha, fit_intercept=True, normalize=True, random_state=1)\n",
    "\n",
    "# save fitted imputer\n",
    "pickle.dump(ridge_imputer, open(config[\"Ridge_Imputation\"][\"model\"], \"wb\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Then, run the fitted ridge regression imputer to impute the data\n",
    "- the following is implemented in $\\texttt{ridge_impute_eval_train_val_test.py}$\n",
    "- to run in shell, use $\\texttt{ridge-impute-tt.sh}$ with appropriate arguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Shell arguments: \n",
    "args_val = True\n",
    "args_backup_strategy = 'mean'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#CODE FROM ridge_impute_eval_train_val_test.py\n",
    "import pickle\n",
    "# this imported function was created for this package to split datasets (see data_split_tune_utils.py)\n",
    "from data_split_tune_utils import X_y_site_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#CODE FROM ridge_impute_eval_train_val_test.py\n",
    "train, val, test = None, None, None\n",
    "\n",
    "if args_val:\n",
    "    train = pd.read_csv(config[\"data\"][\"trainV\"])\n",
    "    val   = pd.read_csv(config[\"data\"][\"valV\"])\n",
    "    test  = pd.read_csv(config[\"data\"][\"testV\"])\n",
    "\n",
    "else:\n",
    "    train = pd.read_csv(config[\"data\"][\"train\"])\n",
    "    test  = pd.read_csv(config[\"data\"][\"test\"])\n",
    "\n",
    "# set seed for reproducibility\n",
    "np.random.seed(1)\n",
    "\n",
    "# load fitted ridge imputer\n",
    "ridge_imputer = pickle.load(open(config[\"Ridge_Imputation\"][\"model\"], \"rb\"))\n",
    "\n",
    "# split train and test datasets into x, y, and site\n",
    "train_x, train_y, train_sites = X_y_site_split(train, y_var_name=\"MonitorData\", site_var_name=\"site\")\n",
    "test_x, test_y, test_sites = X_y_site_split(test, y_var_name=\"MonitorData\", site_var_name=\"site\")\n",
    "\n",
    "### make imputations on train and test data matrices and create dataframes with imputation R^2 evaluations; computed weighted R^2 values\n",
    "train_x_imp, train_r2_scores_df = ridge_imputer.transform(train_x, evaluate=True, backup_impute_strategy=args_backup_strategy)\n",
    "train_r2_scores_df.columns = [\"Train_R2\", \"Train_num_missing\"]\n",
    "train_r2_scores_df.loc[max(train_r2_scores_df.index)+1, :] = [np.average(train_r2_scores_df.loc[:, \"Train_R2\"].values,\n",
    "                                                                   weights=train_r2_scores_df.loc[:, \"Train_num_missing\"].values,\n",
    "                                                                   axis=0), np.mean(train_r2_scores_df.loc[:, \"Train_num_missing\"].values)]\n",
    "\n",
    "test_x_imp, test_r2_scores_df = ridge_imputer.transform(test_x, evaluate=True, backup_impute_strategy=args_backup_strategy)\n",
    "test_r2_scores_df.columns = [\"Test_R2\", \"Test_num_missing\"]\n",
    "test_r2_scores_df.loc[max(test_r2_scores_df.index)+1, :] = [np.average(test_r2_scores_df.loc[:, \"Test_R2\"].values,\n",
    "                                                                   weights=test_r2_scores_df.loc[:, \"Test_num_missing\"].values,\n",
    "                                                                   axis=0), np.mean(test_r2_scores_df.loc[:, \"Test_num_missing\"].values)]\n",
    "\n",
    "### convert imputed train and test data matrices back into pandas dataframes with column names\n",
    "cols = [\"site\", \"MonitorData\"] + list(train_x.columns)\n",
    "train_imp_df = pd.DataFrame(np.concatenate([train_sites.values.reshape(len(train_sites), -1),\n",
    "                                              train_y.values.reshape(len(train_y), -1),\n",
    "                                              train_x_imp], axis=1),\n",
    "                                              columns=cols)\n",
    "\n",
    "test_imp_df = pd.DataFrame(np.concatenate([test_sites.values.reshape(len(test_sites), -1),\n",
    "                                              test_y.values.reshape(len(test_y), -1),\n",
    "                                              test_x_imp], axis=1),\n",
    "                                              columns=cols)\n",
    "\n",
    "var_df = pd.DataFrame(np.array(cols[2:] + [\"Weighted_Mean_R2\"]).reshape(len(cols)-2+1, -1), columns=[\"Variable\"])\n",
    "\n",
    "\n",
    "\n",
    "if args_val:\n",
    "    # split val into x, y, and site\n",
    "    val_x, val_y, val_sites = X_y_site_split(val, y_var_name=\"MonitorData\", site_var_name=\"site\")\n",
    "\n",
    "    ### make imputations on val data matrix and create dataframe with imputation R^2 evaluations; computed weighted R^2 values\n",
    "    val_x_imp, val_r2_scores_df = ridge_imputer.transform(val_x, evaluate=True, backup_impute_strategy=\"mean\")\n",
    "    val_r2_scores_df.columns = [\"Val_R2\", \"Val_num_missing\"]\n",
    "    val_r2_scores_df.loc[max(val_r2_scores_df.index)+1, :] = [np.average(val_r2_scores_df.loc[:, \"Val_R2\"].values,\n",
    "                                                                       weights=val_r2_scores_df.loc[:, \"Val_num_missing\"].values,\n",
    "                                                                       axis=0), np.mean(val_r2_scores_df.loc[:, \"Val_num_missing\"].values)]\n",
    "\n",
    "    ### convert imputed val data matrix back into pandas dataframes with column names\n",
    "    val_imp_df = pd.DataFrame(np.concatenate([val_sites.values.reshape(len(val_sites), -1),\n",
    "                                                  val_y.values.reshape(len(val_y), -1),\n",
    "                                                  val_x_imp], axis=1),\n",
    "                                                  columns=cols)\n",
    "    # save imputed datasets\n",
    "    train_imp_df.to_csv(config[\"Ridge_Imputation\"][\"trainV\"], index=False)\n",
    "    test_imp_df.to_csv(config[\"Ridge_Imputation\"][\"testV\"], index=False)\n",
    "    val_imp_df.to_csv(config[\"Ridge_Imputation\"][\"valV\"], index=False)\n",
    "\n",
    "    # put R^2 evaluations for train, val, and test datasets into same pandas dataframe\n",
    "    r2_scores_df = pd.concat([var_df, train_r2_scores_df, val_r2_scores_df, test_r2_scores_df], axis=1)\n",
    "\n",
    "    # save evaluations\n",
    "    r2_scores_df.to_csv(config[\"Ridge_Imputation\"][\"r2_scores\"], index=False)\n",
    "    \n",
    "else:\n",
    "    # save imputed train and test datasets\n",
    "    train_imp_df.to_csv(config[\"Ridge_Imputation\"][\"train\"], index=False)\n",
    "    test_imp_df.to_csv(config[\"Ridge_Imputation\"][\"test\"], index=False)\n",
    "    # put R^2 evaluations for train and test datasets into same pandas dataframe and save\n",
    "    r2_scores_df = pd.concat([var_df, train_r2_scores_df, test_r2_scores_df], axis=1)\n",
    "    r2_scores_df.to_csv(config[\"Ridge_Imputation\"][\"r2_scores\"], index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Variable</th>\n",
       "      <th>Train_R2</th>\n",
       "      <th>Train_num_missing</th>\n",
       "      <th>Val_R2</th>\n",
       "      <th>Val_num_missing</th>\n",
       "      <th>Test_R2</th>\n",
       "      <th>Test_num_missing</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Unnamed: 0</td>\n",
       "      <td>0.078775</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.035756</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.007915</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>month</td>\n",
       "      <td>0.999993</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.999993</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.999993</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>cumulative_month</td>\n",
       "      <td>0.999993</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.999993</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.999993</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>sin_time</td>\n",
       "      <td>0.957092</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.954576</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.955320</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>cos_time</td>\n",
       "      <td>0.998618</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.998628</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.998622</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           Variable  Train_R2  Train_num_missing    Val_R2  Val_num_missing  \\\n",
       "0        Unnamed: 0  0.078775                0.0  0.035756              0.0   \n",
       "1             month  0.999993                0.0  0.999993              0.0   \n",
       "2  cumulative_month  0.999993                0.0  0.999993              0.0   \n",
       "3          sin_time  0.957092                0.0  0.954576              0.0   \n",
       "4          cos_time  0.998618                0.0  0.998628              0.0   \n",
       "\n",
       "    Test_R2  Test_num_missing  \n",
       "0  0.007915               0.0  \n",
       "1  0.999993               0.0  \n",
       "2  0.999993               0.0  \n",
       "3  0.955320               0.0  \n",
       "4  0.998622               0.0  "
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nrows = 5\n",
    "r2_scores_df.head(nrows)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Impute with Random Forest variant of MissForest\n",
    "- The random forest MissForest, while effective, struggles with memory. As such, the random forest imputation script does not save the fitted imputer (since saving the model object itself requires excessive memory). The same script both fits the imputer and makes the imputations.\n",
    "- The following is implemented in $\\texttt{rf_fit_impute_eval_train_val_test.py}$\n",
    "- To run, use $\\texttt{rf-impute-tvt.sh}$ with appropriate args"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Shell arguments for rf-impute-tvt.sh: \n",
    "args_val = True \n",
    "args_impute_split = 0.5 \n",
    "args_initial_strategy = 'mean'\n",
    "args_backup_strategy = 'median'\n",
    "args_max_iter = 2\n",
    "args_max_features = 5 \n",
    "args_n_estimators = 5 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#CODE FROM rf_fit_impute_eval_train_val_test.py\n",
    "# these are imported functions created for this package that split datasets (see data_split_tune_utils.py)\n",
    "from data_split_tune_utils import train_test_split, X_y_site_split, train_val_test_split\n",
    "# this is the PredictiveImputer class inspired by the MissForest algorithm (see predictiveImputer_mod.py)\n",
    "from predictiveImputer_mod import PredictiveImputer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of variables: 60\n",
      "Iteration 1\n",
      "Variable # 0\n",
      "Variable # 1\n",
      "Variable # 2\n",
      "Variable # 3\n",
      "Variable # 4\n",
      "Variable # 5\n",
      "Variable # 6\n",
      "Variable # 7\n",
      "Variable # 8\n",
      "Variable # 9\n",
      "Variable # 10\n",
      "Variable # 11\n",
      "Variable # 12\n",
      "Variable # 13\n",
      "Variable # 14\n",
      "Variable # 15\n",
      "Variable # 16\n",
      "Variable # 17\n",
      "Variable # 18\n",
      "Variable # 19\n",
      "Variable # 20\n",
      "Variable # 21\n",
      "Variable # 22\n",
      "Variable # 23\n",
      "Variable # 24\n",
      "Variable # 25\n",
      "Variable # 26\n",
      "Variable # 27\n",
      "Variable # 28\n",
      "Variable # 29\n",
      "Variable # 30\n",
      "Variable # 31\n",
      "Variable # 32\n",
      "Variable # 33\n",
      "Variable # 34\n",
      "Variable # 35\n",
      "Variable # 36\n",
      "Variable # 37\n",
      "Variable # 38\n",
      "Variable # 39\n",
      "Variable # 40\n",
      "Variable # 41\n",
      "Variable # 42\n",
      "Variable # 43\n",
      "Variable # 44\n",
      "Variable # 45\n",
      "Variable # 46\n",
      "Variable # 47\n",
      "Variable # 48\n",
      "Variable # 49\n",
      "Variable # 50\n",
      "Variable # 51\n",
      "Variable # 52\n",
      "Variable # 53\n",
      "Variable # 54\n",
      "Variable # 55\n",
      "Variable # 56\n",
      "Variable # 57\n",
      "Variable # 58\n",
      "Variable # 59\n",
      "Difference: 0.000556477402734\n",
      "\n",
      "Iteration 2\n",
      "Variable # 0\n",
      "Variable # 1\n",
      "Variable # 2\n",
      "Variable # 3\n",
      "Variable # 4\n",
      "Variable # 5\n",
      "Variable # 6\n",
      "Variable # 7\n",
      "Variable # 8\n",
      "Variable # 9\n",
      "Variable # 10\n",
      "Variable # 11\n",
      "Variable # 12\n",
      "Variable # 13\n",
      "Variable # 14\n",
      "Variable # 15\n",
      "Variable # 16\n",
      "Variable # 17\n",
      "Variable # 18\n",
      "Variable # 19\n",
      "Variable # 20\n",
      "Variable # 21\n",
      "Variable # 22\n",
      "Variable # 23\n",
      "Variable # 24\n",
      "Variable # 25\n",
      "Variable # 26\n",
      "Variable # 27\n",
      "Variable # 28\n",
      "Variable # 29\n",
      "Variable # 30\n",
      "Variable # 31\n",
      "Variable # 32\n",
      "Variable # 33\n",
      "Variable # 34\n",
      "Variable # 35\n",
      "Variable # 36\n",
      "Variable # 37\n",
      "Variable # 38\n",
      "Variable # 39\n",
      "Variable # 40\n",
      "Variable # 41\n",
      "Variable # 42\n",
      "Variable # 43\n",
      "Variable # 44\n",
      "Variable # 45\n",
      "Variable # 46\n",
      "Variable # 47\n",
      "Variable # 48\n",
      "Variable # 49\n",
      "Variable # 50\n",
      "Variable # 51\n",
      "Variable # 52\n",
      "Variable # 53\n",
      "Variable # 54\n",
      "Variable # 55\n",
      "Variable # 56\n",
      "Variable # 57\n",
      "Variable # 58\n",
      "Variable # 59\n",
      "Difference: 0.00019487492385\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#CODE FROM rf_fit_impute_eval_train_val_test.py\n",
    "train, val, test = None, None, None\n",
    "\n",
    "if args_val:\n",
    "    train = pd.read_csv(config[\"data\"][\"trainV\"])\n",
    "    val   = pd.read_csv(config[\"data\"][\"valV\"])\n",
    "    test  = pd.read_csv(config[\"data\"][\"testV\"])\n",
    "\n",
    "else:\n",
    "    train = pd.read_csv(config[\"data\"][\"train\"])\n",
    "    test  = pd.read_csv(config[\"data\"][\"test\"])\n",
    "\n",
    "# set seed for reproducibility\n",
    "np.random.seed(1)\n",
    "\n",
    "# split train up; only fit imputer on part of train set due to memory/time\n",
    "train1, train2 = train_test_split(train, train_prop=args_impute_split, site_var_name=\"site\")\n",
    "\n",
    "# split train and test datasets into x, y, and site\n",
    "train1_x, train1_y, train1_sites = X_y_site_split(train1, y_var_name=\"MonitorData\", site_var_name=\"site\")\n",
    "train2_x, train2_y, train2_sites = X_y_site_split(train2, y_var_name=\"MonitorData\", site_var_name=\"site\")\n",
    "test_x, test_y, test_sites = X_y_site_split(test, y_var_name=\"MonitorData\", site_var_name=\"site\")\n",
    "\n",
    "# create imputer and fit on part of train set\n",
    "rf_imputer = PredictiveImputer(max_iter=args_max_iter, initial_strategy=args_initial_strategy, f_model=\"RandomForest\")\n",
    "rf_imputer.fit(train1_x, max_features=args_max_features, n_estimators=args_n_estimators, n_jobs=-1, verbose=0, random_state=1)\n",
    "\n",
    "### make imputations on train and test data matrices and create dataframes with imputation R^2 evaluations; computed weighted R^2 values\n",
    "train1_x_imp, train1_r2_scores_df = rf_imputer.transform(train1_x, evaluate=True, backup_impute_strategy=args_backup_strategy)\n",
    "train1_r2_scores_df.columns = [\"Train1_R2\", \"Train1_num_missing\"]\n",
    "train1_r2_scores_df.loc[max(train1_r2_scores_df.index)+1, :] = [np.average(train1_r2_scores_df.loc[:, \"Train1_R2\"].values,\n",
    "                                                                   weights=train1_r2_scores_df.loc[:, \"Train1_num_missing\"].values,\n",
    "                                                                   axis=0), np.mean(train1_r2_scores_df.loc[:, \"Train1_num_missing\"].values)]\n",
    "\n",
    "train2_x_imp, train2_r2_scores_df = rf_imputer.transform(train2_x, evaluate = True, backup_impute_strategy = \"mean\")\n",
    "train2_r2_scores_df.columns = [\"Train2_R2\", \"Train2_num_missing\"]\n",
    "train2_r2_scores_df.loc[max(train2_r2_scores_df.index)+1, :] = [np.average(train2_r2_scores_df.loc[:, \"Train2_R2\"].values,\n",
    "                                                                   weights=train2_r2_scores_df.loc[:, \"Train2_num_missing\"].values,\n",
    "                                                                   axis=0), np.mean(train2_r2_scores_df.loc[:, \"Train2_num_missing\"].values)]\n",
    "\n",
    "test_x_imp, test_r2_scores_df = rf_imputer.transform(test_x, evaluate = True, backup_impute_strategy = \"mean\")\n",
    "test_r2_scores_df.columns = [\"Test_R2\", \"Test_num_missing\"]\n",
    "test_r2_scores_df.loc[max(test_r2_scores_df.index)+1, :] = [np.average(test_r2_scores_df.loc[:, \"Test_R2\"].values,\n",
    "                                                                   weights = test_r2_scores_df.loc[:, \"Test_num_missing\"].values,\n",
    "                                                                   axis=0), np.mean(test_r2_scores_df.loc[:, \"Test_num_missing\"].values)]\n",
    "\n",
    "### convert imputed train and test data matrices back into pandas dataframes with column names\n",
    "cols = [\"site\", \"MonitorData\"] + list(train1_x.columns)\n",
    "train1_imp_df = pd.DataFrame(np.concatenate([train1_sites.values.reshape(len(train1_sites), -1),\n",
    "                                              train1_y.values.reshape(len(train1_y), -1),\n",
    "                                              train1_x_imp], axis=1),\n",
    "                                              columns=cols)\n",
    "\n",
    "train2_imp_df = pd.DataFrame(np.concatenate([train2_sites.values.reshape(len(train2_sites), -1),\n",
    "                                              train2_y.values.reshape(len(train2_y), -1),\n",
    "                                              train2_x_imp], axis=1),\n",
    "                                              columns=cols)\n",
    "\n",
    "test_imp_df = pd.DataFrame(np.concatenate([test_sites.values.reshape(len(test_sites), -1),\n",
    "                                              test_y.values.reshape(len(test_y), -1),\n",
    "                                              test_x_imp], axis=1),\n",
    "                                              columns=cols)\n",
    "\n",
    "# put R^2 evaluations for train and test datasets into same pandas dataframe\n",
    "var_df = pd.DataFrame(np.array(cols[2:] + [\"Weighted_Mean_R2\"]).reshape(len(cols)-2+1, -1), columns=[\"Variable\"])\n",
    "train_imp_df = pd.concat([train1_imp_df, train2_imp_df])\n",
    "\n",
    "# recombine partial train sets (both imputed) into single train set\n",
    "train_imp_df = train_imp_df.reset_index().sort_values([\"site\", \"index\"])\n",
    "train_imp_df.drop(\"index\", axis=1, inplace=True)\n",
    "train_imp_df.reset_index(inplace=True, drop=True)\n",
    "\n",
    "# save evaluations\n",
    "#pickle.dump(rf_imputer, open(\"rfV_imputer.pkl\", \"wb\"))\n",
    "\n",
    "if args_val:\n",
    "    # split val into x, y, and site\n",
    "    val_x, val_y, val_sites = X_y_site_split(val, y_var_name=\"MonitorData\", site_var_name=\"site\")\n",
    "\n",
    "    ### make imputations on val data matrix and create dataframe with imputation R^2 evaluations; computed weighted R^2 values\n",
    "    val_x_imp, val_r2_scores_df = rf_imputer.transform(val_x, evaluate = True, backup_impute_strategy = \"mean\")\n",
    "    val_r2_scores_df.columns = [\"Val_R2\", \"Val_num_missing\"]\n",
    "    val_r2_scores_df.loc[max(val_r2_scores_df.index)+1, :] = [np.average(val_r2_scores_df.loc[:, \"Val_R2\"].values,\n",
    "                                                                       weights = val_r2_scores_df.loc[:, \"Val_num_missing\"].values,\n",
    "                                                                       axis=0), np.mean(val_r2_scores_df.loc[:, \"Val_num_missing\"].values)]\n",
    "\n",
    "    ### convert imputed val data matrix back into pandas dataframes with column names\n",
    "    val_imp_df = pd.DataFrame(np.concatenate([val_sites.values.reshape(len(val_sites), -1),\n",
    "                                                  val_y.values.reshape(len(val_y), -1),\n",
    "                                                  val_x_imp], axis=1),\n",
    "                                                  columns=cols)\n",
    "\n",
    "    # save imputed datasets\n",
    "    train_imp_df.to_csv(config[\"RF_Imputation\"][\"trainV\"], index=False)\n",
    "    val_imp_df.to_csv(config[\"RF_Imputation\"][\"valV\"], index=False)\n",
    "    test_imp_df.to_csv(config[\"RF_Imputation\"][\"testV\"], index=False)\n",
    "\n",
    "    # put R^2 evaluations for train, val, and test datasets into same pandas dataframe\n",
    "    r2_scores_df = pd.concat([var_df, train1_r2_scores_df, train2_r2_scores_df, val_r2_scores_df, test_r2_scores_df], axis=1)\n",
    "\n",
    "    # save evaluations\n",
    "    r2_scores_df.to_csv(config[\"RF_Imputation\"][\"r2_scores\"], index=False)\n",
    "\n",
    "\n",
    "else:\n",
    "    train_imp_df.to_csv(config[\"RF_Imputation\"][\"train\"], index=False)\n",
    "    test_imp_df.to_csv(config[\"RF_Imputation\"][\"test\"], index=False)\n",
    "\n",
    "    # put R^2 evaluations for train and test datasets into same pandas dataframe and save\n",
    "    r2_scores_df = pd.concat([var_df, train1_r2_scores_df, train2_r2_scores_df, test_r2_scores_df], axis=1)\n",
    "    r2_scores_df.to_csv(config[\"RF_Imputation\"][\"r2_scores\"], index=False)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Making Pollution Predictions with time-domain CNNs \n",
    "There are two CNN types. Refer to $\\texttt{/src/CNN_architecture.py}$ for description of the differences. Here, we implement CNN type 2 in two steps: \n",
    "- 1) Tune hyper-parameters \n",
    "- 2) Train and test the CNN to compute an $R^2$ value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## First, validate CNN to tune hyperparams (analog to cross validation with other models)\n",
    "- The validation script performs grid with several different combinations of hyper-parameters using our validation dataset. It will print the optimal hyper-parameters it finds. If you wish to use the hyper-parameters, modify their corresponding variables in $\\texttt{/src/config/generate_py_config.py}$. Then, rerun the generate_py_config.py script. \n",
    "- The following is implemented in $\\texttt{CNN_validate.py}$\n",
    "- To run, use shell script $\\texttt{CNN-validate.sh}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Arguments for CNN-validate.sh\n",
    "args_cnn_type = 'cnn_2'\n",
    "args_dataset = 'ridgeImp'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#CODE FROM CNN_validate.py\n",
    "\n",
    "import torch\n",
    "from torch.autograd import Variable\n",
    "import sklearn.preprocessing\n",
    "import sklearn.metrics\n",
    "# this imported function was created for this package to split datasets (see data_split_tune_utils.py)\n",
    "from data_split_tune_utils import X_y_site_split\n",
    "# these imported functions were created for this package for the purposes of data pre-processing for CNNs, training CNNs,\n",
    "# and evaluation of CNNs (see CNN_utils.py)\n",
    "from CNN_utils import split_sizes_site, split_data, pad_stack_splits, get_monitorData_indices, r2, get_nonConst_vars, train_CNN\n",
    "# these imported classes are the CNN architectures (see CNN_architecture.py)\n",
    "from CNN_architecture import CNN1, CNN2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of variables: 60\n",
      "Total number of non-constant variables: 21\n",
      "\n",
      "Hidden size conv: 25\n",
      "Kernel size: 3\n",
      "Hidden size full: 50\n",
      "Dropout full: 0.1\n",
      "Hidden size 2 full: 50\n",
      "Dropout 2 full: 0.1\n",
      "Learning rate: 0.01\n",
      "Weight decay: 1e-05\n",
      "Epoch loss after epoch 5: 183.62091827392578\n",
      "\n",
      "Epoch loss after epoch 10: 132.90976905822754\n",
      "\n",
      "Validation R^2: 0.667679489866\n",
      "\n",
      "\n",
      "Hidden size conv: 25\n",
      "Kernel size: 3\n",
      "Hidden size full: 50\n",
      "Dropout full: 0.1\n",
      "Hidden size 2 full: 50\n",
      "Dropout 2 full: 0.4\n",
      "Learning rate: 0.01\n",
      "Weight decay: 1e-05\n",
      "Epoch loss after epoch 5: 201.7007999420166\n",
      "\n",
      "Epoch loss after epoch 10: 149.17006874084473\n",
      "\n",
      "Validation R^2: 0.63532073318\n",
      "\n",
      "\n",
      "Hidden size conv: 25\n",
      "Kernel size: 3\n",
      "Hidden size full: 50\n",
      "Dropout full: 0.1\n",
      "Hidden size 2 full: 100\n",
      "Dropout 2 full: 0.1\n",
      "Learning rate: 0.01\n",
      "Weight decay: 1e-05\n",
      "Epoch loss after epoch 5: 183.10474586486816\n",
      "\n",
      "Epoch loss after epoch 10: 131.5680980682373\n",
      "\n",
      "Validation R^2: 0.675813953056\n",
      "\n",
      "\n",
      "Hidden size conv: 25\n",
      "Kernel size: 3\n",
      "Hidden size full: 50\n",
      "Dropout full: 0.1\n",
      "Hidden size 2 full: 100\n",
      "Dropout 2 full: 0.4\n",
      "Learning rate: 0.01\n",
      "Weight decay: 1e-05\n",
      "Epoch loss after epoch 5: 206.02194213867188\n",
      "\n",
      "Epoch loss after epoch 10: 145.02288055419922\n",
      "\n",
      "Validation R^2: 0.630460594128\n",
      "\n",
      "\n",
      "Hidden size conv: 25\n",
      "Kernel size: 3\n",
      "Hidden size full: 50\n",
      "Dropout full: 0.4\n",
      "Hidden size 2 full: 50\n",
      "Dropout 2 full: 0.1\n",
      "Learning rate: 0.01\n",
      "Weight decay: 1e-05\n",
      "Epoch loss after epoch 5: 205.19343948364258\n",
      "\n",
      "Epoch loss after epoch 10: 148.26194381713867\n",
      "\n",
      "Validation R^2: 0.634234879646\n",
      "\n",
      "\n",
      "Hidden size conv: 25\n",
      "Kernel size: 3\n",
      "Hidden size full: 50\n",
      "Dropout full: 0.4\n",
      "Hidden size 2 full: 50\n",
      "Dropout 2 full: 0.4\n",
      "Learning rate: 0.01\n",
      "Weight decay: 1e-05\n",
      "Epoch loss after epoch 5: 222.6208839416504\n",
      "\n",
      "Epoch loss after epoch 10: 159.23365783691406\n",
      "\n",
      "Validation R^2: 0.600415762684\n",
      "\n",
      "\n",
      "Hidden size conv: 25\n",
      "Kernel size: 3\n",
      "Hidden size full: 50\n",
      "Dropout full: 0.4\n",
      "Hidden size 2 full: 100\n",
      "Dropout 2 full: 0.1\n",
      "Learning rate: 0.01\n",
      "Weight decay: 1e-05\n",
      "Epoch loss after epoch 5: 199.6256275177002\n",
      "\n",
      "Epoch loss after epoch 10: 143.89752769470215\n",
      "\n",
      "Validation R^2: 0.636411023118\n",
      "\n",
      "\n",
      "Hidden size conv: 25\n",
      "Kernel size: 3\n",
      "Hidden size full: 50\n",
      "Dropout full: 0.4\n",
      "Hidden size 2 full: 100\n",
      "Dropout 2 full: 0.4\n",
      "Learning rate: 0.01\n",
      "Weight decay: 1e-05\n",
      "Epoch loss after epoch 5: 211.87213134765625\n",
      "\n",
      "Epoch loss after epoch 10: 147.68312454223633\n",
      "\n",
      "Validation R^2: 0.622473066585\n",
      "\n",
      "\n",
      "Hidden size conv: 25\n",
      "Kernel size: 3\n",
      "Hidden size full: 100\n",
      "Dropout full: 0.1\n",
      "Hidden size 2 full: 50\n",
      "Dropout 2 full: 0.1\n",
      "Learning rate: 0.01\n",
      "Weight decay: 1e-05\n",
      "Epoch loss after epoch 5: 180.65746116638184\n",
      "\n",
      "Epoch loss after epoch 10: 127.26936340332031\n",
      "\n",
      "Validation R^2: 0.66909149479\n",
      "\n",
      "\n",
      "Hidden size conv: 25\n",
      "Kernel size: 3\n",
      "Hidden size full: 100\n",
      "Dropout full: 0.1\n",
      "Hidden size 2 full: 50\n",
      "Dropout 2 full: 0.4\n",
      "Learning rate: 0.01\n",
      "Weight decay: 1e-05\n",
      "Epoch loss after epoch 5: 211.24010276794434\n",
      "\n",
      "Epoch loss after epoch 10: 149.8973503112793\n",
      "\n",
      "Validation R^2: 0.614823923987\n",
      "\n",
      "\n",
      "Hidden size conv: 25\n",
      "Kernel size: 3\n",
      "Hidden size full: 100\n",
      "Dropout full: 0.1\n",
      "Hidden size 2 full: 100\n",
      "Dropout 2 full: 0.1\n",
      "Learning rate: 0.01\n",
      "Weight decay: 1e-05\n",
      "Epoch loss after epoch 5: 167.5049991607666\n",
      "\n",
      "Epoch loss after epoch 10: 127.7283706665039\n",
      "\n",
      "Validation R^2: 0.678950137849\n",
      "\n",
      "\n",
      "Hidden size conv: 25\n",
      "Kernel size: 3\n",
      "Hidden size full: 100\n",
      "Dropout full: 0.1\n",
      "Hidden size 2 full: 100\n",
      "Dropout 2 full: 0.4\n",
      "Learning rate: 0.01\n",
      "Weight decay: 1e-05\n",
      "Epoch loss after epoch 5: 169.04331398010254\n",
      "\n",
      "Epoch loss after epoch 10: 140.05623245239258\n",
      "\n",
      "Validation R^2: 0.652645105571\n",
      "\n",
      "\n",
      "Hidden size conv: 25\n",
      "Kernel size: 3\n",
      "Hidden size full: 100\n",
      "Dropout full: 0.4\n",
      "Hidden size 2 full: 50\n",
      "Dropout 2 full: 0.1\n",
      "Learning rate: 0.01\n",
      "Weight decay: 1e-05\n",
      "Epoch loss after epoch 5: 182.50423431396484\n",
      "\n",
      "Epoch loss after epoch 10: 135.9016571044922\n",
      "\n",
      "Validation R^2: 0.658323645319\n",
      "\n",
      "\n",
      "Hidden size conv: 25\n",
      "Kernel size: 3\n",
      "Hidden size full: 100\n",
      "Dropout full: 0.4\n",
      "Hidden size 2 full: 50\n",
      "Dropout 2 full: 0.4\n",
      "Learning rate: 0.01\n",
      "Weight decay: 1e-05\n",
      "Epoch loss after epoch 5: 208.56830978393555\n",
      "\n",
      "Epoch loss after epoch 10: 157.8165683746338\n",
      "\n",
      "Validation R^2: 0.62778368949\n",
      "\n",
      "\n",
      "Hidden size conv: 25\n",
      "Kernel size: 3\n",
      "Hidden size full: 100\n",
      "Dropout full: 0.4\n",
      "Hidden size 2 full: 100\n",
      "Dropout 2 full: 0.1\n",
      "Learning rate: 0.01\n",
      "Weight decay: 1e-05\n",
      "Epoch loss after epoch 5: 176.38119316101074\n",
      "\n",
      "Epoch loss after epoch 10: 137.8725872039795\n",
      "\n",
      "Validation R^2: 0.66714950997\n",
      "\n",
      "\n",
      "Hidden size conv: 25\n",
      "Kernel size: 3\n",
      "Hidden size full: 100\n",
      "Dropout full: 0.4\n",
      "Hidden size 2 full: 100\n",
      "Dropout 2 full: 0.4\n",
      "Learning rate: 0.01\n",
      "Weight decay: 1e-05\n",
      "Epoch loss after epoch 5: 186.6520175933838\n",
      "\n",
      "Epoch loss after epoch 10: 147.78594779968262\n",
      "\n",
      "Validation R^2: 0.631804993484\n",
      "\n",
      "\n",
      "Hidden size conv: 25\n",
      "Kernel size: 5\n",
      "Hidden size full: 50\n",
      "Dropout full: 0.1\n",
      "Hidden size 2 full: 50\n",
      "Dropout 2 full: 0.1\n",
      "Learning rate: 0.01\n",
      "Weight decay: 1e-05\n",
      "Epoch loss after epoch 5: 187.01734733581543\n",
      "\n",
      "Epoch loss after epoch 10: 132.91092109680176\n",
      "\n",
      "Validation R^2: 0.673300766459\n",
      "\n",
      "\n",
      "Hidden size conv: 25\n",
      "Kernel size: 5\n",
      "Hidden size full: 50\n",
      "Dropout full: 0.1\n",
      "Hidden size 2 full: 50\n",
      "Dropout 2 full: 0.4\n",
      "Learning rate: 0.01\n",
      "Weight decay: 1e-05\n",
      "Epoch loss after epoch 5: 206.5174789428711\n",
      "\n",
      "Epoch loss after epoch 10: 150.68610382080078\n",
      "\n",
      "Validation R^2: 0.617039320182\n",
      "\n",
      "\n",
      "Hidden size conv: 25\n",
      "Kernel size: 5\n",
      "Hidden size full: 50\n",
      "Dropout full: 0.1\n",
      "Hidden size 2 full: 100\n",
      "Dropout 2 full: 0.1\n",
      "Learning rate: 0.01\n",
      "Weight decay: 1e-05\n",
      "Epoch loss after epoch 5: 178.8384952545166\n",
      "\n",
      "Epoch loss after epoch 10: 128.677095413208\n",
      "\n",
      "Validation R^2: 0.674215970565\n",
      "\n",
      "\n",
      "Hidden size conv: 25\n",
      "Kernel size: 5\n",
      "Hidden size full: 50\n",
      "Dropout full: 0.1\n",
      "Hidden size 2 full: 100\n",
      "Dropout 2 full: 0.4\n",
      "Learning rate: 0.01\n",
      "Weight decay: 1e-05\n",
      "Epoch loss after epoch 5: 199.9126682281494\n",
      "\n",
      "Epoch loss after epoch 10: 139.6981315612793\n",
      "\n",
      "Validation R^2: 0.644267183276\n",
      "\n",
      "\n",
      "Hidden size conv: 25\n",
      "Kernel size: 5\n",
      "Hidden size full: 50\n",
      "Dropout full: 0.4\n",
      "Hidden size 2 full: 50\n",
      "Dropout 2 full: 0.1\n",
      "Learning rate: 0.01\n",
      "Weight decay: 1e-05\n",
      "Epoch loss after epoch 5: 198.0303497314453\n",
      "\n",
      "Epoch loss after epoch 10: 145.77274131774902\n",
      "\n",
      "Validation R^2: 0.632570749888\n",
      "\n",
      "\n",
      "Hidden size conv: 25\n",
      "Kernel size: 5\n",
      "Hidden size full: 50\n",
      "Dropout full: 0.4\n",
      "Hidden size 2 full: 50\n",
      "Dropout 2 full: 0.4\n",
      "Learning rate: 0.01\n",
      "Weight decay: 1e-05\n",
      "Epoch loss after epoch 5: 217.55657386779785\n",
      "\n",
      "Epoch loss after epoch 10: 155.7939453125\n",
      "\n",
      "Validation R^2: 0.600861723121\n",
      "\n",
      "\n",
      "Hidden size conv: 25\n",
      "Kernel size: 5\n",
      "Hidden size full: 50\n",
      "Dropout full: 0.4\n",
      "Hidden size 2 full: 100\n",
      "Dropout 2 full: 0.1\n",
      "Learning rate: 0.01\n",
      "Weight decay: 1e-05\n",
      "Epoch loss after epoch 5: 192.23321342468262\n",
      "\n",
      "Epoch loss after epoch 10: 146.87072944641113\n",
      "\n",
      "Validation R^2: 0.63879081871\n",
      "\n",
      "\n",
      "Hidden size conv: 25\n",
      "Kernel size: 5\n",
      "Hidden size full: 50\n",
      "Dropout full: 0.4\n",
      "Hidden size 2 full: 100\n",
      "Dropout 2 full: 0.4\n",
      "Learning rate: 0.01\n",
      "Weight decay: 1e-05\n",
      "Epoch loss after epoch 5: 208.7303638458252\n",
      "\n",
      "Epoch loss after epoch 10: 157.79657554626465\n",
      "\n",
      "Validation R^2: 0.627000086454\n",
      "\n",
      "\n",
      "Hidden size conv: 25\n",
      "Kernel size: 5\n",
      "Hidden size full: 100\n",
      "Dropout full: 0.1\n",
      "Hidden size 2 full: 50\n",
      "Dropout 2 full: 0.1\n",
      "Learning rate: 0.01\n",
      "Weight decay: 1e-05\n",
      "Epoch loss after epoch 5: 174.68819046020508\n",
      "\n",
      "Epoch loss after epoch 10: 129.3569622039795\n",
      "\n",
      "Validation R^2: 0.676822862463\n",
      "\n",
      "\n",
      "Hidden size conv: 25\n",
      "Kernel size: 5\n",
      "Hidden size full: 100\n",
      "Dropout full: 0.1\n",
      "Hidden size 2 full: 50\n",
      "Dropout 2 full: 0.4\n",
      "Learning rate: 0.01\n",
      "Weight decay: 1e-05\n",
      "Epoch loss after epoch 5: 201.89330673217773\n",
      "\n",
      "Epoch loss after epoch 10: 137.04962348937988\n",
      "\n",
      "Validation R^2: 0.625398821626\n",
      "\n",
      "\n",
      "Hidden size conv: 25\n",
      "Kernel size: 5\n",
      "Hidden size full: 100\n",
      "Dropout full: 0.1\n",
      "Hidden size 2 full: 100\n",
      "Dropout 2 full: 0.1\n",
      "Learning rate: 0.01\n",
      "Weight decay: 1e-05\n",
      "Epoch loss after epoch 5: 172.9888744354248\n",
      "\n",
      "Epoch loss after epoch 10: 126.7570219039917\n",
      "\n",
      "Validation R^2: 0.68574944811\n",
      "\n",
      "\n",
      "Hidden size conv: 25\n",
      "Kernel size: 5\n",
      "Hidden size full: 100\n",
      "Dropout full: 0.1\n",
      "Hidden size 2 full: 100\n",
      "Dropout 2 full: 0.4\n",
      "Learning rate: 0.01\n",
      "Weight decay: 1e-05\n",
      "Epoch loss after epoch 5: 171.95508003234863\n",
      "\n",
      "Epoch loss after epoch 10: 136.35294342041016\n",
      "\n",
      "Validation R^2: 0.654613459497\n",
      "\n",
      "\n",
      "Hidden size conv: 25\n",
      "Kernel size: 5\n",
      "Hidden size full: 100\n",
      "Dropout full: 0.4\n",
      "Hidden size 2 full: 50\n",
      "Dropout 2 full: 0.1\n",
      "Learning rate: 0.01\n",
      "Weight decay: 1e-05\n",
      "Epoch loss after epoch 5: 184.14102363586426\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch loss after epoch 10: 137.98619842529297\n",
      "\n",
      "Validation R^2: 0.645463039066\n",
      "\n",
      "\n",
      "Hidden size conv: 25\n",
      "Kernel size: 5\n",
      "Hidden size full: 100\n",
      "Dropout full: 0.4\n",
      "Hidden size 2 full: 50\n",
      "Dropout 2 full: 0.4\n",
      "Learning rate: 0.01\n",
      "Weight decay: 1e-05\n",
      "Epoch loss after epoch 5: 217.55944061279297\n",
      "\n",
      "Epoch loss after epoch 10: 163.14635848999023\n",
      "\n",
      "Validation R^2: 0.599544309799\n",
      "\n",
      "\n",
      "Hidden size conv: 25\n",
      "Kernel size: 5\n",
      "Hidden size full: 100\n",
      "Dropout full: 0.4\n",
      "Hidden size 2 full: 100\n",
      "Dropout 2 full: 0.1\n",
      "Learning rate: 0.01\n",
      "Weight decay: 1e-05\n",
      "Epoch loss after epoch 5: 175.58785247802734\n",
      "\n",
      "Epoch loss after epoch 10: 137.55784225463867\n",
      "\n",
      "Validation R^2: 0.660688938901\n",
      "\n",
      "\n",
      "Hidden size conv: 25\n",
      "Kernel size: 5\n",
      "Hidden size full: 100\n",
      "Dropout full: 0.4\n",
      "Hidden size 2 full: 100\n",
      "Dropout 2 full: 0.4\n",
      "Learning rate: 0.01\n",
      "Weight decay: 1e-05\n",
      "Epoch loss after epoch 5: 181.59077262878418\n",
      "\n",
      "Epoch loss after epoch 10: 152.2690143585205\n",
      "\n",
      "Validation R^2: 0.628058482613\n",
      "\n",
      "\n",
      "Hidden size conv: 50\n",
      "Kernel size: 3\n",
      "Hidden size full: 50\n",
      "Dropout full: 0.1\n",
      "Hidden size 2 full: 50\n",
      "Dropout 2 full: 0.1\n",
      "Learning rate: 0.01\n",
      "Weight decay: 1e-05\n",
      "Epoch loss after epoch 5: 186.21160888671875\n",
      "\n",
      "Epoch loss after epoch 10: 126.8487491607666\n",
      "\n",
      "Validation R^2: 0.671438019479\n",
      "\n",
      "\n",
      "Hidden size conv: 50\n",
      "Kernel size: 3\n",
      "Hidden size full: 50\n",
      "Dropout full: 0.1\n",
      "Hidden size 2 full: 50\n",
      "Dropout 2 full: 0.4\n",
      "Learning rate: 0.01\n",
      "Weight decay: 1e-05\n",
      "Epoch loss after epoch 5: 198.05391120910645\n",
      "\n",
      "Epoch loss after epoch 10: 142.08588027954102\n",
      "\n",
      "Validation R^2: 0.654346861599\n",
      "\n",
      "\n",
      "Hidden size conv: 50\n",
      "Kernel size: 3\n",
      "Hidden size full: 50\n",
      "Dropout full: 0.1\n",
      "Hidden size 2 full: 100\n",
      "Dropout 2 full: 0.1\n",
      "Learning rate: 0.01\n",
      "Weight decay: 1e-05\n",
      "Epoch loss after epoch 5: 175.83682250976562\n",
      "\n",
      "Epoch loss after epoch 10: 128.6238145828247\n",
      "\n",
      "Validation R^2: 0.679283203722\n",
      "\n",
      "\n",
      "Hidden size conv: 50\n",
      "Kernel size: 3\n",
      "Hidden size full: 50\n",
      "Dropout full: 0.1\n",
      "Hidden size 2 full: 100\n",
      "Dropout 2 full: 0.4\n",
      "Learning rate: 0.01\n",
      "Weight decay: 1e-05\n",
      "Epoch loss after epoch 5: 186.31736183166504\n",
      "\n",
      "Epoch loss after epoch 10: 136.06121826171875\n",
      "\n",
      "Validation R^2: 0.661475572095\n",
      "\n",
      "\n",
      "Hidden size conv: 50\n",
      "Kernel size: 3\n",
      "Hidden size full: 50\n",
      "Dropout full: 0.4\n",
      "Hidden size 2 full: 50\n",
      "Dropout 2 full: 0.1\n",
      "Learning rate: 0.01\n",
      "Weight decay: 1e-05\n",
      "Epoch loss after epoch 5: 188.87781524658203\n",
      "\n",
      "Epoch loss after epoch 10: 140.60195350646973\n",
      "\n",
      "Validation R^2: 0.657426049409\n",
      "\n",
      "\n",
      "Hidden size conv: 50\n",
      "Kernel size: 3\n",
      "Hidden size full: 50\n",
      "Dropout full: 0.4\n",
      "Hidden size 2 full: 50\n",
      "Dropout 2 full: 0.4\n",
      "Learning rate: 0.01\n",
      "Weight decay: 1e-05\n",
      "Epoch loss after epoch 5: 213.86843872070312\n",
      "\n",
      "Epoch loss after epoch 10: 150.3547706604004\n",
      "\n",
      "Validation R^2: 0.634178741998\n",
      "\n",
      "\n",
      "Hidden size conv: 50\n",
      "Kernel size: 3\n",
      "Hidden size full: 50\n",
      "Dropout full: 0.4\n",
      "Hidden size 2 full: 100\n",
      "Dropout 2 full: 0.1\n",
      "Learning rate: 0.01\n",
      "Weight decay: 1e-05\n",
      "Epoch loss after epoch 5: 187.13127899169922\n",
      "\n",
      "Epoch loss after epoch 10: 138.14415740966797\n",
      "\n",
      "Validation R^2: 0.66328502141\n",
      "\n",
      "\n",
      "Hidden size conv: 50\n",
      "Kernel size: 3\n",
      "Hidden size full: 50\n",
      "Dropout full: 0.4\n",
      "Hidden size 2 full: 100\n",
      "Dropout 2 full: 0.4\n",
      "Learning rate: 0.01\n",
      "Weight decay: 1e-05\n",
      "Epoch loss after epoch 5: 196.07123374938965\n",
      "\n",
      "Epoch loss after epoch 10: 143.84477996826172\n",
      "\n",
      "Validation R^2: 0.645614255108\n",
      "\n",
      "\n",
      "Hidden size conv: 50\n",
      "Kernel size: 3\n",
      "Hidden size full: 100\n",
      "Dropout full: 0.1\n",
      "Hidden size 2 full: 50\n",
      "Dropout 2 full: 0.1\n",
      "Learning rate: 0.01\n",
      "Weight decay: 1e-05\n",
      "Epoch loss after epoch 5: 175.09451484680176\n",
      "\n",
      "Epoch loss after epoch 10: 126.15167427062988\n",
      "\n",
      "Validation R^2: 0.686514865713\n",
      "\n",
      "\n",
      "Hidden size conv: 50\n",
      "Kernel size: 3\n",
      "Hidden size full: 100\n",
      "Dropout full: 0.1\n",
      "Hidden size 2 full: 50\n",
      "Dropout 2 full: 0.4\n",
      "Learning rate: 0.01\n",
      "Weight decay: 1e-05\n",
      "Epoch loss after epoch 5: 184.84235954284668\n",
      "\n",
      "Epoch loss after epoch 10: 140.68557929992676\n",
      "\n",
      "Validation R^2: 0.661162408702\n",
      "\n",
      "\n",
      "Hidden size conv: 50\n",
      "Kernel size: 3\n",
      "Hidden size full: 100\n",
      "Dropout full: 0.1\n",
      "Hidden size 2 full: 100\n",
      "Dropout 2 full: 0.1\n",
      "Learning rate: 0.01\n",
      "Weight decay: 1e-05\n",
      "Epoch loss after epoch 5: 157.9090747833252\n",
      "\n",
      "Epoch loss after epoch 10: 127.36720085144043\n",
      "\n",
      "Validation R^2: 0.691436651792\n",
      "\n",
      "\n",
      "Hidden size conv: 50\n",
      "Kernel size: 3\n",
      "Hidden size full: 100\n",
      "Dropout full: 0.1\n",
      "Hidden size 2 full: 100\n",
      "Dropout 2 full: 0.4\n",
      "Learning rate: 0.01\n",
      "Weight decay: 1e-05\n",
      "Epoch loss after epoch 5: 168.5186882019043\n",
      "\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-11-3960e9c09500>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m    200\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    201\u001b[0m                                     \u001b[1;31m# train\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 202\u001b[1;33m                                     \u001b[0mtrain_CNN\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_x_std_stack_nonConst\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_x_std_tuple\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_y_tuple\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcnn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmse_loss\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnum_epochs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    203\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    204\u001b[0m                                     \u001b[1;31m# evaluate\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\OneDrive\\Documents\\Harvard Grad school\\2017-2018 Academic Year\\Spring 2018\\CSE Capstone Project (APCOMP 297R)\\airpred_git\\tutorial\\src\\CNN_utils.py\u001b[0m in \u001b[0;36mtrain_CNN\u001b[1;34m(x_stack_nonConst, x_tuple, y_tuple, cnn, optimizer, loss, num_epochs, batch_size)\u001b[0m\n\u001b[0;32m    275\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    276\u001b[0m             \u001b[1;31m# get model output\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 277\u001b[1;33m             \u001b[0mpred_batch\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcnn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx_stack_batch_nonConst\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx_batch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_ind_by_site\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    278\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    279\u001b[0m             \u001b[1;31m# zero gradient, compute loss, backprop, and update parameters\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\py35\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m    222\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_forward_pre_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    223\u001b[0m             \u001b[0mhook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 224\u001b[1;33m         \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    225\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    226\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\OneDrive\\Documents\\Harvard Grad school\\2017-2018 Academic Year\\Spring 2018\\CSE Capstone Project (APCOMP 297R)\\airpred_git\\tutorial\\src\\CNN_architecture.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, input_conv, input_full, y_ind_by_site)\u001b[0m\n\u001b[0;32m    281\u001b[0m         \u001b[0mhidden_conv_w_response\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    282\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhidden_conv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 283\u001b[1;33m             \u001b[0mhidden_conv_w_response\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhidden_conv\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_ind_by_site\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    284\u001b[0m         \u001b[0mhidden_conv_w_response\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhidden_conv_w_response\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    285\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\py35\\lib\\site-packages\\torch\\autograd\\variable.py\u001b[0m in \u001b[0;36mtranspose\u001b[1;34m(self, dim1, dim2)\u001b[0m\n\u001b[0;32m    731\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    732\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mtranspose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdim1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdim2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 733\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mTranspose\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdim1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdim2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    734\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    735\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mselect\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_index\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\py35\\lib\\site-packages\\torch\\autograd\\_functions\\tensor.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(ctx, i, dim1, dim2)\u001b[0m\n\u001b[0;32m     78\u001b[0m     \u001b[1;33m@\u001b[0m\u001b[0mstaticmethod\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     79\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mctx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mi\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdim1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdim2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 80\u001b[1;33m         \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mi\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdim1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdim2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     81\u001b[0m         \u001b[0mctx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdims\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mdim1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdim2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     82\u001b[0m         \u001b[0mctx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmark_shared_storage\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#CODE FROM CNN_validate.py\n",
    "\n",
    "# set seeds for reproducibility\n",
    "np.random.seed(1)\n",
    "torch.manual_seed(1)\n",
    "\n",
    "train = None\n",
    "val = None\n",
    "\n",
    "if args_dataset == \"ridgeImp\":\n",
    "    # read in train and val sets\n",
    "    train = pd.read_csv(config[\"Ridge_Imputation\"][\"trainV\"])\n",
    "    val = pd.read_csv(config[\"Ridge_Imputation\"][\"valV\"])\n",
    "\n",
    "elif args_dataset == \"rfImp\":\n",
    "    # read in train and val sets\n",
    "    train = pd.read_csv(config[\"RF_Imputation\"][\"trainV\"])\n",
    "    val = pd.read_csv(config[\"RF_Imputation\"][\"valV\"])\n",
    "\n",
    "\n",
    "### delete sites from datasets where all monitor outputs are nan\n",
    "train_sites_all_nan_df = train.loc[:, ['site', 'MonitorData']].groupby('site').any()\n",
    "train_sites_to_delete = list(train_sites_all_nan_df[train_sites_all_nan_df['MonitorData']==False].index)\n",
    "train = train[~train['site'].isin(train_sites_to_delete)]\n",
    "\n",
    "val_sites_all_nan_df = val.loc[:, ['site', 'MonitorData']].groupby('site').any()\n",
    "val_sites_to_delete = list(val_sites_all_nan_df[val_sites_all_nan_df['MonitorData']==False].index)\n",
    "val = val[~val['site'].isin(val_sites_to_delete)]\n",
    "\n",
    "# split train, val into x, y, and sites\n",
    "train_x, train_y, train_sites = X_y_site_split(train, y_var_name='MonitorData', site_var_name='site')\n",
    "val_x, val_y, val_sites = X_y_site_split(val, y_var_name='MonitorData', site_var_name='site')\n",
    "\n",
    "# get dataframes with non-constant features only\n",
    "nonConst_vars = get_nonConst_vars(train, site_var_name='site', y_var_name='MonitorData', cutoff=20)\n",
    "train_x_nonConst = train_x.loc[:, nonConst_vars]\n",
    "val_x_nonConst = val_x.loc[:, nonConst_vars]\n",
    "\n",
    "# standardize all features\n",
    "standardizer_all = sklearn.preprocessing.StandardScaler(with_mean = True, with_std = True)\n",
    "train_x_std_all = standardizer_all.fit_transform(train_x)\n",
    "val_x_std_all = standardizer_all.transform(val_x)\n",
    "\n",
    "# standardize non-constant features\n",
    "standardizer_nonConst = sklearn.preprocessing.StandardScaler(with_mean = True, with_std = True)\n",
    "train_x_std_nonConst = standardizer_nonConst.fit_transform(train_x_nonConst)\n",
    "val_x_std_nonConst = standardizer_nonConst.transform(val_x_nonConst)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# get split sizes for TRAIN data (splitting by site)\n",
    "train_split_sizes = split_sizes_site(train_sites.values)\n",
    "\n",
    "# get tuples by site\n",
    "train_x_std_tuple_nonConst = split_data(torch.from_numpy(train_x_std_nonConst).float(), train_split_sizes, dim = 0)\n",
    "train_x_std_tuple = split_data(torch.from_numpy(train_x_std_all).float(), train_split_sizes, dim = 0)\n",
    "train_y_tuple = split_data(torch.from_numpy(train_y.values), train_split_sizes, dim = 0)\n",
    "\n",
    "# get site sequences stacked into matrix to go through CNN\n",
    "train_x_std_stack_nonConst = pad_stack_splits(train_x_std_tuple_nonConst, np.array(train_split_sizes), 'x')\n",
    "train_x_std_stack_nonConst = Variable(torch.transpose(train_x_std_stack_nonConst, 1, 2))\n",
    "\n",
    "\n",
    "# get split sizes for VALIDATION data (splitting by site)\n",
    "val_split_sizes = split_sizes_site(val_sites.values)\n",
    "\n",
    "# get tuples by site\n",
    "val_x_std_tuple_nonConst = split_data(torch.from_numpy(val_x_std_nonConst).float(), val_split_sizes, dim = 0)\n",
    "val_x_std_tuple = split_data(torch.from_numpy(val_x_std_all).float(), val_split_sizes, dim = 0)\n",
    "val_y_tuple = split_data(torch.from_numpy(val_y.values), val_split_sizes, dim = 0)\n",
    "\n",
    "# get site sequences stacked into matrix to go through CNN\n",
    "val_x_std_stack_nonConst = pad_stack_splits(val_x_std_tuple_nonConst, np.array(val_split_sizes), 'x')\n",
    "val_x_std_stack_nonConst = Variable(torch.transpose(val_x_std_stack_nonConst, 1, 2))\n",
    "\n",
    "\n",
    "# training parameters and model input sizes\n",
    "num_epochs = 10\n",
    "batch_size = 128\n",
    "input_size_conv = train_x_std_nonConst.shape[1]\n",
    "input_size_full = train_x_std_all.shape[1]\n",
    "print('Total number of variables: ' + str(input_size_full))\n",
    "print('Total number of non-constant variables: ' + str(input_size_conv))\n",
    "print()\n",
    "\n",
    "### tune CNN1\n",
    "if args_cnn_type == \"cnn_1\":\n",
    "    # CNN and optimizer hyper-parameters to test\n",
    "    hidden_size_conv_list = [25, 50]\n",
    "    kernel_size_list = [3, 5]\n",
    "    padding_list = [1, 2]\n",
    "    hidden_size_full_list = [50, 100]\n",
    "    dropout_full_list = [0.1, 0.4]\n",
    "    hidden_size_combo_list = [50, 100]\n",
    "    dropout_combo_list = [0.1, 0.4]\n",
    "    lr_list = [0.1]\n",
    "    weight_decay_list = [0.00001]\n",
    "\n",
    "    # Loss function\n",
    "    mse_loss = torch.nn.MSELoss(size_average=True)\n",
    "\n",
    "    # grid search\n",
    "    best_val_r2 = -np.inf\n",
    "    for hidden_size_conv in hidden_size_conv_list:\n",
    "        for kernel_size, padding in zip(kernel_size_list, padding_list):\n",
    "            for hidden_size_full in hidden_size_full_list:\n",
    "                for dropout_full in dropout_full_list:\n",
    "                    for hidden_size_combo in hidden_size_combo_list:\n",
    "                        for dropout_combo in dropout_combo_list:\n",
    "                            for lr in lr_list:\n",
    "                                for weight_decay in weight_decay_list:\n",
    "                                    # instantiate CNN\n",
    "                                    cnn = CNN1(input_size_conv, hidden_size_conv, kernel_size, padding, input_size_full, hidden_size_full,\n",
    "                                              dropout_full, hidden_size_combo, dropout_combo)\n",
    "\n",
    "                                    # instantiate optimizer\n",
    "                                    optimizer = torch.optim.Adam(cnn.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "\n",
    "                                    print('Hidden size conv: ' + str(hidden_size_conv))\n",
    "                                    print('Kernel size: ' + str(kernel_size))\n",
    "                                    print('Hidden size full: ' + str(hidden_size_full))\n",
    "                                    print('Dropout full: ' + str(dropout_full))\n",
    "                                    print('Hidden size combo: ' + str(hidden_size_combo))\n",
    "                                    print('Dropout combo: ' + str(dropout_combo))\n",
    "                                    print('Learning rate: ' + str(lr))\n",
    "                                    print('Weight decay: ' + str(weight_decay))\n",
    "\n",
    "                                    # train\n",
    "                                    train_CNN(train_x_std_stack_nonConst, train_x_std_tuple, train_y_tuple, cnn, optimizer, mse_loss, num_epochs, batch_size)\n",
    "\n",
    "                                    # evaluate\n",
    "                                    val_r2 = r2(cnn, batch_size, val_x_std_stack_nonConst, val_x_std_tuple, val_y_tuple, get_pred=False)\n",
    "                                    print('Validation R^2: ' + str(val_r2))\n",
    "                                    print()\n",
    "                                    print()\n",
    "\n",
    "                                    # keep track of best validation R^2 and hyper-parameters\n",
    "                                    if val_r2 > best_val_r2:\n",
    "                                        best_val_r2 = val_r2\n",
    "                                        best_hidden_size_conv = hidden_size_conv\n",
    "                                        best_kernel_size = kernel_size\n",
    "                                        best_hidden_size_full = hidden_size_full\n",
    "                                        best_dropout_full = dropout_full\n",
    "                                        best_hidden_size_combo = hidden_size_combo\n",
    "                                        best_dropout_combo = dropout_combo\n",
    "                                        best_lr = lr\n",
    "                                        best_weight_decay = weight_decay\n",
    "                                        \n",
    "    print('Best validation R^2: ' + str(best_val_r2))\n",
    "    print('Best hidden size conv: ' + str(best_hidden_size_conv))\n",
    "    print('Best kernel size: ' + str(best_kernel_size))\n",
    "    print('Best hidden size full: ' + str(best_hidden_size_full))\n",
    "    print('Best dropout full: ' + str(best_dropout_full))\n",
    "    print('Best hidden size combo: ' + str(best_hidden_size_combo))\n",
    "    print('Best dropout combo: ' + str(best_dropout_combo))\n",
    "    print('Best learning rate: ' + str(best_lr))\n",
    "    print('Best weight decay: ' + str(best_weight_decay))\n",
    "    \n",
    "### tune CNN2\n",
    "else:\n",
    "    hidden_size_conv_list = [25, 50]\n",
    "    kernel_size_list = [3, 5]\n",
    "    padding_list = [1, 2]\n",
    "    hidden_size_full_list = [50, 100]\n",
    "    dropout_full_list = [0.1, 0.4]\n",
    "    hidden_size2_full_list = [50, 100]\n",
    "    dropout2_full_list = [0.1, 0.4]\n",
    "    lr_list = [0.01]\n",
    "    weight_decay_list = [0.00001]\n",
    "\n",
    "    # Loss function\n",
    "    mse_loss = torch.nn.MSELoss(size_average=True)\n",
    "\n",
    "    # grid search\n",
    "    best_val_r2 = -np.inf\n",
    "    for hidden_size_conv in hidden_size_conv_list:\n",
    "        for kernel_size, padding in zip(kernel_size_list, padding_list):\n",
    "            for hidden_size_full in hidden_size_full_list:\n",
    "                for dropout_full in dropout_full_list:\n",
    "                    for hidden_size2_full in hidden_size2_full_list:\n",
    "                        for dropout2_full in dropout2_full_list:\n",
    "                            for lr in lr_list:\n",
    "                                for weight_decay in weight_decay_list:\n",
    "                                    # instantiate CNN\n",
    "                                    cnn = CNN2(input_size_conv, hidden_size_conv, kernel_size, padding, input_size_full, hidden_size_full,\n",
    "                                              dropout_full, hidden_size2_full, dropout2_full)\n",
    "\n",
    "                                    # instantiate optimizer\n",
    "                                    optimizer = torch.optim.Adam(cnn.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "\n",
    "                                    print('Hidden size conv: ' + str(hidden_size_conv))\n",
    "                                    print('Kernel size: ' + str(kernel_size))\n",
    "                                    print('Hidden size full: ' + str(hidden_size_full))\n",
    "                                    print('Dropout full: ' + str(dropout_full))\n",
    "                                    print('Hidden size 2 full: ' + str(hidden_size2_full))\n",
    "                                    print('Dropout 2 full: ' + str(dropout2_full))\n",
    "                                    print('Learning rate: ' + str(lr))\n",
    "                                    print('Weight decay: ' + str(weight_decay))\n",
    "\n",
    "                                    # train\n",
    "                                    train_CNN(train_x_std_stack_nonConst, train_x_std_tuple, train_y_tuple, cnn, optimizer, mse_loss, num_epochs, batch_size)\n",
    "\n",
    "                                    # evaluate\n",
    "                                    val_r2 = r2(cnn, batch_size, val_x_std_stack_nonConst, val_x_std_tuple, val_y_tuple, get_pred=False)\n",
    "                                    print('Validation R^2: ' + str(val_r2))\n",
    "                                    print()\n",
    "                                    print()\n",
    "\n",
    "                                    # keep track of best validation R^2 and hyper-parameters\n",
    "                                    if val_r2 > best_val_r2:\n",
    "                                        best_val_r2 = val_r2\n",
    "                                        best_hidden_size_conv = hidden_size_conv\n",
    "                                        best_kernel_size = kernel_size\n",
    "                                        best_hidden_size_full = hidden_size_full\n",
    "                                        best_dropout_full = dropout_full\n",
    "                                        best_hidden_size2_full = hidden_size2_full\n",
    "                                        best_dropout2_full = dropout2_full\n",
    "                                        best_lr = lr\n",
    "                                        best_weight_decay = weight_decay\n",
    "                                        \n",
    "    print('Best validation R^2: ' + str(best_val_r2))\n",
    "    print('Best hidden size conv: ' + str(best_hidden_size_conv))\n",
    "    print('Best kernel size: ' + str(best_kernel_size))\n",
    "    print('Best hidden size full: ' + str(best_hidden_size_full))\n",
    "    print('Best dropout full: ' + str(best_dropout_full))\n",
    "    print('Best hidden size 2 full: ' + str(best_hidden_size2_full))\n",
    "    print('Best dropout 2 full: ' + str(best_dropout2_full))\n",
    "    print('Best learning rate: ' + str(best_lr))\n",
    "    print('Best weight decay: ' + str(best_weight_decay))                               "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Then, train type 2 CNN\n",
    "- this step will pull hyper-parameters from the config file ($\\texttt{/src/config/py_config.ini}$), which, as described above, you may set as the hyper-parameters chosen by validation\n",
    "- the following code is executed in $\\texttt{CNN_train_test.py}$\n",
    "- to run, use $\\texttt{CNN-tt.sh}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Command Line Args for CNN-tt.sh: \n",
    "args_cnn_type = 'cnn_2'\n",
    "args_dataset = 'ridgeImp'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#CODE FROM CNN_train_test.py\n",
    "\n",
    "import torch\n",
    "from torch.autograd import Variable\n",
    "import sklearn.preprocessing\n",
    "import sklearn.metrics\n",
    "# this imported function was created for this package to split datasets (see data_split_tune_utils.py)\n",
    "from data_split_tune_utils import X_y_site_split\n",
    "# these imported functions were created for this package for the purposes of data pre-processing for CNNs, training CNNs,\n",
    "# and evaluation of CNNs (see CNN_utils.py)\n",
    "from CNN_utils import split_sizes_site, split_data, pad_stack_splits, get_monitorData_indices, r2, get_nonConst_vars, train_CNN\n",
    "# these imported classes are the CNN architectures (see CNN_architecture.py)\n",
    "from CNN_architecture import CNN1, CNN2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of variables: 60\n",
      "Total number of non-constant variables: 21\n",
      "Hidden size conv: 25\n",
      "Kernel size: 3\n",
      "Hidden size full: 100\n",
      "Dropout full: 0.1\n",
      "Hidden size 2 full: 100\n",
      "Dropout 2 full: 0.1\n",
      "Learning rate: 0.1\n",
      "Weight decay: 1e-05\n",
      "\n",
      "Epoch loss after epoch 5: 121.91483116149902\n",
      "\n",
      "Epoch loss after epoch 10: 126.83883666992188\n",
      "\n",
      "Epoch loss after epoch 15: 123.53434371948242\n",
      "\n",
      "Epoch loss after epoch 20: 114.59788227081299\n",
      "\n",
      "Epoch loss after epoch 25: 114.38522338867188\n",
      "\n",
      "Epoch loss after epoch 30: 110.7976655960083\n",
      "\n",
      "Epoch loss after epoch 35: 110.20826053619385\n",
      "\n",
      "Epoch loss after epoch 40: 114.17048645019531\n",
      "\n",
      "Epoch loss after epoch 45: 102.69075775146484\n",
      "\n",
      "Epoch loss after epoch 50: 108.55745220184326\n",
      "\n",
      "Epoch loss after epoch 55: 97.1395320892334\n",
      "\n",
      "Epoch loss after epoch 60: 97.53401756286621\n",
      "\n",
      "Epoch loss after epoch 65: 98.39294147491455\n",
      "\n",
      "Epoch loss after epoch 70: 91.31104850769043\n",
      "\n",
      "Epoch loss after epoch 75: 93.83342361450195\n",
      "\n",
      "Epoch loss after epoch 80: 94.49025440216064\n",
      "\n",
      "Epoch loss after epoch 85: 87.22250175476074\n",
      "\n",
      "Epoch loss after epoch 90: 89.67413139343262\n",
      "\n",
      "Epoch loss after epoch 95: 87.56521892547607\n",
      "\n",
      "Epoch loss after epoch 100: 89.45602512359619\n",
      "\n",
      "\n",
      "Test R^2: 0.762719503117\n"
     ]
    }
   ],
   "source": [
    "#CODE FROM CNN_train_test.py\n",
    "\n",
    "# set seeds for reproducibility\n",
    "np.random.seed(1)\n",
    "torch.manual_seed(1)\n",
    "\n",
    "# read in train, val, and test\n",
    "train, val, test = None, None, None\n",
    "\n",
    "\n",
    "if args_dataset == \"ridgeImp\":\n",
    "    train = pd.read_csv(config[\"Ridge_Imputation\"][\"trainV\"])\n",
    "    val   = pd.read_csv(config[\"Ridge_Imputation\"][\"valV\"])\n",
    "    test  = pd.read_csv(config[\"Ridge_Imputation\"][\"testV\"])\n",
    "\n",
    "\n",
    "elif args_dataset == \"rfImp\":\n",
    "    train = pd.read_csv(config[\"RF_Imputation\"][\"trainV\"])\n",
    "    val   = pd.read_csv(config[\"RF_Imputation\"][\"valV\"])\n",
    "    test  = pd.read_csv(config[\"RF_Imputation\"][\"testV\"])\n",
    "\n",
    "\n",
    "# combine train and validation sets into train set\n",
    "train = pd.concat([train, val], axis=0, ignore_index=True)\n",
    "\n",
    "### delete sites from datasets where all monitor outputs are nan\n",
    "train_sites_all_nan_df = train.loc[:, ['site', 'MonitorData']].groupby('site').any()\n",
    "train_sites_to_delete = list(train_sites_all_nan_df[train_sites_all_nan_df['MonitorData']==False].index)\n",
    "train = train[~train['site'].isin(train_sites_to_delete)]\n",
    "\n",
    "\n",
    "# test_sites_all_nan_df = pd.DataFrame(np.isnan(test.groupby('site').sum()['MonitorData']))\n",
    "# test_sites_to_delete = list(test_sites_all_nan_df[test_sites_all_nan_df['MonitorData'] == True].index)\n",
    "# test = test[~test['site'].isin(test_sites_to_delete)]\n",
    "\n",
    "test_sites_all_nan_df = test.loc[:, ['site', 'MonitorData']].groupby('site').any()\n",
    "test_sites_to_delete = list(test_sites_all_nan_df[test_sites_all_nan_df['MonitorData']==False].index)\n",
    "test = test[~test['site'].isin(test_sites_to_delete)]\n",
    "\n",
    "# split train, test into x, y, and sites\n",
    "train_x, train_y, train_sites = X_y_site_split(train, y_var_name='MonitorData', site_var_name='site')\n",
    "test_x, test_y, test_sites = X_y_site_split(test, y_var_name='MonitorData', site_var_name='site')\n",
    "\n",
    "# get dataframes with non-constant features only\n",
    "nonConst_vars = get_nonConst_vars(train, site_var_name='site', y_var_name='MonitorData', cutoff=20)\n",
    "train_x_nonConst = train_x.loc[:, nonConst_vars]\n",
    "test_x_nonConst = test_x.loc[:, nonConst_vars]\n",
    "\n",
    "# standardize all features\n",
    "standardizer_all = sklearn.preprocessing.StandardScaler(with_mean = True, with_std = True)\n",
    "train_x_std_all = standardizer_all.fit_transform(train_x)\n",
    "test_x_std_all = standardizer_all.transform(test_x)\n",
    "\n",
    "# standardize non-constant features\n",
    "standardizer_nonConst = sklearn.preprocessing.StandardScaler(with_mean = True, with_std = True)\n",
    "train_x_std_nonConst = standardizer_nonConst.fit_transform(train_x_nonConst)\n",
    "test_x_std_nonConst = standardizer_nonConst.transform(test_x_nonConst)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# get split sizes for TRAIN data (splitting by site)\n",
    "train_split_sizes = split_sizes_site(train_sites.values)\n",
    "\n",
    "# get tuples by site\n",
    "train_x_std_tuple_nonConst = split_data(torch.from_numpy(train_x_std_nonConst).float(), train_split_sizes, dim = 0)\n",
    "train_x_std_tuple = split_data(torch.from_numpy(train_x_std_all).float(), train_split_sizes, dim = 0)\n",
    "train_y_tuple = split_data(torch.from_numpy(train_y.values), train_split_sizes, dim = 0)\n",
    "\n",
    "# get site sequences stacked into matrix to go through CNN\n",
    "train_x_std_stack_nonConst = pad_stack_splits(train_x_std_tuple_nonConst, np.array(train_split_sizes), 'x')\n",
    "train_x_std_stack_nonConst = Variable(torch.transpose(train_x_std_stack_nonConst, 1, 2))\n",
    "\n",
    "\n",
    "# get split sizes for TEST data (splitting by site)\n",
    "test_split_sizes = split_sizes_site(test_sites.values)\n",
    "\n",
    "# get tuples by site\n",
    "test_x_std_tuple_nonConst = split_data(torch.from_numpy(test_x_std_nonConst).float(), test_split_sizes, dim = 0)\n",
    "test_x_std_tuple = split_data(torch.from_numpy(test_x_std_all).float(), test_split_sizes, dim = 0)\n",
    "test_y_tuple = split_data(torch.from_numpy(test_y.values), test_split_sizes, dim = 0)\n",
    "\n",
    "# get site sequences stacked into matrix to go through CNN\n",
    "test_x_std_stack_nonConst = pad_stack_splits(test_x_std_tuple_nonConst, np.array(test_split_sizes), 'x')\n",
    "test_x_std_stack_nonConst = Variable(torch.transpose(test_x_std_stack_nonConst, 1, 2))\n",
    "\n",
    "\n",
    "# training parameters and model input sizes\n",
    "num_epochs = 50\n",
    "batch_size = 128\n",
    "input_size_conv = train_x_std_nonConst.shape[1]\n",
    "input_size_full = train_x_std_all.shape[1]\n",
    "\n",
    "# train/test CNN1\n",
    "if args_cnn_type == \"cnn_1\":\n",
    "    hidden_size_conv  = int[\"CNN_hyperparam_1\"][\"hidden_size_conv\"]\n",
    "    kernel_size       = int[\"CNN_hyperparam_1\"][\"kernel_size\"]\n",
    "    padding           = int[\"CNN_hyperparam_1\"][\"padding\"]\n",
    "    hidden_size_full  = int[\"CNN_hyperparam_1\"][\"hidden_size_full\"]\n",
    "    dropout_full      = np.float64[\"CNN_hyperparam_1\"][\"dropout_full\"]\n",
    "    hidden_size_combo = config[\"CNN_hyperparam_1\"][\"hidden_size_combo\"]\n",
    "    dropout_combo     = np.float64[\"CNN_hyperparam_1\"][\"dropout_combo\"]\n",
    "    lr                = np.float64[\"CNN_hyperparam_1\"][\"lr\"]\n",
    "    weight_decay      = np.float64[\"CNN_hyperparam_1\"][\"weight_decay\"]\n",
    "\n",
    "    # Loss function\n",
    "    mse_loss = torch.nn.MSELoss(size_average=True)\n",
    "\n",
    "    # instantiate CNN\n",
    "    cnn = CNN1(input_size_conv, hidden_size_conv, kernel_size, padding, input_size_full, hidden_size_full,\n",
    "               dropout_full, hidden_size_combo, dropout_combo)\n",
    "\n",
    "    # instantiate optimizer\n",
    "    optimizer = torch.optim.Adam(cnn.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "\n",
    "    print('Total number of variables: ' + str(input_size_full))\n",
    "    print('Total number of non-constant variables: ' + str(input_size_conv))\n",
    "    print('Hidden size conv: ' + str(hidden_size_conv))\n",
    "    print('Kernel size: ' + str(kernel_size))\n",
    "    print('Hidden size full: ' + str(hidden_size_full))\n",
    "    print('Dropout full: ' + str(dropout_full))\n",
    "    print('Hidden size combo: ' + str(hidden_size_combo))\n",
    "    print('Dropout combo: ' + str(dropout_combo))\n",
    "    print('Learning rate: ' + str(lr))\n",
    "    print('Weight decay: ' + str(weight_decay))\n",
    "    print()\n",
    "\n",
    "    # train\n",
    "    train_CNN(train_x_std_stack_nonConst, train_x_std_tuple, train_y_tuple, cnn, optimizer, mse_loss, num_epochs, batch_size)\n",
    "\n",
    "    # evaluate\n",
    "    test_r2, test_pred_cnn = r2(cnn, batch_size, test_x_std_stack_nonConst, test_x_std_tuple, test_y_tuple, get_pred=True)\n",
    "\n",
    "    print()\n",
    "    print('Test R^2: ' + str(test_r2))\n",
    "\n",
    "    # put model predictions into test dataframe (note that these predictions do not include those for rows where there is no response value)\n",
    "    test = test.dropna(axis=0)\n",
    "    test['MonitorData_pred'] = pd.Series(test_pred_cnn, index=test.index)\n",
    "\n",
    "    # save test dataframe with predictions and final model\n",
    "    \n",
    "    pickle.dump(cnn, open(config[\"Regression\"][\"cnn_1_model\"], 'wb'))\n",
    "    test.to_csv(config[\"Regression\"][\"cnn_1_pred\"], index=False)\n",
    "\n",
    "\n",
    "# train/test CNN2\n",
    "else:\n",
    "    hidden_size_conv  = int(config[\"CNN_hyperparam_2\"][\"hidden_size_conv\"])\n",
    "    kernel_size       = int(config[\"CNN_hyperparam_2\"][\"kernel_size\"])\n",
    "    padding           = int(config[\"CNN_hyperparam_2\"][\"padding\"])\n",
    "    hidden_size_full  = int(config[\"CNN_hyperparam_2\"][\"hidden_size_full\"])\n",
    "    dropout_full      = np.float64(config[\"CNN_hyperparam_2\"][\"dropout_full\"])\n",
    "    hidden_size2_full = int(config[\"CNN_hyperparam_2\"][\"hidden_size2_full\"])\n",
    "    dropout2_full     = np.float64(config[\"CNN_hyperparam_2\"][\"dropout2_full\"])\n",
    "    lr                = np.float64(config[\"CNN_hyperparam_2\"][\"lr\"])\n",
    "    weight_decay      = np.float64(config[\"CNN_hyperparam_2\"][\"weight_decay\"])\n",
    "\n",
    "    # Loss function\n",
    "    mse_loss = torch.nn.MSELoss(size_average=True)\n",
    "\n",
    "    # instantiate CNN\n",
    "    cnn = CNN2(input_size_conv, hidden_size_conv, kernel_size, padding, input_size_full, hidden_size_full,\n",
    "               dropout_full, hidden_size2_full, dropout2_full)\n",
    "\n",
    "    # instantiate optimizer\n",
    "    optimizer = torch.optim.Adam(cnn.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "\n",
    "    print('Total number of variables: ' + str(input_size_full))\n",
    "    print('Total number of non-constant variables: ' + str(input_size_conv))\n",
    "    print('Hidden size conv: ' + str(hidden_size_conv))\n",
    "    print('Kernel size: ' + str(kernel_size))\n",
    "    print('Hidden size full: ' + str(hidden_size_full))\n",
    "    print('Dropout full: ' + str(dropout_full))\n",
    "    print('Hidden size 2 full: ' + str(hidden_size2_full))\n",
    "    print('Dropout 2 full: ' + str(dropout2_full))\n",
    "    print('Learning rate: ' + str(lr))\n",
    "    print('Weight decay: ' + str(weight_decay))\n",
    "    print()\n",
    "\n",
    "    # train\n",
    "    train_CNN(train_x_std_stack_nonConst, train_x_std_tuple, train_y_tuple, cnn, optimizer, mse_loss, num_epochs, batch_size)\n",
    "\n",
    "    # evaluate\n",
    "    test_r2, test_pred_cnn = r2(cnn, batch_size, test_x_std_stack_nonConst, test_x_std_tuple, test_y_tuple, get_pred=True)\n",
    "    \n",
    "    print()\n",
    "    print('Test R^2: ' + str(test_r2))\n",
    "\n",
    "    # put model predictions into test dataframe (note that these predictions do not include those for rows where there is no response value)\n",
    "    test = test.dropna(axis=0)\n",
    "    test['MonitorData_pred'] = pd.Series(test_pred_cnn, index=test.index)\n",
    "\n",
    "    # save test dataframe with predictions and final model\n",
    "    pickle.dump(cnn, open(config[\"Regression\"][\"cnn_2_model\"], 'wb'))\n",
    "    test.to_csv(config[\"Regression\"][\"cnn_2_pred\"], index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.5.3",
   "language": "python",
   "name": "py35"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
