{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import datetime\n",
    "\n",
    "# read in data\n",
    "sensor_census = pd.read_csv('../data/subset_with_sensor_locations_and_census.csv')\n",
    "\n",
    "# extract month from dates and add to dataframe\n",
    "sensor_census['month'] = pd.Series(sensor_census['date'].map(lambda x: datetime.datetime.strptime(x, \"%Y-%m-%d\").month))\n",
    "\n",
    "# add interaction between lat and long\n",
    "sensor_census['lat_long_int'] = pd.Series(sensor_census['Lat'] * sensor_census['Lon'])\n",
    "\n",
    "# convert months to dummy cols\n",
    "sensor_census = pd.get_dummies(sensor_census, columns = ['month'])\n",
    "\n",
    "# have year start at year zero\n",
    "sensor_census['year'] = sensor_census['year'] - sensor_census['year'].min()\n",
    "\n",
    "# keep rows where continental_ind == 1\n",
    "sensor_census = sensor_census[sensor_census['Continental_ind'] == 1]\n",
    "\n",
    "# drop census count data and keep proportions; drop site, city, state, county, zip, country, date;\n",
    "# drop REANALYSIS_windspeed_10m_1Day because all missing\n",
    "sensor_census = sensor_census.drop(['White', 'Black', 'Native', 'Asian', 'Islander', 'Other', 'Two', 'Hispanic', \n",
    "    'Age_0_to_9', 'Age_10_to_19', 'Age_20_to_29','Age_30_to_39','Age_40_to_49','Age_50_to_59','Age_60_to_69', 'Age_70_plus', \n",
    "    'Income_less_than_25k', 'Income_25k_to_50k', 'Income_25k_to_50k', 'Income_50k_to_75k', 'Income_75k_to_100k', 'Income_100k_to_150k', 'Income_150k_to_200k', 'Income_200k_or_more',\n",
    "    'Households', 'Family_Households', 'site', 'City', 'State', 'County', 'Zip', 'Country', 'date', 'REANALYSIS_windspeed_10m_1Day'], axis = 1)\n",
    "\n",
    "# going to try to impute in R, so saving and reloading in R\n",
    "sensor_census.to_csv('../data/subet_sensor_census_toImpute.csv', index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now using R kernel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading required package: mice\n",
      "Loading required package: ImputeRobust\n",
      "Loading required package: gamlss\n",
      "Loading required package: splines\n",
      "Loading required package: gamlss.data\n",
      "Loading required package: gamlss.dist\n",
      "Loading required package: MASS\n",
      "Loading required package: nlme\n",
      "Loading required package: parallel\n",
      " **********   GAMLSS Version 5.0-1  ********** \n",
      "For more on GAMLSS look at http://www.gamlss.org/\n",
      "Type gamlssNews() to see new features/changes/bug fixes.\n",
      "\n",
      "Loading required package: lattice\n"
     ]
    }
   ],
   "source": [
    "require('missForest')\n",
    "install.packa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# read in data\n",
    "sensor_census2 = read.csv('subet_sensor_census_readyToImpute.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# get correlation matrix with for all variables\n",
    "corr.mat = as.matrix(cor(sensor_census2, use = 'pairwise.complete.obs')) - diag(ncol(sensor_census2))\n",
    "\n",
    "# get variable names for variables with high correlations \n",
    "high.corr.colnames = colnames(corr.mat)[apply(corr.mat, 1, max) >= 0.9]\n",
    "\n",
    "# columns to delete\n",
    "to.delete = c('USElevation_dsc10000','USElevation_max100','USElevation_max10000','USElevation_mea10000','USElevation_med100','USElevation_med10000','USElevation_min100',\n",
    "'USElevation_min10000','USElevation_bln100','USElevation_bln10000', 'NLCD_Developed10000', 'NLCD_Impervious10000', 'MAIACUS_Optical_Depth_055_Aqua_Nearest4', \n",
    "'MAIACUS_Optical_Depth_055_Terra_Nearest4', 'REANALYSIS_shum_2m_DailyMax', 'REANALYSIS_prate_DailyMax', 'REANALYSIS_prate_DailyMean', 'REANALYSIS_dlwrf_DailyMean',\n",
    "'REANALYSIS_shum_2m_DailyMin', 'REANALYSIS_shum_2m_1Day', 'REANALYSIS_air_sfc_DailyMin', 'REANALYSIS_air_sfc_DailyMax', 'REANALYSIS_air_sfc_DailyMean',\n",
    "'Nearby_Peak2_MaxTemperature', 'Nearby_Peak2_MinTemperature', 'Nearby_Peak2Lag1_MaxTemperature', 'Nearby_Peak2Lag1_MeanTemperature', 'Nearby_Peak2Lag1_MinTemperature', \n",
    "'Nearby_Peak2Lag3_MaxTemperature', 'Nearby_Peak2Lag3_MinTemperature')\n",
    "\n",
    "# columns to keep that have high correlations by initial analysis\n",
    "high.corr.colnames.del = high.corr.colnames[!(high.corr.colnames %in% to.delete)]\n",
    "\n",
    "# to inspect which variables have high correlations\n",
    "#corr.mat[high.corr.colnames.del, high.corr.colnames.del]\n",
    "\n",
    "# all columns to keep\n",
    "cols.to.keep = names(sensor_census2)[!(names(sensor_census2) %in% to.delete)]\n",
    "\n",
    "# final data for imputation\n",
    "sensor_census2.high.corr.drop = sensor_census2[, cols.to.keep]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "write.csv(sensor_census2.high.corr.drop, 'subet_sensor_census_readyToImpute.csv', row.names = F)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.5.3",
   "language": "python",
   "name": "py35"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
