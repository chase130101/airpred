{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.autograd import Variable\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data import TensorDataset\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sklearn.preprocessing\n",
    "import sklearn.metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# read in imputed data\n",
    "sensor_census_imp = pd.read_csv('../data/sensor_census_imputed_mean.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "np.random.seed(1)\n",
    "\n",
    "# get sites for val/test data\n",
    "val_test_sites = np.random.choice(np.unique(sensor_census_imp['site'].values), round(len(np.unique(sensor_census_imp['site'].values))/5), replace = False)\n",
    "\n",
    "# get sites for test data\n",
    "test_sites = np.random.choice(np.unique(val_test_sites), round(len(np.unique(val_test_sites))/2), replace = False)\n",
    "\n",
    "# train sites/rows and x/y split\n",
    "sensor_census_imp_train = sensor_census_imp[~sensor_census_imp['site'].isin(val_test_sites)]\n",
    "sensor_census_imp_train_x = sensor_census_imp_train.iloc[:, 2:]\n",
    "sensor_census_imp_train_y = sensor_census_imp_train.iloc[:, 1]\n",
    "\n",
    "# val sites/rows and x/y split\n",
    "sensor_census_imp_val = sensor_census_imp[(sensor_census_imp['site'].isin(val_test_sites)) & (~sensor_census_imp['site'].isin(test_sites))]\n",
    "sensor_census_imp_val_x = sensor_census_imp_val.iloc[:, 2:]\n",
    "sensor_census_imp_val_y = sensor_census_imp_val.iloc[:, 1]\n",
    "\n",
    "# test sites/rows and x/y split\n",
    "sensor_census_imp_test = sensor_census_imp[sensor_census_imp['site'].isin(test_sites)]\n",
    "sensor_census_imp_test_x = sensor_census_imp_test.iloc[:, 2:]\n",
    "sensor_census_imp_test_y = sensor_census_imp_test.iloc[:, 1]\n",
    "\n",
    "# standardize train, val, and test data\n",
    "standardizer = sklearn.preprocessing.StandardScaler(with_mean = True, with_std = True)\n",
    "sensor_census_imp_train_x_stand = standardizer.fit_transform(sensor_census_imp_train_x)\n",
    "sensor_census_imp_val_x_stand = standardizer.transform(sensor_census_imp_val_x)\n",
    "sensor_census_imp_test_x_stand = standardizer.transform(sensor_census_imp_test_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# create torch tensor tuples for train, val, test\n",
    "train = TensorDataset(torch.from_numpy(sensor_census_imp_train_x_stand), torch.from_numpy(sensor_census_imp_train_y.values))\n",
    "val = TensorDataset(torch.from_numpy(sensor_census_imp_val_x_stand), torch.from_numpy(sensor_census_imp_val_y.values))\n",
    "test = TensorDataset(torch.from_numpy(sensor_census_imp_test_x_stand), torch.from_numpy(sensor_census_imp_test_y.values))\n",
    "\n",
    "# create batches\n",
    "batch_size = 100\n",
    "train_loader = torch.utils.data.DataLoader(dataset = train, batch_size = batch_size, shuffle = True)\n",
    "val_loader = torch.utils.data.DataLoader(dataset = val, batch_size = batch_size, shuffle = False)\n",
    "test_loader = torch.utils.data.DataLoader(dataset = test, batch_size = batch_size, shuffle = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# function for computing accuracy\n",
    "def compute_accuracy(data_loader, model):\n",
    "    model_pred = []\n",
    "    targets = []\n",
    "    for batch in data_loader:\n",
    "        model_output = model(Variable(batch[0]).float()).data.numpy() # model preds\n",
    "        model_pred += model_output.tolist() # add to list of preds\n",
    "        targets += batch[1].numpy().tolist() # true digit values\n",
    "        \n",
    "    return sklearn.metrics.r2_score(targets, model_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lambda=0.1, learning rate=0.1, hidden units=33, activation=tanh\n",
      "0.700580851153 0\n",
      "lambda=0.1, learning rate=0.1, hidden units=33, activation=ReLU\n",
      "0.661431065372 1\n",
      "lambda=0.1, learning rate=0.1, hidden units=67, activation=tanh\n",
      "0.700162941688 2\n",
      "lambda=0.1, learning rate=0.1, hidden units=67, activation=ReLU\n",
      "0.717577509307 3\n",
      "lambda=0.1, learning rate=0.1, hidden units=100, activation=tanh\n",
      "0.659220283322 4\n",
      "lambda=0.1, learning rate=0.1, hidden units=100, activation=ReLU\n",
      "0.682076613295 5\n",
      "lambda=0.1, learning rate=0.1, hidden units=133, activation=tanh\n",
      "0.709378232682 6\n",
      "lambda=0.1, learning rate=0.1, hidden units=133, activation=ReLU\n",
      "0.727576246348 7\n",
      "lambda=0.1, learning rate=0.1, hidden units=166, activation=tanh\n",
      "0.683327903289 8\n",
      "lambda=0.1, learning rate=0.1, hidden units=166, activation=ReLU\n",
      "0.717064907438 9\n",
      "lambda=0.001, learning rate=0.1, hidden units=33, activation=tanh\n",
      "0.713091079133 10\n",
      "lambda=0.001, learning rate=0.1, hidden units=33, activation=ReLU\n",
      "0.74474205513 11\n",
      "lambda=0.001, learning rate=0.1, hidden units=67, activation=tanh\n",
      "0.666197910879 12\n",
      "lambda=0.001, learning rate=0.1, hidden units=67, activation=ReLU\n",
      "0.727967067774 13\n",
      "lambda=0.001, learning rate=0.1, hidden units=100, activation=tanh\n",
      "0.750348959683 14\n",
      "lambda=0.001, learning rate=0.1, hidden units=100, activation=ReLU\n",
      "0.764561803991 15\n",
      "lambda=0.001, learning rate=0.1, hidden units=133, activation=tanh\n",
      "0.72838210862 16\n",
      "lambda=0.001, learning rate=0.1, hidden units=133, activation=ReLU\n",
      "0.7463748058 17\n",
      "lambda=0.001, learning rate=0.1, hidden units=166, activation=tanh\n",
      "0.760758172834 18\n",
      "lambda=0.001, learning rate=0.1, hidden units=166, activation=ReLU\n",
      "0.76070836254 19\n",
      "lambda=1e-05, learning rate=0.1, hidden units=33, activation=tanh\n",
      "0.691007806319 20\n",
      "lambda=1e-05, learning rate=0.1, hidden units=33, activation=ReLU\n",
      "0.708116923943 21\n",
      "lambda=1e-05, learning rate=0.1, hidden units=67, activation=tanh\n",
      "0.696315251862 22\n",
      "lambda=1e-05, learning rate=0.1, hidden units=67, activation=ReLU\n",
      "0.758367725664 23\n",
      "lambda=1e-05, learning rate=0.1, hidden units=100, activation=tanh\n",
      "0.566206279069 24\n",
      "lambda=1e-05, learning rate=0.1, hidden units=100, activation=ReLU\n",
      "0.744848575053 25\n",
      "lambda=1e-05, learning rate=0.1, hidden units=133, activation=tanh\n",
      "0.658378726023 26\n",
      "lambda=1e-05, learning rate=0.1, hidden units=133, activation=ReLU\n",
      "0.745701197956 27\n",
      "lambda=1e-05, learning rate=0.1, hidden units=166, activation=tanh\n",
      "0.685934749051 28\n",
      "lambda=1e-05, learning rate=0.1, hidden units=166, activation=ReLU\n",
      "0.763885374536 29\n",
      "lambda=0, learning rate=0.1, hidden units=33, activation=tanh\n",
      "0.679547222501 30\n",
      "lambda=0, learning rate=0.1, hidden units=33, activation=ReLU\n",
      "0.760604144476 31\n",
      "lambda=0, learning rate=0.1, hidden units=67, activation=tanh\n",
      "0.68033490096 32\n",
      "lambda=0, learning rate=0.1, hidden units=67, activation=ReLU\n",
      "0.772551093853 33\n",
      "lambda=0, learning rate=0.1, hidden units=100, activation=tanh\n",
      "0.660564802362 34\n",
      "lambda=0, learning rate=0.1, hidden units=100, activation=ReLU\n",
      "0.732171059283 35\n",
      "lambda=0, learning rate=0.1, hidden units=133, activation=tanh\n",
      "0.685280055509 36\n",
      "lambda=0, learning rate=0.1, hidden units=133, activation=ReLU\n",
      "0.740724778167 37\n",
      "lambda=0, learning rate=0.1, hidden units=166, activation=tanh\n",
      "0.640045899354 38\n",
      "lambda=0, learning rate=0.1, hidden units=166, activation=ReLU\n",
      "0.774519872728 39\n",
      "lambda=0.1, learning rate=0.01, hidden units=33, activation=tanh\n",
      "0.755949937966 40\n",
      "lambda=0.1, learning rate=0.01, hidden units=33, activation=ReLU\n",
      "0.763181189444 41\n",
      "lambda=0.1, learning rate=0.01, hidden units=67, activation=tanh\n",
      "0.724244237437 42\n",
      "lambda=0.1, learning rate=0.01, hidden units=67, activation=ReLU\n",
      "0.734199490042 43\n",
      "lambda=0.1, learning rate=0.01, hidden units=100, activation=tanh\n",
      "0.756420795083 44\n",
      "lambda=0.1, learning rate=0.01, hidden units=100, activation=ReLU\n",
      "0.769963947175 45\n",
      "lambda=0.1, learning rate=0.01, hidden units=133, activation=tanh\n",
      "0.710624498106 46\n",
      "lambda=0.1, learning rate=0.01, hidden units=133, activation=ReLU\n",
      "0.777705102662 47\n",
      "lambda=0.1, learning rate=0.01, hidden units=166, activation=tanh\n",
      "0.709886630882 48\n",
      "lambda=0.1, learning rate=0.01, hidden units=166, activation=ReLU\n",
      "0.75667580887 49\n",
      "lambda=0.001, learning rate=0.01, hidden units=33, activation=tanh\n",
      "0.704312511616 50\n",
      "lambda=0.001, learning rate=0.01, hidden units=33, activation=ReLU\n",
      "0.736464725 51\n",
      "lambda=0.001, learning rate=0.01, hidden units=67, activation=tanh\n",
      "0.709233861824 52\n",
      "lambda=0.001, learning rate=0.01, hidden units=67, activation=ReLU\n",
      "0.73719369731 53\n",
      "lambda=0.001, learning rate=0.01, hidden units=100, activation=tanh\n",
      "0.678964232364 54\n",
      "lambda=0.001, learning rate=0.01, hidden units=100, activation=ReLU\n",
      "0.752885140858 55\n",
      "lambda=0.001, learning rate=0.01, hidden units=133, activation=tanh\n",
      "0.642359923818 56\n",
      "lambda=0.001, learning rate=0.01, hidden units=133, activation=ReLU\n",
      "0.766559660303 57\n",
      "lambda=0.001, learning rate=0.01, hidden units=166, activation=tanh\n",
      "0.683248997013 58\n",
      "lambda=0.001, learning rate=0.01, hidden units=166, activation=ReLU\n",
      "0.7211192124 59\n",
      "lambda=1e-05, learning rate=0.01, hidden units=33, activation=tanh\n",
      "0.709577780764 60\n",
      "lambda=1e-05, learning rate=0.01, hidden units=33, activation=ReLU\n",
      "0.747239361792 61\n",
      "lambda=1e-05, learning rate=0.01, hidden units=67, activation=tanh\n",
      "0.684000438549 62\n",
      "lambda=1e-05, learning rate=0.01, hidden units=67, activation=ReLU\n",
      "0.754657226274 63\n",
      "lambda=1e-05, learning rate=0.01, hidden units=100, activation=tanh\n",
      "0.689353182918 64\n",
      "lambda=1e-05, learning rate=0.01, hidden units=100, activation=ReLU\n",
      "0.740830347578 65\n",
      "lambda=1e-05, learning rate=0.01, hidden units=133, activation=tanh\n",
      "0.676337114327 66\n",
      "lambda=1e-05, learning rate=0.01, hidden units=133, activation=ReLU\n",
      "0.741694787013 67\n",
      "lambda=1e-05, learning rate=0.01, hidden units=166, activation=tanh\n",
      "0.669537820417 68\n",
      "lambda=1e-05, learning rate=0.01, hidden units=166, activation=ReLU\n",
      "0.73821032758 69\n",
      "lambda=0, learning rate=0.01, hidden units=33, activation=tanh\n",
      "0.714891403024 70\n",
      "lambda=0, learning rate=0.01, hidden units=33, activation=ReLU\n",
      "0.727631030874 71\n",
      "lambda=0, learning rate=0.01, hidden units=67, activation=tanh\n",
      "0.665728270615 72\n",
      "lambda=0, learning rate=0.01, hidden units=67, activation=ReLU\n",
      "0.75624186799 73\n",
      "lambda=0, learning rate=0.01, hidden units=100, activation=tanh\n",
      "0.676189248286 74\n",
      "lambda=0, learning rate=0.01, hidden units=100, activation=ReLU\n",
      "0.753337008278 75\n",
      "lambda=0, learning rate=0.01, hidden units=133, activation=tanh\n",
      "0.677962713078 76\n",
      "lambda=0, learning rate=0.01, hidden units=133, activation=ReLU\n",
      "0.730997051962 77\n",
      "lambda=0, learning rate=0.01, hidden units=166, activation=tanh\n",
      "0.653796806446 78\n",
      "lambda=0, learning rate=0.01, hidden units=166, activation=ReLU\n",
      "0.739154069333 79\n",
      "lambda=0.1, learning rate=0.001, hidden units=33, activation=tanh\n",
      "0.757270307815 80\n",
      "lambda=0.1, learning rate=0.001, hidden units=33, activation=ReLU\n",
      "0.764912210426 81\n",
      "lambda=0.1, learning rate=0.001, hidden units=67, activation=tanh\n",
      "0.763661923217 82\n",
      "lambda=0.1, learning rate=0.001, hidden units=67, activation=ReLU\n",
      "0.758875593447 83\n",
      "lambda=0.1, learning rate=0.001, hidden units=100, activation=tanh\n",
      "0.774846314465 84\n",
      "lambda=0.1, learning rate=0.001, hidden units=100, activation=ReLU\n",
      "0.776432670123 85\n",
      "lambda=0.1, learning rate=0.001, hidden units=133, activation=tanh\n",
      "0.759402662582 86\n",
      "lambda=0.1, learning rate=0.001, hidden units=133, activation=ReLU\n",
      "0.763458011109 87\n",
      "lambda=0.1, learning rate=0.001, hidden units=166, activation=tanh\n",
      "0.760734190766 88\n",
      "lambda=0.1, learning rate=0.001, hidden units=166, activation=ReLU\n",
      "0.753783290854 89\n",
      "lambda=0.001, learning rate=0.001, hidden units=33, activation=tanh\n",
      "0.747437618596 90\n",
      "lambda=0.001, learning rate=0.001, hidden units=33, activation=ReLU\n",
      "0.768720836938 91\n",
      "lambda=0.001, learning rate=0.001, hidden units=67, activation=tanh\n",
      "0.767355748583 92\n",
      "lambda=0.001, learning rate=0.001, hidden units=67, activation=ReLU\n",
      "0.762560877206 93\n",
      "lambda=0.001, learning rate=0.001, hidden units=100, activation=tanh\n",
      "0.761863059674 94\n",
      "lambda=0.001, learning rate=0.001, hidden units=100, activation=ReLU\n",
      "0.760328553526 95\n",
      "lambda=0.001, learning rate=0.001, hidden units=133, activation=tanh\n",
      "0.760465608612 96\n",
      "lambda=0.001, learning rate=0.001, hidden units=133, activation=ReLU\n",
      "0.750428455953 97\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lambda=0.001, learning rate=0.001, hidden units=166, activation=tanh\n",
      "0.770508772179 98\n",
      "lambda=0.001, learning rate=0.001, hidden units=166, activation=ReLU\n",
      "0.76685617172 99\n",
      "lambda=1e-05, learning rate=0.001, hidden units=33, activation=tanh\n",
      "0.757861813422 100\n",
      "lambda=1e-05, learning rate=0.001, hidden units=33, activation=ReLU\n",
      "0.760870712753 101\n",
      "lambda=1e-05, learning rate=0.001, hidden units=67, activation=tanh\n",
      "0.765369604424 102\n",
      "lambda=1e-05, learning rate=0.001, hidden units=67, activation=ReLU\n",
      "0.751207458732 103\n",
      "lambda=1e-05, learning rate=0.001, hidden units=100, activation=tanh\n",
      "0.75831694688 104\n",
      "lambda=1e-05, learning rate=0.001, hidden units=100, activation=ReLU\n",
      "0.774101981496 105\n",
      "lambda=1e-05, learning rate=0.001, hidden units=133, activation=tanh\n",
      "0.754032037598 106\n",
      "lambda=1e-05, learning rate=0.001, hidden units=133, activation=ReLU\n",
      "0.763546443594 107\n",
      "lambda=1e-05, learning rate=0.001, hidden units=166, activation=tanh\n",
      "0.761086101139 108\n",
      "lambda=1e-05, learning rate=0.001, hidden units=166, activation=ReLU\n",
      "0.760425682169 109\n",
      "lambda=0, learning rate=0.001, hidden units=33, activation=tanh\n",
      "0.759368923635 110\n",
      "lambda=0, learning rate=0.001, hidden units=33, activation=ReLU\n",
      "0.764158521548 111\n",
      "lambda=0, learning rate=0.001, hidden units=67, activation=tanh\n",
      "0.759623284607 112\n",
      "lambda=0, learning rate=0.001, hidden units=67, activation=ReLU\n",
      "0.751723184668 113\n",
      "lambda=0, learning rate=0.001, hidden units=100, activation=tanh\n",
      "0.752377092047 114\n",
      "lambda=0, learning rate=0.001, hidden units=100, activation=ReLU\n",
      "0.760570019177 115\n",
      "lambda=0, learning rate=0.001, hidden units=133, activation=tanh\n",
      "0.743680145103 116\n",
      "lambda=0, learning rate=0.001, hidden units=133, activation=ReLU\n",
      "0.77024379604 117\n",
      "lambda=0, learning rate=0.001, hidden units=166, activation=tanh\n",
      "0.757350088457 118\n",
      "lambda=0, learning rate=0.001, hidden units=166, activation=ReLU\n",
      "0.754511718744 119\n",
      "lambda=0.1, learning rate=0.01, hidden units=133, activation=ReLU\n",
      "Train R2: 0.777705102662\n",
      "Val R2: 0.777705102662\n",
      "Test R2: 0.735088939815\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 25\n",
    "\n",
    "# hyperparams to test\n",
    "lrs = [0.1, 0.01, 0.001]\n",
    "lams = [0.1, 0.001, 0.00001, 0]\n",
    "hiddens = [33, 67, 100, 133, 166]\n",
    "activations = ['tanh', 'ReLU']\n",
    "\n",
    "count = 0\n",
    "max_val_r2 = 0\n",
    "for lr in lrs:\n",
    "    for lam in lams:\n",
    "        for hidden in hiddens:\n",
    "            for activation in activations:\n",
    "\n",
    "                # model architecture\n",
    "                ff_nn = torch.nn.Sequential()\n",
    "                ff_nn.add_module(name = '1st linear', module = torch.nn.Linear(in_features = sensor_census_imp_train_x_stand.shape[1], out_features = hidden, bias = True)) # linear layer\n",
    "                ff_nn.add_module(name = 'norm', module = torch.nn.BatchNorm1d(num_features = hidden)) # normalize before tanh\n",
    "                \n",
    "                if activation == 'tanh':\n",
    "                    ff_nn.add_module(name = 'tanh', module = torch.nn.Tanh()) # tanh function for hidden layer\n",
    "                    \n",
    "                elif activation == 'ReLU':\n",
    "                    ff_nn.add_module(name = 'ReLU', module = torch.nn.ReLU()) # tanh function for hidden layer\n",
    "                    \n",
    "                ff_nn.add_module(name = '2nd linear', module = torch.nn.Linear(in_features = hidden, out_features = 1, bias = True)) # 2nd linear layer\n",
    "\n",
    "                # mse loss\n",
    "                loss_function = torch.nn.MSELoss()\n",
    "\n",
    "                # adam optimizer\n",
    "                adam = torch.optim.Adam(ff_nn.parameters(), lr=lr, weight_decay=lam)\n",
    "\n",
    "                for epoch in range(num_epochs): \n",
    "                    for batch in train_loader:\n",
    "                        model_output = ff_nn(Variable(batch[0]).float()) # model predictions\n",
    "                        targets = Variable(batch[1].float()) # true digit values\n",
    "\n",
    "                        adam.zero_grad() # zero gradient\n",
    "                        loss_batch = loss_function(model_output, targets) # compute loss\n",
    "                        loss_batch.backward() # take the gradient wrt parameters\n",
    "                        adam.step() # update parameters\n",
    "                \n",
    "                # get validation R2\n",
    "                val_r2 = compute_accuracy(val_loader, ff_nn)\n",
    "                \n",
    "                print('lambda=' + str(lam) + ', learning rate=' + str(lr) + ', hidden units=' + str(hidden) + ', activation=' + activation)\n",
    "                print(str(val_r2) + ' ' + str(count))\n",
    "                count += 1\n",
    "                \n",
    "                if val_r2 > max_val_r2: # compare current validation R2 to best R2 obtained so far\n",
    "                    final_lr = lr\n",
    "                    final_lam = lam\n",
    "                    final_hidden = hidden\n",
    "                    final_activation = activation\n",
    "                    train_r2 = compute_accuracy(val_loader, ff_nn)\n",
    "                    max_val_r2 = compute_accuracy(val_loader, ff_nn)\n",
    "                    test_r2 = compute_accuracy(test_loader, ff_nn)\n",
    "                        \n",
    "\n",
    "print('lambda=' + str(final_lam) + ', learning rate=' + str(final_lr) + ', hidden units=' + str(final_hidden) + ', activation=' + final_activation)\n",
    "print('Train R2: ' + str(train_r2))\n",
    "print('Val R2: ' + str(max_val_r2))\n",
    "print('Test R2: ' + str(test_r2))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.5.3",
   "language": "python",
   "name": "py35"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
