{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.autograd import Variable\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data import TensorDataset\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sklearn.preprocessing\n",
    "import sklearn.metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read in imputed data\n",
    "sensor_census_imp = pd.read_csv('../data/sensor_census_imputed_rf.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(1)\n",
    "\n",
    "# get sites for val/test data\n",
    "val_test_sites = np.random.choice(np.unique(sensor_census_imp['site'].values), round(len(np.unique(sensor_census_imp['site'].values))/5), replace = False)\n",
    "\n",
    "# get sites for test data\n",
    "test_sites = np.random.choice(np.unique(val_test_sites), round(len(np.unique(val_test_sites))/2), replace = False)\n",
    "\n",
    "# train sites/rows and x/y split\n",
    "sensor_census_imp_train = sensor_census_imp[~sensor_census_imp['site'].isin(val_test_sites)]\n",
    "sensor_census_imp_train_x = sensor_census_imp_train.iloc[:, 2:]\n",
    "sensor_census_imp_train_y = sensor_census_imp_train.iloc[:, 1]\n",
    "\n",
    "# val sites/rows and x/y split\n",
    "sensor_census_imp_val = sensor_census_imp[(sensor_census_imp['site'].isin(val_test_sites)) & (~sensor_census_imp['site'].isin(test_sites))]\n",
    "sensor_census_imp_val_x = sensor_census_imp_val.iloc[:, 2:]\n",
    "sensor_census_imp_val_y = sensor_census_imp_val.iloc[:, 1]\n",
    "\n",
    "# test sites/rows and x/y split\n",
    "sensor_census_imp_test = sensor_census_imp[sensor_census_imp['site'].isin(test_sites)]\n",
    "sensor_census_imp_test_x = sensor_census_imp_test.iloc[:, 2:]\n",
    "sensor_census_imp_test_y = sensor_census_imp_test.iloc[:, 1]\n",
    "\n",
    "# standardize train, val, and test data\n",
    "standardizer = sklearn.preprocessing.StandardScaler(with_mean = True, with_std = True)\n",
    "sensor_census_imp_train_x_stand = standardizer.fit_transform(sensor_census_imp_train_x)\n",
    "sensor_census_imp_val_x_stand = standardizer.transform(sensor_census_imp_val_x)\n",
    "sensor_census_imp_test_x_stand = standardizer.transform(sensor_census_imp_test_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create torch tensor tuples for train, val, test\n",
    "train = TensorDataset(torch.from_numpy(sensor_census_imp_train_x_stand), torch.from_numpy(sensor_census_imp_train_y.values))\n",
    "val = TensorDataset(torch.from_numpy(sensor_census_imp_val_x_stand), torch.from_numpy(sensor_census_imp_val_y.values))\n",
    "test = TensorDataset(torch.from_numpy(sensor_census_imp_test_x_stand), torch.from_numpy(sensor_census_imp_test_y.values))\n",
    "\n",
    "# create batches\n",
    "batch_size = 100\n",
    "train_loader = torch.utils.data.DataLoader(dataset = train, batch_size = batch_size, shuffle = True)\n",
    "val_loader = torch.utils.data.DataLoader(dataset = val, batch_size = batch_size, shuffle = False)\n",
    "test_loader = torch.utils.data.DataLoader(dataset = test, batch_size = batch_size, shuffle = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# function for computing accuracy\n",
    "def compute_accuracy(data_loader, model):\n",
    "    model_pred = []\n",
    "    targets = []\n",
    "    for batch in data_loader:\n",
    "        model_output = model(Variable(batch[0]).float()).data.numpy() # model preds\n",
    "        model_pred += model_output.tolist() # add to list of preds\n",
    "        targets += batch[1].numpy().tolist() # true digit values\n",
    "        \n",
    "    return sklearn.metrics.r2_score(targets, model_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lambda=0.1, learning rate=0.1, hidden units=33, activation=tanh\n",
      "0.671635039623 0\n",
      "lambda=0.1, learning rate=0.1, hidden units=33, activation=ReLU\n",
      "0.697424717558 1\n",
      "lambda=0.1, learning rate=0.1, hidden units=67, activation=tanh\n",
      "0.668153921417 2\n",
      "lambda=0.1, learning rate=0.1, hidden units=67, activation=ReLU\n",
      "0.694663874097 3\n",
      "lambda=0.1, learning rate=0.1, hidden units=100, activation=tanh\n",
      "0.684034833585 4\n",
      "lambda=0.1, learning rate=0.1, hidden units=100, activation=ReLU\n",
      "0.715791100132 5\n",
      "lambda=0.1, learning rate=0.1, hidden units=133, activation=tanh\n",
      "0.696561124378 6\n",
      "lambda=0.1, learning rate=0.1, hidden units=133, activation=ReLU\n",
      "0.695907746724 7\n",
      "lambda=0.1, learning rate=0.1, hidden units=166, activation=tanh\n",
      "0.575040774688 8\n",
      "lambda=0.1, learning rate=0.1, hidden units=166, activation=ReLU\n",
      "0.707583862344 9\n",
      "lambda=0.001, learning rate=0.1, hidden units=33, activation=tanh\n",
      "0.757113680256 10\n",
      "lambda=0.001, learning rate=0.1, hidden units=33, activation=ReLU\n",
      "0.705366835038 11\n",
      "lambda=0.001, learning rate=0.1, hidden units=67, activation=tanh\n",
      "0.748158102803 12\n",
      "lambda=0.001, learning rate=0.1, hidden units=67, activation=ReLU\n",
      "0.701116636187 13\n",
      "lambda=0.001, learning rate=0.1, hidden units=100, activation=tanh\n",
      "0.677735395909 14\n",
      "lambda=0.001, learning rate=0.1, hidden units=100, activation=ReLU\n",
      "0.780041101904 15\n",
      "lambda=0.001, learning rate=0.1, hidden units=133, activation=tanh\n",
      "0.717662050896 16\n",
      "lambda=0.001, learning rate=0.1, hidden units=133, activation=ReLU\n",
      "0.761328738384 17\n",
      "lambda=0.001, learning rate=0.1, hidden units=166, activation=tanh\n",
      "0.700271402986 18\n",
      "lambda=0.001, learning rate=0.1, hidden units=166, activation=ReLU\n",
      "0.752138852395 19\n",
      "lambda=1e-05, learning rate=0.1, hidden units=33, activation=tanh\n",
      "0.617873973113 20\n",
      "lambda=1e-05, learning rate=0.1, hidden units=33, activation=ReLU\n",
      "0.730222479606 21\n",
      "lambda=1e-05, learning rate=0.1, hidden units=67, activation=tanh\n",
      "0.668944158716 22\n",
      "lambda=1e-05, learning rate=0.1, hidden units=67, activation=ReLU\n",
      "0.758784372856 23\n",
      "lambda=1e-05, learning rate=0.1, hidden units=100, activation=tanh\n",
      "0.702653548406 24\n",
      "lambda=1e-05, learning rate=0.1, hidden units=100, activation=ReLU\n",
      "0.728197924516 25\n",
      "lambda=1e-05, learning rate=0.1, hidden units=133, activation=tanh\n",
      "0.603031723365 26\n",
      "lambda=1e-05, learning rate=0.1, hidden units=133, activation=ReLU\n",
      "0.769869052282 27\n",
      "lambda=1e-05, learning rate=0.1, hidden units=166, activation=tanh\n",
      "0.691725539722 28\n",
      "lambda=1e-05, learning rate=0.1, hidden units=166, activation=ReLU\n",
      "0.741336904135 29\n",
      "lambda=0, learning rate=0.1, hidden units=33, activation=tanh\n",
      "0.650108060644 30\n",
      "lambda=0, learning rate=0.1, hidden units=33, activation=ReLU\n",
      "0.774688233211 31\n",
      "lambda=0, learning rate=0.1, hidden units=67, activation=tanh\n",
      "0.709639536441 32\n",
      "lambda=0, learning rate=0.1, hidden units=67, activation=ReLU\n",
      "0.748991400514 33\n",
      "lambda=0, learning rate=0.1, hidden units=100, activation=tanh\n",
      "0.697602886508 34\n",
      "lambda=0, learning rate=0.1, hidden units=100, activation=ReLU\n",
      "0.703440755894 35\n",
      "lambda=0, learning rate=0.1, hidden units=133, activation=tanh\n",
      "0.665874409984 36\n",
      "lambda=0, learning rate=0.1, hidden units=133, activation=ReLU\n",
      "0.7465325419 37\n",
      "lambda=0, learning rate=0.1, hidden units=166, activation=tanh\n",
      "0.670307762945 38\n",
      "lambda=0, learning rate=0.1, hidden units=166, activation=ReLU\n",
      "0.656102407863 39\n",
      "lambda=0.1, learning rate=0.01, hidden units=33, activation=tanh\n",
      "0.768037602398 40\n",
      "lambda=0.1, learning rate=0.01, hidden units=33, activation=ReLU\n",
      "0.741390931204 41\n",
      "lambda=0.1, learning rate=0.01, hidden units=67, activation=tanh\n",
      "0.759170509671 42\n",
      "lambda=0.1, learning rate=0.01, hidden units=67, activation=ReLU\n",
      "0.755609169798 43\n",
      "lambda=0.1, learning rate=0.01, hidden units=100, activation=tanh\n",
      "0.758016983059 44\n",
      "lambda=0.1, learning rate=0.01, hidden units=100, activation=ReLU\n",
      "0.761380684668 45\n",
      "lambda=0.1, learning rate=0.01, hidden units=133, activation=tanh\n",
      "0.693162309319 46\n",
      "lambda=0.1, learning rate=0.01, hidden units=133, activation=ReLU\n",
      "0.766345620071 47\n",
      "lambda=0.1, learning rate=0.01, hidden units=166, activation=tanh\n",
      "0.738353360558 48\n",
      "lambda=0.1, learning rate=0.01, hidden units=166, activation=ReLU\n",
      "0.655883508806 49\n",
      "lambda=0.001, learning rate=0.01, hidden units=33, activation=tanh\n",
      "0.716562440222 50\n",
      "lambda=0.001, learning rate=0.01, hidden units=33, activation=ReLU\n",
      "0.754024360188 51\n",
      "lambda=0.001, learning rate=0.01, hidden units=67, activation=tanh\n",
      "0.719691006489 52\n",
      "lambda=0.001, learning rate=0.01, hidden units=67, activation=ReLU\n",
      "0.770077776637 53\n",
      "lambda=0.001, learning rate=0.01, hidden units=100, activation=tanh\n",
      "0.710107123359 54\n",
      "lambda=0.001, learning rate=0.01, hidden units=100, activation=ReLU\n",
      "0.735647681262 55\n",
      "lambda=0.001, learning rate=0.01, hidden units=133, activation=tanh\n",
      "0.698860334532 56\n",
      "lambda=0.001, learning rate=0.01, hidden units=133, activation=ReLU\n",
      "0.732747698245 57\n",
      "lambda=0.001, learning rate=0.01, hidden units=166, activation=tanh\n",
      "0.694888515107 58\n",
      "lambda=0.001, learning rate=0.01, hidden units=166, activation=ReLU\n",
      "0.74837894384 59\n",
      "lambda=1e-05, learning rate=0.01, hidden units=33, activation=tanh\n",
      "0.712949892441 60\n",
      "lambda=1e-05, learning rate=0.01, hidden units=33, activation=ReLU\n",
      "0.758534146409 61\n",
      "lambda=1e-05, learning rate=0.01, hidden units=67, activation=tanh\n",
      "0.679490792922 62\n",
      "lambda=1e-05, learning rate=0.01, hidden units=67, activation=ReLU\n",
      "0.743003585277 63\n",
      "lambda=1e-05, learning rate=0.01, hidden units=100, activation=tanh\n",
      "0.700893380203 64\n",
      "lambda=1e-05, learning rate=0.01, hidden units=100, activation=ReLU\n",
      "0.716909150866 65\n",
      "lambda=1e-05, learning rate=0.01, hidden units=133, activation=tanh\n",
      "0.687706462148 66\n",
      "lambda=1e-05, learning rate=0.01, hidden units=133, activation=ReLU\n",
      "0.748198223609 67\n",
      "lambda=1e-05, learning rate=0.01, hidden units=166, activation=tanh\n",
      "0.688975721926 68\n",
      "lambda=1e-05, learning rate=0.01, hidden units=166, activation=ReLU\n",
      "0.730333772808 69\n",
      "lambda=0, learning rate=0.01, hidden units=33, activation=tanh\n",
      "0.72702327045 70\n",
      "lambda=0, learning rate=0.01, hidden units=33, activation=ReLU\n",
      "0.772155830446 71\n",
      "lambda=0, learning rate=0.01, hidden units=67, activation=tanh\n",
      "0.702379896038 72\n",
      "lambda=0, learning rate=0.01, hidden units=67, activation=ReLU\n",
      "0.734786746834 73\n",
      "lambda=0, learning rate=0.01, hidden units=100, activation=tanh\n",
      "0.697846017294 74\n",
      "lambda=0, learning rate=0.01, hidden units=100, activation=ReLU\n",
      "0.695350205498 75\n",
      "lambda=0, learning rate=0.01, hidden units=133, activation=tanh\n",
      "0.692809672582 76\n",
      "lambda=0, learning rate=0.01, hidden units=133, activation=ReLU\n",
      "0.709212364918 77\n",
      "lambda=0, learning rate=0.01, hidden units=166, activation=tanh\n",
      "0.6645683127 78\n",
      "lambda=0, learning rate=0.01, hidden units=166, activation=ReLU\n",
      "0.677438987784 79\n",
      "lambda=0.1, learning rate=0.001, hidden units=33, activation=tanh\n",
      "0.760456074659 80\n",
      "lambda=0.1, learning rate=0.001, hidden units=33, activation=ReLU\n",
      "0.769545754736 81\n",
      "lambda=0.1, learning rate=0.001, hidden units=67, activation=tanh\n",
      "0.763497278042 82\n",
      "lambda=0.1, learning rate=0.001, hidden units=67, activation=ReLU\n",
      "0.755236016419 83\n",
      "lambda=0.1, learning rate=0.001, hidden units=100, activation=tanh\n",
      "0.764165512981 84\n",
      "lambda=0.1, learning rate=0.001, hidden units=100, activation=ReLU\n",
      "0.761193266189 85\n",
      "lambda=0.1, learning rate=0.001, hidden units=133, activation=tanh\n",
      "0.776929654184 86\n",
      "lambda=0.1, learning rate=0.001, hidden units=133, activation=ReLU\n",
      "0.76257448871 87\n",
      "lambda=0.1, learning rate=0.001, hidden units=166, activation=tanh\n",
      "0.748086634157 88\n",
      "lambda=0.1, learning rate=0.001, hidden units=166, activation=ReLU\n",
      "0.784105424551 89\n",
      "lambda=0.001, learning rate=0.001, hidden units=33, activation=tanh\n",
      "0.74756441127 90\n",
      "lambda=0.001, learning rate=0.001, hidden units=33, activation=ReLU\n",
      "0.764454623072 91\n",
      "lambda=0.001, learning rate=0.001, hidden units=67, activation=tanh\n",
      "0.751345982376 92\n",
      "lambda=0.001, learning rate=0.001, hidden units=67, activation=ReLU\n",
      "0.7615604716 93\n",
      "lambda=0.001, learning rate=0.001, hidden units=100, activation=tanh\n",
      "0.765536088429 94\n",
      "lambda=0.001, learning rate=0.001, hidden units=100, activation=ReLU\n",
      "0.760348023895 95\n",
      "lambda=0.001, learning rate=0.001, hidden units=133, activation=tanh\n",
      "0.75956708219 96\n",
      "lambda=0.001, learning rate=0.001, hidden units=133, activation=ReLU\n",
      "0.755058166111 97\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lambda=0.001, learning rate=0.001, hidden units=166, activation=tanh\n",
      "0.767097956802 98\n",
      "lambda=0.001, learning rate=0.001, hidden units=166, activation=ReLU\n",
      "0.767450629618 99\n",
      "lambda=1e-05, learning rate=0.001, hidden units=33, activation=tanh\n",
      "0.759880321532 100\n",
      "lambda=1e-05, learning rate=0.001, hidden units=33, activation=ReLU\n",
      "0.762608342514 101\n",
      "lambda=1e-05, learning rate=0.001, hidden units=67, activation=tanh\n",
      "0.763984215251 102\n",
      "lambda=1e-05, learning rate=0.001, hidden units=67, activation=ReLU\n",
      "0.766231591504 103\n",
      "lambda=1e-05, learning rate=0.001, hidden units=100, activation=tanh\n",
      "0.764337798884 104\n",
      "lambda=1e-05, learning rate=0.001, hidden units=100, activation=ReLU\n",
      "0.768048106217 105\n",
      "lambda=1e-05, learning rate=0.001, hidden units=133, activation=tanh\n",
      "0.744271760693 106\n",
      "lambda=1e-05, learning rate=0.001, hidden units=133, activation=ReLU\n",
      "0.750333061205 107\n",
      "lambda=1e-05, learning rate=0.001, hidden units=166, activation=tanh\n",
      "0.771503876317 108\n",
      "lambda=1e-05, learning rate=0.001, hidden units=166, activation=ReLU\n",
      "0.770120053203 109\n",
      "lambda=0, learning rate=0.001, hidden units=33, activation=tanh\n",
      "0.749420860248 110\n",
      "lambda=0, learning rate=0.001, hidden units=33, activation=ReLU\n",
      "0.770712356205 111\n",
      "lambda=0, learning rate=0.001, hidden units=67, activation=tanh\n",
      "0.7611017048 112\n",
      "lambda=0, learning rate=0.001, hidden units=67, activation=ReLU\n",
      "0.769445726068 113\n",
      "lambda=0, learning rate=0.001, hidden units=100, activation=tanh\n",
      "0.735420565562 114\n",
      "lambda=0, learning rate=0.001, hidden units=100, activation=ReLU\n",
      "0.754967204645 115\n",
      "lambda=0, learning rate=0.001, hidden units=133, activation=tanh\n",
      "0.757791278359 116\n",
      "lambda=0, learning rate=0.001, hidden units=133, activation=ReLU\n",
      "0.765530012906 117\n",
      "lambda=0, learning rate=0.001, hidden units=166, activation=tanh\n",
      "0.760170373287 118\n",
      "lambda=0, learning rate=0.001, hidden units=166, activation=ReLU\n",
      "0.760767925849 119\n",
      "lambda=0.1, learning rate=0.001, hidden units=166, activation=ReLU\n",
      "Train R2: 0.784105424551\n",
      "Val R2: 0.784105424551\n",
      "Test R2: 0.740230647611\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 25\n",
    "\n",
    "# hyperparams to test\n",
    "lrs = [0.1, 0.01, 0.001]\n",
    "lams = [0.1, 0.001, 0.00001, 0]\n",
    "hiddens = [33, 67, 100, 133, 166]\n",
    "activations = ['tanh', 'ReLU']\n",
    "\n",
    "count = 0\n",
    "max_val_r2 = 0\n",
    "for lr in lrs:\n",
    "    for lam in lams:\n",
    "        for hidden in hiddens:\n",
    "            for activation in activations:\n",
    "\n",
    "                # model architecture\n",
    "                ff_nn = torch.nn.Sequential()\n",
    "                ff_nn.add_module(name = '1st linear', module = torch.nn.Linear(in_features = sensor_census_imp_train_x_stand.shape[1], out_features = hidden, bias = True)) # linear layer\n",
    "                ff_nn.add_module(name = 'norm', module = torch.nn.BatchNorm1d(num_features = hidden)) # normalize before tanh\n",
    "                \n",
    "                if activation == 'tanh':\n",
    "                    ff_nn.add_module(name = 'tanh', module = torch.nn.Tanh()) # tanh function for hidden layer\n",
    "                    \n",
    "                elif activation == 'ReLU':\n",
    "                    ff_nn.add_module(name = 'ReLU', module = torch.nn.ReLU()) # tanh function for hidden layer\n",
    "                    \n",
    "                ff_nn.add_module(name = '2nd linear', module = torch.nn.Linear(in_features = hidden, out_features = 1, bias = True)) # 2nd linear layer\n",
    "\n",
    "                # mse loss\n",
    "                loss_function = torch.nn.MSELoss()\n",
    "\n",
    "                # adam optimizer\n",
    "                adam = torch.optim.Adam(ff_nn.parameters(), lr=lr, weight_decay=lam)\n",
    "\n",
    "                for epoch in range(num_epochs): \n",
    "                    for batch in train_loader:\n",
    "                        model_output = ff_nn(Variable(batch[0]).float()) # model predictions\n",
    "                        targets = Variable(batch[1].float()) # true digit values\n",
    "\n",
    "                        adam.zero_grad() # zero gradient\n",
    "                        loss_batch = loss_function(model_output, targets) # compute loss\n",
    "                        loss_batch.backward() # take the gradient wrt parameters\n",
    "                        adam.step() # update parameters\n",
    "                \n",
    "                # get validation R2\n",
    "                val_r2 = compute_accuracy(val_loader, ff_nn)\n",
    "                \n",
    "                print('lambda=' + str(lam) + ', learning rate=' + str(lr) + ', hidden units=' + str(hidden) + ', activation=' + activation)\n",
    "                print(str(val_r2) + ' ' + str(count))\n",
    "                count += 1\n",
    "                \n",
    "                if val_r2 > max_val_r2: # compare current validation R2 to best R2 obtained so far\n",
    "                    final_lr = lr\n",
    "                    final_lam = lam\n",
    "                    final_hidden = hidden\n",
    "                    final_activation = activation\n",
    "                    train_r2 = compute_accuracy(val_loader, ff_nn)\n",
    "                    max_val_r2 = compute_accuracy(val_loader, ff_nn)\n",
    "                    test_r2 = compute_accuracy(test_loader, ff_nn)\n",
    "                        \n",
    "\n",
    "print('lambda=' + str(final_lam) + ', learning rate=' + str(final_lr) + ', hidden units=' + str(final_hidden) + ', activation=' + final_activation)\n",
    "print('Train R2: ' + str(train_r2))\n",
    "print('Val R2: ' + str(max_val_r2))\n",
    "print('Test R2: ' + str(test_r2))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.5.3",
   "language": "python",
   "name": "py35"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
