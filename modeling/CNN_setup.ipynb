{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sklearn.preprocessing\n",
    "import sklearn.metrics\n",
    "import torch\n",
    "from torch.autograd import Variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>site</th>\n",
       "      <th>year</th>\n",
       "      <th>date</th>\n",
       "      <th>MonitorData</th>\n",
       "      <th>GFEDFireCarbon</th>\n",
       "      <th>USElevation_dsc10000</th>\n",
       "      <th>USElevation_max100</th>\n",
       "      <th>USElevation_max10000</th>\n",
       "      <th>USElevation_mea100</th>\n",
       "      <th>USElevation_mea10000</th>\n",
       "      <th>...</th>\n",
       "      <th>Nearby_Peak2Lag3_MeanTemperature</th>\n",
       "      <th>Nearby_Peak2Lag3_MinTemperature</th>\n",
       "      <th>OMAEROe_UVAerosolIndex_Mean</th>\n",
       "      <th>OMAEROe_VISAerosolIndex_Mean</th>\n",
       "      <th>OMAERUVd_UVAerosolIndex_Mean</th>\n",
       "      <th>OMNO2d_ColumnAmountNO2StratoCloudScreened_Mean</th>\n",
       "      <th>OMO3PR</th>\n",
       "      <th>OMSO2e_ColumnAmountSO2_PBL_Mean</th>\n",
       "      <th>OMTO3e_ColumnAmountO3</th>\n",
       "      <th>OMUVBd_UVindex_Mean</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>2000</td>\n",
       "      <td>2000-01-01</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.001167</td>\n",
       "      <td>26.790501</td>\n",
       "      <td>43</td>\n",
       "      <td>30.143499</td>\n",
       "      <td>36.0</td>\n",
       "      <td>26.504299</td>\n",
       "      <td>...</td>\n",
       "      <td>286.112711</td>\n",
       "      <td>280.293551</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>2000</td>\n",
       "      <td>2000-01-02</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.001236</td>\n",
       "      <td>26.790501</td>\n",
       "      <td>43</td>\n",
       "      <td>30.143499</td>\n",
       "      <td>36.0</td>\n",
       "      <td>26.504299</td>\n",
       "      <td>...</td>\n",
       "      <td>286.112711</td>\n",
       "      <td>280.293551</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>2000</td>\n",
       "      <td>2000-01-03</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.001305</td>\n",
       "      <td>26.790501</td>\n",
       "      <td>43</td>\n",
       "      <td>30.143499</td>\n",
       "      <td>36.0</td>\n",
       "      <td>26.504299</td>\n",
       "      <td>...</td>\n",
       "      <td>286.112711</td>\n",
       "      <td>280.293551</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>2000</td>\n",
       "      <td>2000-01-04</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.001373</td>\n",
       "      <td>26.790501</td>\n",
       "      <td>43</td>\n",
       "      <td>30.143499</td>\n",
       "      <td>36.0</td>\n",
       "      <td>26.504299</td>\n",
       "      <td>...</td>\n",
       "      <td>286.112711</td>\n",
       "      <td>280.293551</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>2000</td>\n",
       "      <td>2000-01-05</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.001442</td>\n",
       "      <td>26.790501</td>\n",
       "      <td>43</td>\n",
       "      <td>30.143499</td>\n",
       "      <td>36.0</td>\n",
       "      <td>26.504299</td>\n",
       "      <td>...</td>\n",
       "      <td>290.424271</td>\n",
       "      <td>286.541158</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 116 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   site  year        date  MonitorData  GFEDFireCarbon  USElevation_dsc10000  \\\n",
       "0     1  2000  2000-01-01          NaN        0.001167             26.790501   \n",
       "1     1  2000  2000-01-02          NaN        0.001236             26.790501   \n",
       "2     1  2000  2000-01-03          NaN        0.001305             26.790501   \n",
       "3     1  2000  2000-01-04          NaN        0.001373             26.790501   \n",
       "4     1  2000  2000-01-05          NaN        0.001442             26.790501   \n",
       "\n",
       "   USElevation_max100  USElevation_max10000  USElevation_mea100  \\\n",
       "0                  43             30.143499                36.0   \n",
       "1                  43             30.143499                36.0   \n",
       "2                  43             30.143499                36.0   \n",
       "3                  43             30.143499                36.0   \n",
       "4                  43             30.143499                36.0   \n",
       "\n",
       "   USElevation_mea10000         ...           \\\n",
       "0             26.504299         ...            \n",
       "1             26.504299         ...            \n",
       "2             26.504299         ...            \n",
       "3             26.504299         ...            \n",
       "4             26.504299         ...            \n",
       "\n",
       "   Nearby_Peak2Lag3_MeanTemperature  Nearby_Peak2Lag3_MinTemperature  \\\n",
       "0                        286.112711                       280.293551   \n",
       "1                        286.112711                       280.293551   \n",
       "2                        286.112711                       280.293551   \n",
       "3                        286.112711                       280.293551   \n",
       "4                        290.424271                       286.541158   \n",
       "\n",
       "   OMAEROe_UVAerosolIndex_Mean  OMAEROe_VISAerosolIndex_Mean  \\\n",
       "0                          NaN                           NaN   \n",
       "1                          NaN                           NaN   \n",
       "2                          NaN                           NaN   \n",
       "3                          NaN                           NaN   \n",
       "4                          NaN                           NaN   \n",
       "\n",
       "   OMAERUVd_UVAerosolIndex_Mean  \\\n",
       "0                           NaN   \n",
       "1                           NaN   \n",
       "2                           NaN   \n",
       "3                           NaN   \n",
       "4                           NaN   \n",
       "\n",
       "   OMNO2d_ColumnAmountNO2StratoCloudScreened_Mean  OMO3PR  \\\n",
       "0                                             NaN     NaN   \n",
       "1                                             NaN     NaN   \n",
       "2                                             NaN     NaN   \n",
       "3                                             NaN     NaN   \n",
       "4                                             NaN     NaN   \n",
       "\n",
       "   OMSO2e_ColumnAmountSO2_PBL_Mean  OMTO3e_ColumnAmountO3  OMUVBd_UVindex_Mean  \n",
       "0                              NaN                    NaN                  NaN  \n",
       "1                              NaN                    NaN                  NaN  \n",
       "2                              NaN                    NaN                  NaN  \n",
       "3                              NaN                    NaN                  NaN  \n",
       "4                              NaN                    NaN                  NaN  \n",
       "\n",
       "[5 rows x 116 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### read in 1st X rows of big data\n",
    "data = pd.read_csv('assembled_data.csv', nrows = 2000000-380)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_sizes_site(sites):\n",
    "    \"\"\"Gets the split sizes to split dataset by site for a dataset with multiple sites.\n",
    "    \n",
    "    Arguments:\n",
    "        sites (array): array indicating the site of each row \n",
    "    \"\"\"\n",
    "    split_sizes = []\n",
    "    for i in range(len(sites)):\n",
    "        if i == 0:\n",
    "            site = sites[i]\n",
    "            split_sizes.append(i)\n",
    "        elif site != sites[i]:\n",
    "            site = sites[i]\n",
    "            split_sizes.append(i - (len(split_sizes)-1)*split_sizes[len(split_sizes)-1])\n",
    "        elif i == len(sites)-1:\n",
    "            split_sizes.append((i+1) - (len(split_sizes)-1)*split_sizes[len(split_sizes)-1])\n",
    "    \n",
    "    split_sizes = split_sizes[1:]\n",
    "    return split_sizes\n",
    "\n",
    "\n",
    "def split_data(tensor, split_sizes, dim=0):\n",
    "    \"\"\"Splits the tensor according to chunks of split_sizes.\n",
    "    \n",
    "    Arguments:\n",
    "        tensor (Tensor): tensor to split.\n",
    "        split_sizes (list(int)): sizes of chunks\n",
    "        dim (int): dimension along which to split the tensor.\n",
    "    \"\"\"\n",
    "    if dim < 0:\n",
    "        dim += tensor.dim()\n",
    "    \n",
    "    dim_size = tensor.size(dim)\n",
    "    if dim_size != torch.sum(torch.Tensor(split_sizes)):\n",
    "        raise KeyError(\"Sum of split sizes exceeds tensor dim\")\n",
    "    \n",
    "    splits = torch.cumsum(torch.Tensor([0] + split_sizes), dim=0)[:-1]\n",
    "    return tuple(tensor.narrow(int(dim), int(start), int(length)) \n",
    "        for start, length in zip(splits, split_sizes))\n",
    "\n",
    "\n",
    "def pad_stack_splits(site_tuple, split_sizes, x_or_y):\n",
    "    \"\"\"Zero (x) or nan (y) pads site data sequences and stacks them into a matrix.\n",
    "    \n",
    "    Arguments:\n",
    "        site_tuple (tuple): tuple of site data sequences to pad and stack\n",
    "        split_sizes (array): lengths of site data sequences\n",
    "        x_or_y (string): 'x' or 'y' indicating whether to pad and stack x or y\n",
    "    \"\"\"\n",
    "    data_padded_list = []\n",
    "    for sequence in site_tuple:\n",
    "        max_sequence_length = torch.max(torch.from_numpy(split_sizes))\n",
    "\n",
    "        if x_or_y == 'x':\n",
    "            zero_padding_rows = torch.zeros(max_sequence_length - sequence.size()[0], sequence.size()[1])\n",
    "            data_padded_list.append(torch.cat((sequence, zero_padding_rows), dim = 0))\n",
    "            \n",
    "        elif x_or_y == 'y':\n",
    "            nan_padding = torch.zeros(max_sequence_length - sequence.size()[0]).double() * np.nan\n",
    "            data_padded_list.append(torch.cat((sequence, nan_padding), dim = 0))\n",
    "            \n",
    "    return torch.stack(data_padded_list, dim = 0)\n",
    "\n",
    "\n",
    "def get_monitorData_indices(sequence):\n",
    "    \"\"\"Gets indices for a site data sequence for which there is an output for MonitorData.\n",
    "    \n",
    "    Arguments:\n",
    "        sequence (tensor): sequence of MonitorData outputs for a given site, including NaNs\n",
    "    \"\"\"\n",
    "    response_indicator_vec = sequence == sequence\n",
    "    num_responses = torch.sum(response_indicator_vec)\n",
    "    response_indices = torch.sort(response_indicator_vec, dim = 0, descending = True)[1][:num_responses]\n",
    "    ordered_response_indices = torch.sort(response_indices)[0]\n",
    "    return ordered_response_indices\n",
    "\n",
    "\n",
    "def r2(model, batch_size, x_stack, y_tuple):\n",
    "    \"\"\"Computes R-squared\n",
    "    \n",
    "    Arguments:\n",
    "        model (torch): model to test\n",
    "        batch_size (int): to determine how many sequences to read in at a time\n",
    "        x_stack (tensor): stack of site data sequences\n",
    "        y_tuple (tuple): tuple of true y values by sequence, including NaNs\n",
    "    \n",
    "    \"\"\"\n",
    "    y = []\n",
    "    pred = []\n",
    "    \n",
    "    # get number of batches\n",
    "    if x_stack.size()[0] % batch_size != 0:\n",
    "        num_batches = int(np.floor(x_stack.size()[0]/batch_size) + 1)\n",
    "    else:\n",
    "        num_batches = int(x_stack.size()[0]/batch_size)\n",
    "        \n",
    "    for batch in range(num_batches):\n",
    "        # get x and y for this batch\n",
    "        x_stack_batch = x_stack[batch_size * batch:batch_size * (batch+1)]\n",
    "        y_tuple_nans = y_tuple[batch_size * batch:batch_size * (batch+1)]\n",
    "        \n",
    "        # get indices for monitor data and actual monitor data\n",
    "        y_by_site = []\n",
    "        y_ind_by_site = []\n",
    "        for i in range(len(y_tuple_nans)):\n",
    "            y_ind = get_monitorData_indices(y_tuple_nans[i])\n",
    "            y_by_site.append(y_tuple_nans[i][y_ind])\n",
    "            y_ind_by_site.append(y_ind)\n",
    "        y_batch = list(Variable(torch.cat(y_by_site, dim=0)).data.numpy())\n",
    "        \n",
    "        # get model output\n",
    "        pred_batch = list(cnn(x_stack_batch, y_ind_by_site).data.numpy())\n",
    "        \n",
    "        # concatenate new predictions with ones from previous batches\n",
    "        y += y_batch\n",
    "        pred += pred_batch\n",
    "        \n",
    "    return sklearn.metrics.r2_score(y, pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "### CNN model architecture\n",
    "class CNN(torch.nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, kernel_size, padding):\n",
    "        super(CNN, self).__init__()\n",
    "        \n",
    "        self.conv1d = torch.nn.Conv1d(in_channels=input_size, out_channels=hidden_size, kernel_size=kernel_size, padding=padding, bias=bias)\n",
    "        self.norm1 = torch.nn.BatchNorm1d(num_features = hidden_size)\n",
    "        self.tanh = torch.nn.Tanh()\n",
    "        self.linear = torch.nn.Linear(in_features = hidden_size, out_features = 1, bias = True)\n",
    "        \n",
    "    def forward(self, input, y_ind_by_site):\n",
    "        hidden = self.conv1d(input)\n",
    "        hidden = self.norm1(hidden)\n",
    "        hidden = self.tanh(hidden)\n",
    "        \n",
    "        hidden_w_response = []\n",
    "        for i in range(hidden.size()[0]):\n",
    "            hidden_w_response.append(torch.transpose(hidden[i][:, y_ind_by_site[i]], 0, 1)) \n",
    "        hidden_w_response = torch.cat(hidden_w_response, dim = 0)\n",
    "        \n",
    "        output = self.linear(hidden_w_response)\n",
    "\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### train/val/test split by site id\n",
    "np.random.seed(2)\n",
    "\n",
    "# get sites for val/test data\n",
    "val_test_sites = np.random.choice(np.unique(data['site'].values), round(len(np.unique(data['site'].values))/4), replace = False)\n",
    "\n",
    "# get sites for test data\n",
    "test_sites = np.random.choice(np.unique(val_test_sites), round(len(np.unique(val_test_sites))/2), replace = False)\n",
    "\n",
    "# train sites/rows and x/y split\n",
    "train = data[~data['site'].isin(val_test_sites)]\n",
    "train_x = train.iloc[:, 4:]\n",
    "train_y = train.loc[:, 'MonitorData']\n",
    "train_sites = train.loc[:, 'site']\n",
    "\n",
    "# val sites/rows and x/y split\n",
    "val = data[(data['site'].isin(val_test_sites)) & (~data['site'].isin(test_sites))]\n",
    "val_x = val.iloc[:, 4:]\n",
    "val_y = val.loc[:, 'MonitorData']\n",
    "val_sites = val.loc[:, 'site']\n",
    "\n",
    "# test sites/rows and x/y split\n",
    "test = data[data['site'].isin(test_sites)]\n",
    "test_x = test.iloc[:, 4:]\n",
    "test_y = test.loc[:, 'MonitorData']\n",
    "test_sites = test.loc[:, 'site']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "### impute mean\n",
    "mean_imputer = sklearn.preprocessing.Imputer(strategy = 'mean')\n",
    "train_x_imp = mean_imputer.fit_transform(train_x)\n",
    "val_x_imp = mean_imputer.transform(val_x)\n",
    "test_x_imp = mean_imputer.transform(test_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### standardize features\n",
    "standardizer = sklearn.preprocessing.StandardScaler(with_mean = True, with_std = True)\n",
    "train_x_imp_std = standardizer.fit_transform(train_x_imp)\n",
    "val_x_imp_std = standardizer.transform(val_x_imp)\n",
    "test_x_imp_std = standardizer.transform(test_x_imp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "### get split sizes for TRAIN data (splitting by site)\n",
    "train_split_sizes = split_sizes_site(train_sites.values)\n",
    "\n",
    "### get tuples by site\n",
    "train_x_std_tuple = split_data(torch.from_numpy(train_x_imp_std).float(), train_split_sizes, dim = 0)\n",
    "train_y_tuple = split_data(torch.from_numpy(train_y.values), train_split_sizes, dim = 0)\n",
    "\n",
    "### get site sequences stacked into matrix to go through CNN\n",
    "train_x_std_stack = pad_stack_splits(train_x_std_tuple, np.array(train_split_sizes), 'x')\n",
    "train_x_std_stack = Variable(torch.transpose(train_x_std_stack, 1, 2))\n",
    "\n",
    "\n",
    "### get split sizes for VALIDATION data (splitting by site)\n",
    "val_split_sizes = split_sizes_site(val_sites.values)\n",
    "\n",
    "### get tuples by site\n",
    "val_x_std_tuple = split_data(torch.from_numpy(val_x_imp_std).float(), val_split_sizes, dim = 0)\n",
    "val_y_tuple = split_data(torch.from_numpy(val_y.values), val_split_sizes, dim = 0)\n",
    "\n",
    "### get site sequences stacked into matrix to go through CNN\n",
    "val_x_std_stack = pad_stack_splits(val_x_std_tuple, np.array(val_split_sizes), 'x')\n",
    "val_x_std_stack = Variable(torch.transpose(val_x_std_stack, 1, 2))\n",
    "\n",
    "\n",
    "### get split sizes for TEST data (splitting by site)\n",
    "test_split_sizes = split_sizes_site(test_sites.values)\n",
    "\n",
    "### get tuples by site\n",
    "test_x_std_tuple = split_data(torch.from_numpy(test_x_imp_std).float(), test_split_sizes, dim = 0)\n",
    "test_y_tuple = split_data(torch.from_numpy(test_y.values), test_split_sizes, dim = 0)\n",
    "\n",
    "### get site sequences stacked into matrix to go through CNN\n",
    "test_x_std_stack = pad_stack_splits(test_x_std_tuple, np.array(test_split_sizes), 'x')\n",
    "test_x_std_stack = Variable(torch.transpose(test_x_std_stack, 1, 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# CNN parameters\n",
    "input_size = train_x_imp_std.shape[1]\n",
    "hidden_size = 25\n",
    "kernel_size = 3\n",
    "padding = 1\n",
    "bias = True\n",
    "\n",
    "# instantiate model\n",
    "cnn = CNN(input_size, hidden_size, kernel_size, padding)\n",
    "\n",
    "# Loss function\n",
    "mse_loss = torch.nn.MSELoss(size_average=True)\n",
    "\n",
    "# Optimizer\n",
    "lr = 0.0005\n",
    "weight_decay = 0.000001\n",
    "optimizer = torch.optim.Adam(cnn.parameters(), lr=lr, weight_decay=weight_decay)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-1.84605296764\n",
      "2151.6719512939453\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 10000\n",
    "batch_size = 20\n",
    "\n",
    "# get number of batches\n",
    "if train_x_std_stack.size()[0] % batch_size != 0:\n",
    "    num_batches = int(np.floor(train_x_std_stack.size()[0]/batch_size) + 1)\n",
    "else:\n",
    "    num_batches = int(train_x_std_stack.size()[0]/batch_size)\n",
    "    \n",
    "    \n",
    "for epoch in range(num_epochs):\n",
    "    epoch_loss = 0\n",
    "    \n",
    "    for batch in range(num_batches):\n",
    "        # get x and y for this batch\n",
    "        x_stack_batch = train_x_std_stack[batch_size * batch:batch_size * (batch+1)]\n",
    "        y_tuple_nans = train_y_tuple[batch_size * batch:batch_size * (batch+1)]\n",
    "        \n",
    "        # get indices for monitor data and actual monitor data\n",
    "        y_by_site = []\n",
    "        y_ind_by_site = []\n",
    "        for i in range(len(y_tuple_nans)):\n",
    "            y_ind = get_monitorData_indices(y_tuple_nans[i])\n",
    "            y_by_site.append(y_tuple_nans[i][y_ind])\n",
    "            y_ind_by_site.append(y_ind)\n",
    "        y_batch = Variable(torch.cat(y_by_site, dim=0)).float()\n",
    "        \n",
    "        # get model output\n",
    "        pred_batch = cnn(x_stack_batch, y_ind_by_site)\n",
    "        \n",
    "        # compute loss, backprop, and update parameters\n",
    "        loss_batch = mse_loss(pred_batch, y_batch)\n",
    "        loss_batch.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        # accumulate loss over epoch\n",
    "        epoch_loss += loss_batch.data[0]\n",
    "        \n",
    "    print(r2(cnn, batch_size, val_x_std_stack, val_y_tuple))\n",
    "    print(epoch_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.40990543897281861"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r2(cnn, batch_size, train_x_std_stack, train_y_tuple) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 736,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x = train_x.loc[train_y.dropna(axis = 0).index, :]\n",
    "test_x = test_x.loc[test_y.dropna(axis = 0).index, :]\n",
    "\n",
    "### impute mean\n",
    "mean_imputer = sklearn.preprocessing.Imputer(strategy = 'mean')\n",
    "train_x_imp = mean_imputer.fit_transform(train_x)\n",
    "test_x_imp = mean_imputer.transform(test_x)\n",
    "\n",
    "train_x_imp_std = standardizer.fit_transform(train_x_imp)\n",
    "train_y = train_y.dropna(axis = 0).values\n",
    "\n",
    "test_x_imp_std = standardizer.transform(test_x_imp)\n",
    "test_y = test_y.dropna(axis = 0).values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 738,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.67064558617874481"
      ]
     },
     "execution_count": 738,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sklearn.preprocessing\n",
    "import sklearn.linear_model\n",
    "import sklearn.model_selection\n",
    "import sklearn.metrics\n",
    "import sklearn.ensemble\n",
    "\n",
    "# get ridge coefficients\n",
    "ridge = sklearn.linear_model.Ridge(alpha = 1000, random_state = 1)\n",
    "ridge.fit(train_x_imp_std, train_y)\n",
    "ridge.score(test_x_imp_std, test_y)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.5.3",
   "language": "python",
   "name": "py35"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
