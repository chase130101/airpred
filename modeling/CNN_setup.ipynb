{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sklearn.preprocessing\n",
    "import torch\n",
    "from torch.autograd import Variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>site</th>\n",
       "      <th>year</th>\n",
       "      <th>date</th>\n",
       "      <th>MonitorData</th>\n",
       "      <th>GFEDFireCarbon</th>\n",
       "      <th>USElevation_dsc10000</th>\n",
       "      <th>USElevation_max100</th>\n",
       "      <th>USElevation_max10000</th>\n",
       "      <th>USElevation_mea100</th>\n",
       "      <th>USElevation_mea10000</th>\n",
       "      <th>...</th>\n",
       "      <th>Nearby_Peak2Lag3_MeanTemperature</th>\n",
       "      <th>Nearby_Peak2Lag3_MinTemperature</th>\n",
       "      <th>OMAEROe_UVAerosolIndex_Mean</th>\n",
       "      <th>OMAEROe_VISAerosolIndex_Mean</th>\n",
       "      <th>OMAERUVd_UVAerosolIndex_Mean</th>\n",
       "      <th>OMNO2d_ColumnAmountNO2StratoCloudScreened_Mean</th>\n",
       "      <th>OMO3PR</th>\n",
       "      <th>OMSO2e_ColumnAmountSO2_PBL_Mean</th>\n",
       "      <th>OMTO3e_ColumnAmountO3</th>\n",
       "      <th>OMUVBd_UVindex_Mean</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>2000</td>\n",
       "      <td>2000-01-01</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.001167</td>\n",
       "      <td>26.790501</td>\n",
       "      <td>43</td>\n",
       "      <td>30.143499</td>\n",
       "      <td>36</td>\n",
       "      <td>26.504299</td>\n",
       "      <td>...</td>\n",
       "      <td>286.112711</td>\n",
       "      <td>280.293551</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>2000</td>\n",
       "      <td>2000-01-02</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.001236</td>\n",
       "      <td>26.790501</td>\n",
       "      <td>43</td>\n",
       "      <td>30.143499</td>\n",
       "      <td>36</td>\n",
       "      <td>26.504299</td>\n",
       "      <td>...</td>\n",
       "      <td>286.112711</td>\n",
       "      <td>280.293551</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>2000</td>\n",
       "      <td>2000-01-03</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.001305</td>\n",
       "      <td>26.790501</td>\n",
       "      <td>43</td>\n",
       "      <td>30.143499</td>\n",
       "      <td>36</td>\n",
       "      <td>26.504299</td>\n",
       "      <td>...</td>\n",
       "      <td>286.112711</td>\n",
       "      <td>280.293551</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>2000</td>\n",
       "      <td>2000-01-04</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.001373</td>\n",
       "      <td>26.790501</td>\n",
       "      <td>43</td>\n",
       "      <td>30.143499</td>\n",
       "      <td>36</td>\n",
       "      <td>26.504299</td>\n",
       "      <td>...</td>\n",
       "      <td>286.112711</td>\n",
       "      <td>280.293551</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>2000</td>\n",
       "      <td>2000-01-05</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.001442</td>\n",
       "      <td>26.790501</td>\n",
       "      <td>43</td>\n",
       "      <td>30.143499</td>\n",
       "      <td>36</td>\n",
       "      <td>26.504299</td>\n",
       "      <td>...</td>\n",
       "      <td>290.424271</td>\n",
       "      <td>286.541158</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 116 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   site  year        date  MonitorData  GFEDFireCarbon  USElevation_dsc10000  \\\n",
       "0     1  2000  2000-01-01          NaN        0.001167             26.790501   \n",
       "1     1  2000  2000-01-02          NaN        0.001236             26.790501   \n",
       "2     1  2000  2000-01-03          NaN        0.001305             26.790501   \n",
       "3     1  2000  2000-01-04          NaN        0.001373             26.790501   \n",
       "4     1  2000  2000-01-05          NaN        0.001442             26.790501   \n",
       "\n",
       "   USElevation_max100  USElevation_max10000  USElevation_mea100  \\\n",
       "0                  43             30.143499                  36   \n",
       "1                  43             30.143499                  36   \n",
       "2                  43             30.143499                  36   \n",
       "3                  43             30.143499                  36   \n",
       "4                  43             30.143499                  36   \n",
       "\n",
       "   USElevation_mea10000         ...           \\\n",
       "0             26.504299         ...            \n",
       "1             26.504299         ...            \n",
       "2             26.504299         ...            \n",
       "3             26.504299         ...            \n",
       "4             26.504299         ...            \n",
       "\n",
       "   Nearby_Peak2Lag3_MeanTemperature  Nearby_Peak2Lag3_MinTemperature  \\\n",
       "0                        286.112711                       280.293551   \n",
       "1                        286.112711                       280.293551   \n",
       "2                        286.112711                       280.293551   \n",
       "3                        286.112711                       280.293551   \n",
       "4                        290.424271                       286.541158   \n",
       "\n",
       "   OMAEROe_UVAerosolIndex_Mean  OMAEROe_VISAerosolIndex_Mean  \\\n",
       "0                          NaN                           NaN   \n",
       "1                          NaN                           NaN   \n",
       "2                          NaN                           NaN   \n",
       "3                          NaN                           NaN   \n",
       "4                          NaN                           NaN   \n",
       "\n",
       "   OMAERUVd_UVAerosolIndex_Mean  \\\n",
       "0                           NaN   \n",
       "1                           NaN   \n",
       "2                           NaN   \n",
       "3                           NaN   \n",
       "4                           NaN   \n",
       "\n",
       "   OMNO2d_ColumnAmountNO2StratoCloudScreened_Mean  OMO3PR  \\\n",
       "0                                             NaN     NaN   \n",
       "1                                             NaN     NaN   \n",
       "2                                             NaN     NaN   \n",
       "3                                             NaN     NaN   \n",
       "4                                             NaN     NaN   \n",
       "\n",
       "   OMSO2e_ColumnAmountSO2_PBL_Mean  OMTO3e_ColumnAmountO3  OMUVBd_UVindex_Mean  \n",
       "0                              NaN                    NaN                  NaN  \n",
       "1                              NaN                    NaN                  NaN  \n",
       "2                              NaN                    NaN                  NaN  \n",
       "3                              NaN                    NaN                  NaN  \n",
       "4                              NaN                    NaN                  NaN  \n",
       "\n",
       "[5 rows x 116 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### read in 1st X rows of big data\n",
    "data = pd.read_csv('assembled_data.csv', nrows = 50000)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### train/val/test split by site id\n",
    "np.random.seed(1)\n",
    "\n",
    "# get sites for val/test data\n",
    "val_test_sites = np.random.choice(np.unique(data['site'].values), round(len(np.unique(data['site'].values))/2), replace = False)\n",
    "\n",
    "# get sites for test data\n",
    "test_sites = np.random.choice(np.unique(val_test_sites), round(len(np.unique(val_test_sites))/2), replace = False)\n",
    "\n",
    "# train sites/rows and x/y split\n",
    "train = data[~data['site'].isin(val_test_sites)]\n",
    "train_x = train.iloc[:, 4:]\n",
    "train_y = train.loc[:, 'MonitorData']\n",
    "train_sites = train.loc[:, 'site']\n",
    "\n",
    "# val sites/rows and x/y split\n",
    "val = data[(data['site'].isin(val_test_sites)) & (~data['site'].isin(test_sites))]\n",
    "val_x = val.iloc[:, 4:]\n",
    "val_y = val.loc[:, 'MonitorData']\n",
    "val_sites = val.loc[:, 'site']\n",
    "\n",
    "# test sites/rows and x/y split\n",
    "test = data[data['site'].isin(test_sites)]\n",
    "test_x = test.iloc[:, 4:]\n",
    "test_y = test.loc[:, 'MonitorData']\n",
    "val_sites = test.loc[:, 'site']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "### impute mean\n",
    "mean_imputer = sklearn.preprocessing.Imputer(strategy = 'mean')\n",
    "train_x_imp = mean_imputer.fit_transform(train_x)\n",
    "val_x_imp = mean_imputer.transform(val_x)\n",
    "test_x_imp = mean_imputer.transform(test_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### standardize features\n",
    "standardizer = sklearn.preprocessing.StandardScaler(with_mean = True, with_std = True)\n",
    "train_x_imp_std = standardizer.fit_transform(train_x_imp)\n",
    "val_x_imp_std = standardizer.transform(val_x_imp)\n",
    "test_x_imp_std = standardizer.transform(test_x_imp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 320,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_sizes_site(sites):\n",
    "    \"\"\"Gets the split sizes to split dataset by site for a dataset with multiple sites.\n",
    "    \n",
    "    Arguments:\n",
    "        sites (array): array indicating the site of each row \n",
    "    \"\"\"\n",
    "    split_sizes = []\n",
    "    for i in range(len(sites)):\n",
    "        if i == 0:\n",
    "            site = sites[i]\n",
    "            split_sizes.append(i)\n",
    "        elif site != sites[i]:\n",
    "            site = sites[i]\n",
    "            split_sizes.append(i - (len(split_sizes)-1)*split_sizes[len(split_sizes)-1])\n",
    "        elif i == len(sites)-1:\n",
    "            split_sizes.append((i+1) - (len(split_sizes)-1)*split_sizes[len(split_sizes)-1])\n",
    "    \n",
    "    split_sizes = split_sizes[1:]\n",
    "    return split_sizes\n",
    "\n",
    "\n",
    "def split_data(tensor, split_sizes, dim=0):\n",
    "    \"\"\"Splits the tensor according to chunks of split_sizes.\n",
    "    \n",
    "    Arguments:\n",
    "        tensor (Tensor): tensor to split.\n",
    "        split_sizes (list(int)): sizes of chunks\n",
    "        dim (int): dimension along which to split the tensor.\n",
    "    \"\"\"\n",
    "    if dim < 0:\n",
    "        dim += tensor.dim()\n",
    "    \n",
    "    dim_size = tensor.size(dim)\n",
    "    if dim_size != torch.sum(torch.Tensor(split_sizes)):\n",
    "        raise KeyError(\"Sum of split sizes exceeds tensor dim\")\n",
    "    \n",
    "    splits = torch.cumsum(torch.Tensor([0] + split_sizes), dim=0)[:-1]\n",
    "    return tuple(tensor.narrow(int(dim), int(start), int(length)) \n",
    "        for start, length in zip(splits, split_sizes))\n",
    "\n",
    "\n",
    "def pad_stack_splits(site_tuple, split_sizes, x_or_y):\n",
    "    \"\"\"Zero (x) or nan (y) pads site data sequences and stacks them into a matrix.\n",
    "    \n",
    "    Arguments:\n",
    "        site_tuple (tuple): tuple of site data sequences to pad and stack\n",
    "        split_sizes (array): lengths of site data sequences\n",
    "        x_or_y (string): 'x' or 'y' indicating whether to pad and stack x or y\n",
    "    \"\"\"\n",
    "    data_padded_list = []\n",
    "    for sequence in site_tuple:\n",
    "        max_sequence_length = torch.max(torch.from_numpy(split_sizes))\n",
    "\n",
    "        if x_or_y == 'x':\n",
    "            zero_padding_rows = torch.zeros(max_sequence_length - sequence.size()[0], sequence.size()[1])\n",
    "            data_padded_list.append(torch.cat((sequence, zero_padding_rows), dim = 0))\n",
    "            \n",
    "        elif x_or_y == 'y':\n",
    "            nan_padding = torch.zeros(max_sequence_length - sequence.size()[0]).double() * np.nan\n",
    "            data_padded_list.append(torch.cat((sequence, nan_padding), dim = 0))\n",
    "            \n",
    "    return torch.stack(data_padded_list, dim = 0)\n",
    "\n",
    "\n",
    "def get_monitorData_indices(sequence):\n",
    "    \"\"\"Gets indices for a site data sequence for which there is an output for MonitorData.\n",
    "    \n",
    "    Arguments:\n",
    "        sequence (tensor): sequence of MonitorData outputs for a given site, including NaNs\n",
    "    \"\"\"\n",
    "    response_indicator_vec = sequence == sequence\n",
    "    num_responses = torch.sum(response_indicator_vec)\n",
    "    response_indices = torch.sort(response_indicator_vec, dim = 0, descending = True)[1][:num_responses]\n",
    "    ordered_response_indices = torch.sort(response_indices)[0]\n",
    "    return ordered_response_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 402,
   "metadata": {},
   "outputs": [],
   "source": [
    "### get split sizes for training data (splitting by site)\n",
    "split_sizes = split_sizes_site(train_sites.values[:-2])\n",
    "\n",
    "### get tuples by site\n",
    "train_x_std_tuple = size_splits(torch.from_numpy(train_x_imp_std).float()[:-2, :], split_sizes, dim = 0)\n",
    "train_y_tuple = size_splits(torch.from_numpy(train_y.values[:-2]), split_sizes, dim = 0)\n",
    "\n",
    "### get site sequences stacked into matrix to go through CNN\n",
    "train_x_std_stack = pad_stack_splits(train_x_std_tuple, np.array(split_sizes), 'x')\n",
    "train_x_std_stack = Variable(torch.transpose(train_x_std_stack, 1, 2))\n",
    "\n",
    "### get indices for which there is a monitorData response for each site\n",
    "monitorData_ind_by_site = []\n",
    "for i in range(len(train_y_tuple)):\n",
    "    monitorData_ind_by_site.append(get_monitorData_indices(train_y_tuple[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 417,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CNN parameters\n",
    "input_size = train_x_imp_std.shape[1]\n",
    "hidden_size = 100\n",
    "kernel_size = 3\n",
    "padding = 1\n",
    "bias = True\n",
    "\n",
    "class CNN(torch.nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, kernel_size, padding):\n",
    "        super(CNN, self).__init__()\n",
    "\n",
    "        self.conv1d = torch.nn.Conv1d(in_channels=input_size, out_channels=hidden_size, kernel_size=kernel_size, padding=padding, bias=bias)\n",
    "        self.norm1 = torch.nn.BatchNorm1d(num_features = hidden_size)\n",
    "        self.tanh = torch.nn.Tanh()\n",
    "        self.linear = torch.nn.Linear(in_features = hidden_size, out_features = 1, bias = True)\n",
    "        \n",
    "    def forward(self, input, monitorData_ind_by_site):\n",
    "\n",
    "        hidden = self.conv1d(input)\n",
    "        hidden = self.norm1(hidden)\n",
    "        hidden = self.tanh(hidden)\n",
    "        \n",
    "        hidden_w_response = []\n",
    "        for i in range(hidden.size()[0]):\n",
    "            hidden_w_response.append(torch.transpose(hidden[i][:, monitorData_ind_by_site[i]], 0, 1)) \n",
    "        hidden_w_response = torch.cat(hidden_w_response, dim = 0)\n",
    "        \n",
    "        output = self.linear(hidden_w_response)\n",
    "\n",
    "        return output\n",
    "\n",
    "\n",
    "# Loss function\n",
    "loss_function = torch.nn.MSELoss(size_average=True)\n",
    "\n",
    "cnn = CNN(input_size, hidden_size, kernel_size, padding)\n",
    "\n",
    "# Optimizer\n",
    "lr = 0.01\n",
    "weight_decay = 0.000001\n",
    "optimizer = torch.optim.Adam(cnn.parameters(), lr=lr, weight_decay=weight_decay)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 416,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Variable containing:\n",
       "-0.3727\n",
       "-0.2432\n",
       "-0.3099\n",
       "   â‹®    \n",
       "-0.1612\n",
       "-0.1302\n",
       " 0.1693\n",
       "[torch.FloatTensor of size 9922x1]"
      ]
     },
     "execution_count": 416,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cnn.forward(train_x_std_stack, monitorData_ind_by_site)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'tuple' object has no attribute 'size'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-210-a991ddb26c69>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mtrain_x_std_tuples\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m: 'tuple' object has no attribute 'size'"
     ]
    }
   ],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.5.3",
   "language": "python",
   "name": "py35"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
