####################################################################
README file for HSPH Capstone Project Spring 2018
AC297r, Harvard Institute for Applied Computational Science (IACS)

*Keyan Halperin (keyan_halperin@g.harvard.edu) (Statistics)
*Chris Hase (christopher_hase@g.harvard.edu)   (Statistics)
*Justin Lee (justin_s_lee@g.harvard.edu)       (CSE)
*Casey Meehan (casey_meehan@g.harvard.edu)     (CSE)

Mentor: David Sondak (dsondak@seas.harvard.edu)
Version 1.0 
Created 05.06.2018
####################################################################

Introduction:

This README details the software we wrote for our capstone project. 
It describes (1) the infrastructure used, (2) how to configure dependencies,
and (3) the general software pipeline, and how to run each component.

####################################################################
(1) Hardware + Software Infrastructure

We used Harvard's Odyssey Supercomputing cluster to run our scripts. 
Within Odyssey, we tended to use the shared partition.

Most of our work was done in Python 3.6, with some pre-processing in R.

####################################################################
(2) Dependencies

*Python Dependencies*
All dependencies we needed were installed through the conda package
manager. We found that Odyssey offers a solution to install conda 
packages locally. This can be done by first setting up a local 
root for conda to install all packages to; this is done by running 
the following command:

conda create -n my_root --clone="/n/sw/fasrcsw/apps/Core/Anaconda3/5.0.1-fasrc01/x"

Package installs can then be done as follows. We use PyTorch as an example, but the
general pattern applies for any package.

------------------------------------------------
source activate my_root
conda install pytorch torchvision -c pytorch
source activate my_root
------------------------------------------------

The middle line can be substituted with any conda install.

Below is a list of Python packages that we used heavily. A * indicates that the package comes 
with Python 3 by default., and a ** indicates that the package comes with conda by
default:

-argparse
-configparser*
-numpy**
-pandas**
-pickle*
-sklearn**
-torch
-xgboost

All unmarked packages can be installed through conda. See https://anaconda.org/ for more information on individual packages.

*R Dependencies*
For a small portion of our pipeline, we also had to use R packages. We can install
R packages locally in much the same way as we did for Python above. This process is detailed here: https://www.rc.fas.harvard.edu/resources/documentation/software-on-odyssey/r/

In summary, we create a directory `~/apps/R` and run the following:

---------------------------------------------------
module load R/3.3.3-fasrc01
export R_LIBS_USER=$HOME/apps/R:$R_LIBS_USER
---------------------------------------------------

The second line specifies that we want to save packages locally, and can be put in the user's .bashrc file for convenience.

After this step, one can run `R` and use the `install.packages()` command to 
save package binaries to the local directory.

The packages we need for the R code are:

-dpylr
-data.table

####################################################################
(3) Software Pipeline/How to Run our Code

Data:

There are three data files located on our GitHub located in the data directory.

1. `sensor_locations_with_census.csv` is the only file that is actually needed, and is used to run `data_setup.R`. It contains each site ID, longitude, and latitude joined with the corresponding processed census data.

2. `raw_census_data_by_zip.zip` contains the unprocessed census data by ZIP code in .xlsx format given to us by IACS.

3. `census_data_csv_formatted.zip` contains the file in `raw_census_data_by_zip.zip` reformatted to be in .csv format.
	     


We have provided Slurm scripts that run our pipeline on Odyssey.

Our software pipeline is structured in the following order. We have also listed the corresponding scripts for each step:

- Preprocess + Train/Val/Test Split data and save to separate files
    * data-setup.sh
    * tvt-split.sh
     
- Run ridge or random forest imputation on the split data
    * ridge-imputer-fit.sh
    * ridge-impute-tvt.sh
    * rf-impute-tvt.sh

- Run models 
    * final-train-test.sh
    * cnn-tt.sh
    * cnn-validate.sh
    * model-cross-val.sh


To customize the parameters of the code, please make sure to look at the 
command line arguments of the Python scripts in the Slurm scripts, as well as 
the config directory.

As a rule of thumb, all directory configurations are stored in the config directory, inside `py_config.ini`. This file can be regenerated by running `python3 generate_py_config.py` should there be any changes necessary. `generate_py_config.py` itself contains a dictionary that configures variables.  

Another type of parameter specified in `py_config.ini` is CNN hyperparameters. Although most of our model parameters are specified through command line arguments, we thought that the number of parameters to set in the CNN code would make it difficult to customize through the command line, and therefore elected to put them in the config.

The Slurm scripts are kicked off on an Odyssey login node and allocated to worker nodes. However, it suffices to have the dependencies available in the user home directory. Odyssey automatically looks for them when running jobs.
