{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.autograd import Variable\n",
    "import sklearn.preprocessing\n",
    "import sklearn.metrics\n",
    "from data_split_tune_utils import X_y_site_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### read in train, val, and test\n",
    "train = pd.read_csv('../data/trainV_ridgeImp.csv')\n",
    "val = pd.read_csv('../data/valV_ridgeImp.csv')\n",
    "test = pd.read_csv('../data/testV_ridgeImp.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 319,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### split train, val, and test into x, y, and sites\n",
    "train_x, train_y, train_sites = X_y_site_split(train, y_var_name='MonitorData', site_var_name='site')\n",
    "val_x, val_y, val_sites = X_y_site_split(val, y_var_name='MonitorData', site_var_name='site')\n",
    "test_x, test_y, test_sites = X_y_site_split(test, y_var_name='MonitorData', site_var_name='site')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train_x = train_x.iloc[:, :40]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#train_x_std_all = train_x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### standardize all features\n",
    "standardizer_all = sklearn.preprocessing.StandardScaler(with_mean = True, with_std = True)\n",
    "train_x_std_all = standardizer_all.fit_transform(train_x)\n",
    "val_x_std_all = standardizer_all.transform(val_x)\n",
    "test_x_std_all = standardizer_all.transform(test_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### NN model architecture\n",
    "class NN(torch.nn.Module):\n",
    "    def __init__(self, input_size_full, hidden_size_full):\n",
    "        super(NN, self).__init__()\n",
    "        #self.linear_full = torch.nn.Linear(in_features=input_size_full, out_features=hidden_size_full, bias=True)\n",
    "        #self.norm_full = torch.nn.BatchNorm1d(num_features = hidden_size_full)\n",
    "        #self.relu_full = torch.nn.ReLU()\n",
    "        self.linear_full2 = torch.nn.Linear(in_features=input_size_full, out_features=1, bias=True)\n",
    "        \n",
    "    def forward(self, input_full):\n",
    "        #hidden_full = self.linear_full(input_full)\n",
    "        #hidden_full = self.norm_full(hidden_full)\n",
    "        #hidden_full = self.relu_full(hidden_full)\n",
    "        output_full = self.linear_full2(input_full)\n",
    "\n",
    "        return output_full"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 311,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NN parameters\n",
    "input_size_full = train_x_std_all.shape[1]\n",
    "#input_size_full = 1\n",
    "hidden_size_full = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 312,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "nn = torch.nn.Sequential()\n",
    "nn.add_module('linear', torch.nn.Linear(in_features=input_size_full, out_features=1, bias=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 313,
   "metadata": {},
   "outputs": [],
   "source": [
    "# instantiate model\n",
    "#nn = NN(input_size_full, hidden_size_full)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 314,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Loss function\n",
    "mse_loss = torch.nn.MSELoss(size_average=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 315,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optimizer\n",
    "lr = 0.0001\n",
    "weight_decay = 0.000001\n",
    "optimizer = torch.optim.Adam(nn.parameters(), lr=lr, weight_decay=weight_decay)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 316,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "num_epochs = 50\n",
    "batch_size = 500"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 317,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get number of batches\n",
    "if train_x_std_all.shape[0] % batch_size != 0:\n",
    "    num_batches = int(np.floor(train_x_std_all.shape[0]/batch_size) + 1)\n",
    "else:\n",
    "    num_batches = int(train_x_std_all.shape[0]/batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 318,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-27.3837367083\n",
      "Epoch loss after epoch 0: 3511186.7910633087\n",
      "-9.66310931106\n",
      "Epoch loss after epoch 1: 1312121.5751695633\n",
      "-3.79591421277\n",
      "Epoch loss after epoch 2: 568538.1880731583\n",
      "-0.709527868015\n",
      "Epoch loss after epoch 3: 199373.35906243324\n",
      "0.282875328478\n",
      "Epoch loss after epoch 4: 83861.5897140503\n",
      "0.464567856825\n",
      "Epoch loss after epoch 5: 63016.57800912857\n",
      "0.533406760999\n",
      "Epoch loss after epoch 6: 55002.779361605644\n",
      "0.580256867536\n",
      "Epoch loss after epoch 7: 49510.05389910936\n",
      "0.613094011813\n",
      "Epoch loss after epoch 8: 45643.7243757844\n",
      "0.636686384244\n",
      "Epoch loss after epoch 9: 42851.08884048462\n",
      "0.65420883478\n",
      "Epoch loss after epoch 10: 40765.01810437441\n",
      "0.667616159081\n",
      "Epoch loss after epoch 11: 39159.97445091605\n",
      "0.678097011326\n",
      "Epoch loss after epoch 12: 37898.741636902094\n",
      "0.686402558532\n",
      "Epoch loss after epoch 13: 36894.38498274982\n",
      "0.693038522144\n",
      "Epoch loss after epoch 14: 36088.146144494414\n",
      "0.698366209019\n",
      "Epoch loss after epoch 15: 35437.86316435039\n",
      "0.702656702032\n",
      "Epoch loss after epoch 16: 34911.76716826856\n",
      "0.706118598615\n",
      "Epoch loss after epoch 17: 34485.30305798538\n",
      "0.708916149888\n",
      "Epoch loss after epoch 18: 34139.05743109435\n",
      "0.711179451872\n",
      "Epoch loss after epoch 19: 33857.58086869866\n",
      "0.713012347732\n",
      "Epoch loss after epoch 20: 33628.49484251812\n",
      "0.714497937759\n",
      "Epoch loss after epoch 21: 33441.85027822107\n",
      "0.715702987575\n",
      "Epoch loss after epoch 22: 33289.61556239426\n",
      "0.716681101599\n",
      "Epoch loss after epoch 23: 33165.32612013072\n",
      "0.71747554214\n",
      "Epoch loss after epoch 24: 33063.73780848086\n",
      "0.718121168672\n",
      "Epoch loss after epoch 25: 32980.60727751255\n",
      "0.718646137698\n",
      "Epoch loss after epoch 26: 32912.50130841136\n",
      "0.719073236985\n",
      "Epoch loss after epoch 27: 32856.62327204645\n",
      "0.719420964903\n",
      "Epoch loss after epoch 28: 32810.70250272751\n",
      "0.719704200165\n",
      "Epoch loss after epoch 29: 32772.90150657296\n",
      "0.719935133931\n",
      "Epoch loss after epoch 30: 32741.712702274323\n",
      "0.720123613171\n",
      "Epoch loss after epoch 31: 32715.916172653437\n",
      "0.720277613601\n",
      "Epoch loss after epoch 32: 32694.514943778515\n",
      "0.720403622092\n",
      "Epoch loss after epoch 33: 32676.701787918806\n",
      "0.720507009552\n",
      "Epoch loss after epoch 34: 32661.801135241985\n",
      "0.720591978793\n",
      "Epoch loss after epoch 35: 32649.285956025124\n",
      "0.720662050026\n",
      "Epoch loss after epoch 36: 32638.712470710278\n",
      "0.720720063285\n",
      "Epoch loss after epoch 37: 32629.71948605776\n",
      "0.720768303701\n",
      "Epoch loss after epoch 38: 32622.017190635204\n",
      "0.720808734881\n",
      "Epoch loss after epoch 39: 32615.35963740945\n",
      "0.720842736833\n",
      "Epoch loss after epoch 40: 32609.562790811062\n",
      "0.720871634226\n",
      "Epoch loss after epoch 41: 32604.461459845304\n",
      "0.720896430702\n",
      "Epoch loss after epoch 42: 32599.923955142498\n",
      "0.720917893692\n",
      "Epoch loss after epoch 43: 32595.84885698557\n",
      "0.720936702045\n",
      "Epoch loss after epoch 44: 32592.15279072523\n",
      "0.72095339668\n",
      "Epoch loss after epoch 45: 32588.758925884962\n",
      "0.720968397757\n",
      "Epoch loss after epoch 46: 32585.61445891857\n",
      "0.72098203338\n",
      "Epoch loss after epoch 47: 32582.67464348674\n",
      "0.720994642015\n",
      "Epoch loss after epoch 48: 32579.897008001804\n",
      "0.721006353249\n",
      "Epoch loss after epoch 49: 32577.256901592016\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(num_epochs):\n",
    "    epoch_loss = 0\n",
    "    pred_epoch = []\n",
    "    y_epoch = []\n",
    "    \n",
    "    for batch in range(num_batches):\n",
    "        # get x and y for this batch\n",
    "        x = train_x_std_all[batch_size * batch:batch_size * (batch+1)].values\n",
    "        y = train_y[batch_size * batch:batch_size * (batch+1)].values\n",
    "        \n",
    "        # get indices for monitor data and actual monitor data\n",
    "        y_nan_ind = np.isnan(y)\n",
    "        \n",
    "        if np.sum(y_nan_ind) < len(y_nan_ind):\n",
    "            x = Variable(torch.from_numpy(x[~y_nan_ind])).float()\n",
    "            y = Variable(torch.from_numpy(y[~y_nan_ind])).float()\n",
    "\n",
    "            # get model output\n",
    "            pred_batch = nn(x)\n",
    "            #pred_batch = pred_batch[torch.from_numpy(y_nan_ind.values.argsort()[:len(y_nan_ind.values) - np.sum(y_nan_ind.values)]).long()]\n",
    "\n",
    "            # compute loss, backprop, and update parameters\n",
    "            optimizer.zero_grad()\n",
    "            loss_batch = mse_loss(pred_batch, y)\n",
    "            loss_batch.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            # accumulate loss over epoch\n",
    "            pred_epoch += list(pred_batch.data.numpy())\n",
    "            y_epoch += list(y.data.numpy())\n",
    "            epoch_loss += loss_batch.data[0]\n",
    "            \n",
    "    \n",
    "\n",
    "    #print('Validation R^2 after epoch ' + str(epoch) + ': ' + str(r2(cnn, batch_size, val_x_std_stack_nonConst, val_x_std_tuple, val_y_tuple)))\n",
    "    print(sklearn.metrics.r2_score(y_epoch, pred_epoch))\n",
    "    print('Epoch loss after epoch ' + str(epoch) + ': ' + str(epoch_loss))\n",
    "    \n",
    "\n",
    "#print('Test R^2: ' + str(r2(cnn, batch_size, test_x_std_stack_nonConst, test_x_std_tuple, test_y_tuple)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.5.3",
   "language": "python",
   "name": "py35"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
